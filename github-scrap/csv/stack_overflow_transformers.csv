Post Link,Title,Body
57248098,using huggingface's pytorch- transformers GPT-2 for classifcation tasks,"<p>I want to use GPT-2 to make a text classifier model. I am not really sure what head should I add after I extracted features through the GPT-2. for eample I have a sequence.</p>

<pre><code>import pytorch_transformers as pt 
import torch
text=test.iloc[1,1]
text
'If a fire wanted fanning, it could readily be fanned with a newspaper, and as the government grew weaker, I have no doubt that leather and iron acquired durability in proportion, for, in a very short time, there was not a pair of bellows in all Rotterdam that ever stood in need of a stitch or required the assistance of a hammer.'
len(text)

74
tokenizer = pt.GPT2Tokenizer.from_pretrained('gpt2')
model = pt.GPT2Model.from_pretrained('gpt2')
zz = tokenizer.tokenize(text)
z1=torch.tensor([tokenizer.convert_tokens_to_ids(zz)])
z1
tensor([[ 1532,   257,  2046,  2227,  4336,   768,    11,   340,   714, 14704,
           307,   277,  3577,   351,   257,  7533,    11,   290,   355,   262,
          1230,  6348, 17642,    11,   314,   423,   645,  4719,   326, 11620,
           290,  6953,  9477, 26578,   287,  9823,    11,   329,    11,   287,
           257,   845,  1790,   640,    11,   612,   373,   407,   257,  5166,
           286,  8966,  1666,   287,   477, 18481,   353, 11043,   326,  1683,
          6204,   287,   761,   286,   257, 24695,   393,  2672,   262,  6829,
           286,   257, 15554,    13]])
output,hidden=model(z1)
ouput.shape
torch.Size([1, 74, 768])
</code></pre>

<p>the output of GPT2 is n x m x 768 for me, which n is the batch size,m is the number of tokens in the seqence(for example I can pad/truncate to 128.), so I can not do what as the paper said for a classification task just add a fully connected layer in the tail.And I searched on google, few GPT-2 classification task is mensioned.
I am not sure what is correct. Should I do flatten/max pooling/average pooling before the  fully connected layer or something else?</p>
"
58454157,PyTorch BERT TypeError: forward() got an unexpected keyword argument 'labels',"<p>Training a BERT model using PyTorch transformers (following the tutorial <a href=""https://mccormickml.com/2019/07/22/BERT-fine-tuning/"" rel=""noreferrer"">here</a>).</p>

<p>Following statement in the tutorial</p>

<pre><code>loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
</code></pre>

<p>leads to</p>

<pre><code>TypeError: forward() got an unexpected keyword argument 'labels'
</code></pre>

<p>Here is the full error,</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-53-56aa2f57dcaf&gt; in &lt;module&gt;
     26         optimizer.zero_grad()
     27         # Forward pass
---&gt; 28         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
     29         train_loss_set.append(loss.item())
     30         # Backward pass

~/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

TypeError: forward() got an unexpected keyword argument 'labels'
</code></pre>

<p>I cant seem to figure out what kind of argument the forward() function expects.</p>

<p>There is a similar problem <a href=""https://github.com/allenai/allennlp/issues/2528"" rel=""noreferrer"">here</a>, but I still do not get what the solution is.</p>

<p>System information:</p>

<ul>
<li>OS: Ubuntu 16.04 LTS</li>
<li>Python version: 3.6.x</li>
<li>Torch version: 1.3.0</li>
<li>Torch Vision version: 0.4.1</li>
<li>PyTorch transformers version: 1.2.0</li>
</ul>
"
58532911,Why is the input size of the MultiheadAttention in Pytorch Transformer module 1536?,"<p>When using the <code>torch.nn.modules.transformer.Transformer</code> module/object, the first layer is the <code>encoder.layers.0.self_attn</code> layer that is a <code>MultiheadAttention</code> layer, i.e. </p>

<pre><code>from torch.nn.modules.transformer import Transformer
bumblebee = Transformer()

bumblee.parameters
</code></pre>

<p>[out]:</p>

<pre><code>&lt;bound method Module.parameters of Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
</code></pre>

<p>And if we print out the size of the layer, we see:</p>

<pre><code>for name in bumblebee.encoder.state_dict():
    print(name, '\t', bumblebee.encoder.state_dict()[name].shape)
</code></pre>

<p>[out]:</p>

<pre><code>layers.0.self_attn.in_proj_weight    torch.Size([1536, 512])
layers.0.self_attn.in_proj_bias      torch.Size([1536])
layers.0.self_attn.out_proj.weight   torch.Size([512, 512])
layers.0.self_attn.out_proj.bias     torch.Size([512])
layers.0.linear1.weight      torch.Size([2048, 512])
layers.0.linear1.bias    torch.Size([2048])
layers.0.linear2.weight      torch.Size([512, 2048])
layers.0.linear2.bias    torch.Size([512])
layers.0.norm1.weight    torch.Size([512])
layers.0.norm1.bias      torch.Size([512])
layers.0.norm2.weight    torch.Size([512])
layers.0.norm2.bias      torch.Size([512])
</code></pre>

<p>It seems like 1536 is 512 * 3 and somehow the <code>layers.0.self_attn.in_proj_weight</code> parameter might be storing all three QKV tensors in the transformer architecture in one matrix. </p>

<p>From <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L649"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L649</a> </p>

<pre><code>class MultiheadAttention(Module):
    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, ""embed_dim must be divisible by num_heads""

        if self._qkv_same_embed_dim is False:
            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))
            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))
            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))
        else:
            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))
</code></pre>

<p>And the note in the docstring of the <code>MultiheadAttention</code> says:</p>

<blockquote>
  <p>Note: if kdim and vdim are None, they will be set to embed_dim such that
  query, key, and value have the same number of features.</p>
</blockquote>

<p>Is that correct? </p>
"
59435020,Get probability of multi-token word in MASK position,"<p>It is relatively easy to get a token's probability according to a language model, as the snippet below shows. You can get the output of a model, restrict yourself to the output of the masked token, and then find the probability of your requested token in the output vector. However, this only works with single-token words, e.g. words that are themselves in the tokenizer's vocabulary. When a word does not exist in the vocabulary, the tokenizer will chunk it up into pieces that it <em>does</em> know (see the bottom of the example). But since the input sentence consists of only one masked position, and the requested token has more tokens than that, how can we get its probability? Ultimately I am looking for a solution that works regardless of the number of subword units a word has.</p>

<p>In the code below I have added many comments explaining what is going on, as well as printing out the given output of print statements. You'll see that predicting tokens such as 'love' and 'hate' is straightforward because they are in the tokenizer's vocabulary. 'reprimand' is not, though, so it cannot be predicted in a single masked position - it consists of three subword units. So how can we predict 'reprimand' in the masked position?</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM
import torch

# init model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()
# init softmax to get probabilities later on
sm = torch.nn.Softmax(dim=0)
torch.set_grad_enabled(False)

# set sentence with MASK token, convert to token_ids
sentence = f""I {tokenizer.mask_token} you""
token_ids = tokenizer.encode(sentence, return_tensors='pt')
print(token_ids)
# tensor([[ 101, 1045,  103, 2017,  102]])
# get the position of the masked token
masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()

# forward
output = model(token_ids)
last_hidden_state = output[0].squeeze(0)
# only get output for masked token
# output is the size of the vocabulary
mask_hidden_state = last_hidden_state[masked_position]
# convert to probabilities (softmax)
# giving a probability for each item in the vocabulary
probs = sm(mask_hidden_state)

# get probability of token 'hate'
hate_id = tokenizer.convert_tokens_to_ids('hate')
print('hate probability', probs[hate_id].item())
# hate probability 0.008057191967964172

# get probability of token 'love'
love_id = tokenizer.convert_tokens_to_ids('love')
print('love probability', probs[love_id].item())
# love probability 0.6704086065292358

# get probability of token 'reprimand' (?)
reprimand_id = tokenizer.convert_tokens_to_ids('reprimand')
# reprimand is not in the vocabulary, so it needs to be split into subword units
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# [UNK]

reprimand_id = tokenizer.encode('reprimand', add_special_tokens=False)
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# ['rep', '##rim', '##and']
# but how do we now get the probability of a multi-token word in a single-token position?
</code></pre>
"
59656096,Trouble saving tf.keras model with Bert (huggingface) classifier,"<p>I am training a binary classifier that uses Bert (huggingface). The model looks like this:</p>

<pre><code>def get_model(lr=0.00001):
    inp_bert = Input(shape=(512), dtype=""int32"")
    bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')(inp_bert)[0]
    doc_encodings = tf.squeeze(bert[:, 0:1, :], axis=1)
    out = Dense(1, activation=""sigmoid"")(doc_encodings)
    model = Model(inp_bert, out)
    adam = optimizers.Adam(lr=lr)
    model.compile(optimizer=adam, loss=""binary_crossentropy"", metrics=[""accuracy""])
    return model
</code></pre>

<p>After fine tuning for my classification task, I want to save the model.</p>

<pre><code>model.save(""best_model.h5"")
</code></pre>

<p>However this raises a NotImplementedError:</p>

<pre><code>---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-55-8c5545f0cd9b&gt; in &lt;module&gt;()
----&gt; 1 model.save(""best_spam.h5"")
      2 # import transformers

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
    973     """"""
    974     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,
--&gt; 975                       signatures, options)
    976 
    977   def save_weights(self, filepath, overwrite=True, save_format=None):

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
    110           'or using `save_weights`.')
    111     hdf5_format.save_model_to_hdf5(
--&gt; 112         model, filepath, overwrite, include_optimizer)
    113   else:
    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)
     97 
     98   try:
---&gt; 99     model_metadata = saving_utils.model_metadata(model, include_optimizer)
    100     for k, v in model_metadata.items():
    101       if isinstance(v, (dict, list, tuple)):

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
    163   except NotImplementedError as e:
    164     if require_config:
--&gt; 165       raise e
    166 
    167   metadata = dict(

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
    160   model_config = {'class_name': model.__class__.__name__}
    161   try:
--&gt; 162     model_config['config'] = model.get_config()
    163   except NotImplementedError as e:
    164     if require_config:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in get_config(self)
    885     if not self._is_graph_network:
    886       raise NotImplementedError
--&gt; 887     return copy.deepcopy(get_network_config(self))
    888 
    889   @classmethod

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in get_network_config(network, serialize_layer_fn)
   1940           filtered_inbound_nodes.append(node_data)
   1941 
-&gt; 1942     layer_config = serialize_layer_fn(layer)
   1943     layer_config['name'] = layer.name
   1944     layer_config['inbound_nodes'] = filtered_inbound_nodes

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
    138   if hasattr(instance, 'get_config'):
    139     return serialize_keras_class_and_config(instance.__class__.__name__,
--&gt; 140                                             instance.get_config())
    141   if hasattr(instance, '__name__'):
    142     return instance.__name__

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in get_config(self)
    884   def get_config(self):
    885     if not self._is_graph_network:
--&gt; 886       raise NotImplementedError
    887     return copy.deepcopy(get_network_config(self))
    888 

NotImplementedError: 
</code></pre>

<p>I am aware that huggingface provides a model.save_pretrained() method for TFBertModel, but I prefer to wrap it in tf.keras.Model as I plan to add other components/features to this network. Can anyone suggest a solution to saving the current model?</p>
"
59701981,BERT tokenizer & model download,"<p>I`m beginner.. I'm working with Bert. However, due to the security of the company network, the following code does not receive the bert model directly.</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
model = BertForSequenceClassification.from_pretrained(""bert-base-multilingual-cased"", num_labels=2) 
</code></pre>

<p>So I think I have to download these files and enter the location manually.
But I'm new to this, and I'm wondering if it's simple to download a format like .py from github and put it in a location.</p>

<p>I'm currently using the bert model implemented by hugging face's pytorch, and the address of the source file I found is:</p>

<p><a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">https://github.com/huggingface/transformers</a></p>

<p>Please let me know if the method I thought is correct, and if so, what file to get.</p>

<p>Thanks in advance for the comment.</p>
"
59955402,getting word-level encodings from sub-word tokens encodings,"<p>I'm looking into using a pretrained BERT ('bert-base-uncased') model to extract contextualised word-level encodings from a bunch sentences.</p>

<p>Wordpiece tokenisation breaks down some of the words in my input into subword units. Possibly a trivial question, but I was wondering what would be the most sensible way to combine output encodings for subword tokens into word-level encodings.</p>

<p>Is averaging subword encodings a reasonable way to go? If not, is there any better alternative?</p>
"
60120043,Optimizer and scheduler for BERT fine-tuning,"<p>I'm trying to fine-tune a model with BERT (using <code>transformers</code> library), and I'm a bit unsure about the optimizer and scheduler.</p>

<p>First, I understand that I should use <code>transformers.AdamW</code> instead of Pytorch's version of it. Also, we should use a warmup scheduler as suggested in the paper, so the scheduler is created using <code>get_linear_scheduler_with_warmup</code> function from <code>transformers</code> package.</p>

<p>The main questions I have are:</p>

<ol>
<li><code>get_linear_scheduler_with_warmup</code> should be called with the warm up. Is it ok to use 2 for warmup out of 10 epochs? </li>
<li>When should I call <code>scheduler.step()</code>? If I do after <code>train</code>, the learning rate is zero for the first epoch. Should I call it for each batch?</li>
</ol>

<p>Am I doing something wrong with this?</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AdamW
from transformers.optimization import get_linear_scheduler_with_warmup

N_EPOCHS = 10

model = BertGRUModel(finetune_bert=True,...)
num_training_steps = N_EPOCHS+1
num_warmup_steps = 2
warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1

optimizer = AdamW(model.parameters())
criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([class_weights[1]]))


scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=num_warmup_steps, 
    num_training_steps=num_training_steps
)

for epoch in range(N_EPOCHS):
    scheduler.step() #If I do after train, LR = 0 for the first epoch
    print(optimizer.param_groups[0][""lr""])

    train(...) # here we call optimizer.step()
    evaluate(...)
</code></pre>

<p>My model and train routine(quite similar to <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"" rel=""noreferrer"">this notebook</a>)</p>

<pre class=""lang-py prettyprint-override""><code>class BERTGRUSentiment(nn.Module):
    def __init__(self,
                 bert,
                 hidden_dim,
                 output_dim,
                 n_layers=1, 
                 bidirectional=False,
                 finetune_bert=False,
                 dropout=0.2):

        super().__init__()

        self.bert = bert

        embedding_dim = bert.config.to_dict()['hidden_size']

        self.finetune_bert = finetune_bert

        self.rnn = nn.GRU(embedding_dim,
                          hidden_dim,
                          num_layers = n_layers,
                          bidirectional = bidirectional,
                          batch_first = True,
                          dropout = 0 if n_layers &lt; 2 else dropout)

        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)        
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):    
        #text = [batch size, sent len]

        if not self.finetune_bert:
            with torch.no_grad():
                embedded = self.bert(text)[0]
        else:
            embedded = self.bert(text)[0]
        #embedded = [batch size, sent len, emb dim]
        _, hidden = self.rnn(embedded)

        #hidden = [n layers * n directions, batch size, emb dim]

        if self.rnn.bidirectional:
            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))
        else:
            hidden = self.dropout(hidden[-1,:,:])

        #hidden = [batch size, hid dim]

        output = self.out(hidden)

        #output = [batch size, out dim]

        return output


import torch
from sklearn.metrics import accuracy_score, f1_score


def train(model, iterator, optimizer, criterion, max_grad_norm=None):
    """"""
    Trains the model for one full epoch
    """"""
    epoch_loss = 0
    epoch_acc = 0

    model.train()

    for i, batch in enumerate(iterator):
        optimizer.zero_grad()
        text, lens = batch.text

        predictions = model(text)

        target = batch.target

        loss = criterion(predictions.squeeze(1), target)

        prob_predictions = torch.sigmoid(predictions)

        preds = torch.round(prob_predictions).detach().cpu()
        acc = accuracy_score(preds, target.cpu())

        loss.backward()
        # Gradient clipping
        if max_grad_norm:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)


</code></pre>
"
60120849,Outputting attention for bert-base-uncased with huggingface/transformers (torch),"<p>I was following <a href=""https://www.aclweb.org/anthology/P19-1328/"" rel=""noreferrer"">a paper</a> on BERT-based lexical substitution (specifically trying to implement equation (2) - if someone has already implemented the whole paper that would also be great). Thus, I wanted to obtain both the last hidden layers (only thing I am unsure is the ordering of the layers in the output: last first or first first?) and the attention from a basic BERT model (bert-base-uncased). </p>

<p>However, I am a bit unsure whether the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">huggingface/transformers library</a> actually outputs the attention (I was using torch, but am open to using TF instead) for bert-base-uncased?</p>

<p>From <a href=""https://github.com/huggingface/transformers/issues/1073"" rel=""noreferrer"">what I had read</a>, I was expected to get a tuple of (logits, hidden_states, attentions), but with the example below (runs e.g. in Google Colab), I get of length 2 instead. </p>

<p>Am I misinterpreting what I am getting or going about this the wrong way? I did the obvious test and used <code>output_attention=False</code> instead of <code>output_attention=True</code> (while <code>output_hidden_states=True</code> does indeed seem to add the hidden states, as expected) and nothing change in the output I got. That's clearly a bad sign about my understanding of the library or indicates an issue.</p>

<pre><code>import numpy as np
import torch
!pip install transformers

from transformers import (AutoModelWithLMHead, 
                          AutoTokenizer, 
                          BertConfig)

bert_tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attention=True) # Nothign changes, when I switch to output_attention=False
bert_model = AutoModelWithLMHead.from_config(config)

sequence = ""We went to an ice cream cafe and had a chocolate ice cream.""
bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)

indexed_tokens = bert_tokenizer.encode(bert_tokenized_sequence, return_tensors='pt')

predictions = bert_model(indexed_tokens)

########## Now let's have a look at what the predictions look like #############
print(len(predictions)) # Length is 2, I expected 3: logits, hidden_layers, attention

print(predictions[0].shape) # torch.Size([1, 16, 30522]) - seems to be logits (shape is 1 x sequence length x vocabulary

print(len(predictions[1])) # Length is 13 - the hidden layers?! There are meant to be 12, right? Is one somehow the attention?

for k in range(len(predictions[1])):
  print(predictions[1][k].shape) # These all seem to be torch.Size([1, 16, 768]), so presumably the hidden layers?
</code></pre>

<h1>Explanation of what worked in the end inspired by accepted answer</h1>

<pre><code>import numpy as np
import torch
!pip install transformers

from transformers import BertModel, BertConfig, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attentions=True)
model = BertModel.from_pretrained('bert-base-uncased', config=config)
sequence = ""We went to an ice cream cafe and had a chocolate ice cream.""
tokenized_sequence = tokenizer.tokenize(sequence)
indexed_tokens = tokenizer.encode(tokenized_sequence, return_tensors='pt'
enter code here`outputs = model(indexed_tokens)
print( len(outputs) ) # 4 
print( outputs[0].shape ) #1, 16, 768 
print( outputs[1].shape ) # 1, 768
print( len(outputs[2]) ) # 13  = input embedding (index 0) + 12 hidden layers (indices 1 to 12)
print( outputs[2][0].shape ) # for each of these 13: 1,16,768 = input sequence, index of each input id in sequence, size of hidden layer
print( len(outputs[3]) ) # 12 (=attenion for each layer)
print( outputs[3][0].shape ) # 0 index = first layer, 1,12,16,16 = , layer, index of each input id in sequence, index of each input id in sequence
</code></pre>
"
60121768,while running huggingface gpt2-xl model embedding index getting out of range,"<p>I am trying to run <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#tfgpt2lmheadmodel"" rel=""nofollow noreferrer"">hugginface</a> gpt2-xl model. I ran code from the <a href=""https://huggingface.co/transformers/quickstart.html"" rel=""nofollow noreferrer"">quickstart</a> page that load the small gpt2 model and generate text by the following code: </p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
model = GPT2LMHeadModel.from_pretrained('gpt2')

generated = tokenizer.encode(""The Manhattan bridge"")
context = torch.tensor([generated])
past = None

for i in range(100):
    print(i)
    output, past = model(context, past=past)
    token = torch.argmax(output[0, :])

    generated += [token.tolist()]
    context = token.unsqueeze(0)

sequence = tokenizer.decode(generated)

print(sequence)
</code></pre>

<p>This is running perfectly. Then I try to run <code>gpt2-xl</code> model. 
I changed <code>tokenizer</code> and <code>model</code> loading code like following:
  tokenizer = GPT2Tokenizer.from_pretrained(""gpt2-xl"")
  model = GPT2LMHeadModel.from_pretrained('gpt2-xl')</p>

<p>The <code>tokenizer</code> and <code>model</code> loaded perfectly. But I a getting error on the following line:</p>

<pre><code>output, past = model(context, past=past)
</code></pre>

<p>The error is:</p>

<pre><code>RuntimeError: index out of range: Tried to access index 204483 out of table with 50256 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418
</code></pre>

<p>Looking at error it seems that the embedding size is not correct. So I write the following line to specifically fetch the config file of <code>gpt2-xl</code>:</p>

<pre><code>config = GPT2Config.from_pretrained(""gpt2-xl"")
</code></pre>

<p>But, here <code>vocab_size:50257</code>
So I changed explicitly the value by:</p>

<pre><code>config.vocab_size=204483
</code></pre>

<p>Then after printing the <code>config</code>, I can see that the previous line took effect in the configuration. But still, I am getting the same error. </p>
"
60133236,What does BERT's special characters appearance in SQuAD's QA answers mean?,"<p>I'm running a fine-tuned model of BERT and ALBERT for Questing Answering. And, I'm evaluating the performance of these models on a subset of questions from <a href=""https://rajpurkar.github.io/SQuAD-explorer/"" rel=""nofollow noreferrer"">SQuAD v2.0</a>. I use <a href=""https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/"" rel=""nofollow noreferrer"">SQuAD's official evaluation script</a> for evaluation. </p>

<p>I use Huggingface <code>transformers</code> and in the following you can find an actual code and example I'm running (might be also helpful for some folks who are trying to run fine-tuned model of ALBERT on SQuAD v2.0):</p>

<pre><code>tokenizer = AutoTokenizer.from_pretrained(""ktrapeznikov/albert-xlarge-v2-squad-v2"")
model = AutoModelForQuestionAnswering.from_pretrained(""ktrapeznikov/albert-xlarge-v2-squad-v2"")

question = ""Why aren't the examples of bouregois architecture visible today?""
text = """"""Exceptional examples of the bourgeois architecture of the later periods were not restored by the communist authorities after the war (like mentioned Kronenberg Palace and Insurance Company Rosja building) or they were rebuilt in socialist realism style (like Warsaw Philharmony edifice originally inspired by Palais Garnier in Paris). Despite that the Warsaw University of Technology building (1899\u20131902) is the most interesting of the late 19th-century architecture. Some 19th-century buildings in the Praga district (the Vistula\u2019s right bank) have been restored although many have been poorly maintained. Warsaw\u2019s municipal government authorities have decided to rebuild the Saxon Palace and the Br\u00fchl Palace, the most distinctive buildings in prewar Warsaw.""""""

input_dict = tokenizer.encode_plus(question, text, return_tensors=""pt"")
input_ids = input_dict[""input_ids""].tolist()
start_scores, end_scores = model(**input_dict)

all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]).replace('â–', '')
print(answer)
</code></pre>

<p>And the output is like the following:</p>

<pre><code>[CLS] why aren ' t the examples of bour ego is architecture visible today ? [SEP] exceptional examples of the  bourgeois architecture of the later periods were not restored by the communist authorities after the war
</code></pre>

<p>As you can see there are BERT's special tokens in the answer including <code>[CLS]</code> and <code>[SEP]</code>.</p>

<p>I understand that in cases where the answer is just <code>[CLS]</code> (having two <code>tensor(0)</code> for <code>start_scores</code> and <code>end_scores</code>) it basically means model thinks there's no answer to the question in context which makes sense. And in these cases I just simply set the answer to that question to a null string when running the evaluation script.</p>

<p><strong>But</strong> I wonder in cases like the example above, should I again assume that model could not find an answer and set the answer to empty string? or should I just leave the answer like that when I'm evaluating the model performance?</p>

<p>I'm asking this question because as far as I understand, the performance calculated using the evaluation script can change (correct me if I'm wrong) if I have such cases as answers and I may not get a realistic sense of the performance of these models. </p>
"
60137162,"Pre-training BERT/RoBERTa language model using domain text, how long it gonna take estimately? which is faster?","<p>I want to pre-train BERT and RoBERTa MLM using domain corpus (sentiment-related text). How long it gonna take for using 50k~100k words. Since RoBERTa is not trained on predicting the next sentence objective, one training objective less than BERT and with larger mini-batches and learning rates, I assume RoBERTa will be much faster?</p>
"
60142937,HuggingFace Transformers For Text Generation with CTRL with Google Colab's free GPU,"<p>I wanted to test TextGeneration with CTRL using PyTorch-Transformers, before using it for fine-tuning. But it doesn't prompt anything like it does with GPT-2 and other similar language generation models. I'm very new for this and am stuck and can't figure out what's going on. </p>

<p>This is the procedure I followed in my Colab notebook,</p>

<pre><code>!pip install transformers
</code></pre>

<pre><code>!git clone https://github.com/huggingface/pytorch-transformers.git
</code></pre>

<pre><code>!python pytorch-transformers/examples/run_generation.py \
    --model_type=ctrl \
    --length=100 \
    --model_name_or_path=ctrl \
    --temperature=0.2 \
    --repetition_penalty=1.2 \
</code></pre>

<p>And this is what I get after running the script</p>

<pre><code>02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   loading configuration file https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   Model config CTRLConfig {
  ""architectures"": null,
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 0,
  ""dff"": 8192,
  ""do_sample"": false,
  ""embd_pdrop"": 0.1,
  ""eos_token_ids"": 0,
  ""finetuning_task"": null,
  ""from_tf"": false,
  ""id2label"": {
    ""0"": ""LABEL_0""
  },
  ""initializer_range"": 0.02,
  ""is_decoder"": false,
  ""label2id"": {
    ""LABEL_0"": 0
  },
  ""layer_norm_epsilon"": 1e-06,
  ""length_penalty"": 1.0,
  ""max_length"": 20,
  ""model_type"": ""ctrl"",
  ""n_ctx"": 512,
  ""n_embd"": 1280,
  ""n_head"": 16,
  ""n_layer"": 48,
  ""n_positions"": 50000,
  ""num_beams"": 1,
  ""num_labels"": 1,
  ""num_return_sequences"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pad_token_id"": 0,
  ""pruned_heads"": {},
  ""repetition_penalty"": 1.0,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""temperature"": 1.0,
  ""top_k"": 50,
  ""top_p"": 1.0,
  ""torchscript"": false,
  ""use_bfloat16"": false,
  ""vocab_size"": 246534
}

02/10/2020 01:02:31 - INFO - transformers.modeling_utils -   loading weights file https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0
tcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9
tcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245
^C
</code></pre>

<p>and then terminates. Could this be because of a GPU problem?</p>

<p>Any sort of help is appreciated.</p>
"
60157959,Transformers summarization with Python Pytorch - how to get longer output?,"<p>I use Ai-powered summarization from <a href=""https://github.com/huggingface/transformers/tree/master/examples/summarization"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/summarization</a> - state of the art results.</p>

<p><strong>Should i train it myself to get summary output longer than used in original huggingface github training script?</strong>
:</p>

<pre><code>python run_summarization.py \
    --documents_dir $DATA_PATH \
    --summaries_output_dir $SUMMARIES_PATH \ # optional
    --no_cuda false \
    --batch_size 4 \
    --min_length 50 \
    --max_length 200 \
    --beam_size 5 \
    --alpha 0.95 \
    --block_trigram true \
    --compute_rouge true
</code></pre>

<p>When i do inference with </p>

<pre><code>--min_length 500 \
--max_length 600 \
</code></pre>

<p>I got a good output for 200 tokens, but the rest of the text is </p>

<pre><code>. . . [unused7] [unused7] [unused7] [unused8] [unused4] [unused7] [unused7]  [unused4] [unused7] [unused8]. [unused4] [unused7] . [unused4] [unused8] [unused4] [unused8].  [unused4]  [unused4] [unused8]  [unused4] . .  [unused4] [unused6] [unused4] [unused7] [unused6] [unused4] [unused8] [unused5] [unused4] [unused7] [unused4] [unused4] [unused7]. [unused4] [unused6]. [unused4] [unused4] [unused4] [unused8]  [unused4] [unused7]  [unused4] [unused8] [unused6] [unused4]   [unused4] [unused4]. [unused4].  [unused5] [unused4] [unused8] [unused7] [unused4] [unused7] [unused9] [unused4] [unused7]  [unused4] [unused7] [unused5] [unused4]  [unused5] [unused4] [unused6]  [unused4]. .  . [unused5]. [unused4]  [unused4]   [unused4] [unused6] [unused5] [unused4] [unused4]  [unused6] [unused4] [unused6]  [unused4] [unused4] [unused5] [unused4]. [unused5]  [unused4] . [unused4]  [unused4] [unused8] [unused8] [unused4]  [unused7] [unused4] [unused8]  [unused4] [unused7]  [unused4] [unused8]  [unused4]  [unused8] [unused4] [unused6] 
</code></pre>
"
60173639,Size of the training data of GPT2-XL pre-trained model,"<p>In <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface transformer</a>, it is possible to use the pre-trained GPT2-XL language model. But I don't find, on which dataset it is trained? Is it the same trained model which OpenAI used for their <a href=""https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"" rel=""nofollow noreferrer"">paper</a> (trained on 40GB dataset called <code>webtext</code>) ?</p>
"
60243099,What is the meaning of the second output of Huggingface's Bert?,"<p>Using the vanilla configuration of base BERT model in the huggingface implementation, I get a tuple of length 2.</p>

<pre><code>import torch

import transformers
from transformers import AutoModel,AutoTokenizer

bert_name=""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(bert_name)
BERT = AutoModel.from_pretrained(bert_name)

e=tokenizer.encode('I am hoping for the best', add_special_tokens=True)

q=BERT(torch.tensor([e]))

print (len(q)) #Output: 2
</code></pre>

<p>The first element is what I expect to receive - the 768 dimension embedding of each input token.</p>

<pre><code>print (e) #Output : [101, 1045, 2572, 5327, 2005, 1996, 2190, 102] 
print (q[0].shape) #Output : torch.Size([1, 8, 768])
</code></pre>

<p>But what is the second element in the tuple?</p>

<pre><code>print (q[1].shape) # torch.Size([1, 768])
</code></pre>

<p>It has the same size as the encoding of each token.
But what is it?</p>

<p>Maybe a copy of the [CLS] token, a representation for the classification of the entire encoded text?</p>

<p>Let's check.</p>

<pre><code>a= q[0][:,0,:]
b=q[1]

print (torch.eq(a,b)) #Output : Tensor([[False, False, False, .... False]])
</code></pre>

<p>Nope!</p>

<p>What about a copy the embedding of the last token (for whatever reason)?</p>

<pre><code>c= q[0][:,-1,:]
b=q[1]

print (torch.eq(a,c)) #Output : Tensor([[False, False, False, .... False]])
</code></pre>

<p>So, also not that.</p>

<p>The documentation talks about how changing the <code>config</code> can result in more tuple elements (like hidden states), but I did not find any description of this ""mysterious"" tuple element outputted by the default configuration.</p>

<p>Any ideas as to what is it and what is its usage?    </p>
"
60345277,Migrating from `pytorch-pretrained-bert` to `pytorch-transformers` issue regarding model() output,"<p>I'm having trouble migrating my code from <code>pytorch_pretrained_bert</code> to <code>pytorch_transformers</code>. I'm attempting to run a cosine similarity exercise. I want to extract text embeddings values of the second-to-last of the 12 hidden embedding layer. </p>

<pre><code>
import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel
#from pytorch_transofmers import BertTokenizer, BertModel
import pandas as pd
import numpy as np

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# This is done by default in the pytorch_transformers
model.eval() 

input_query = ""This is my test input query text""
marked_text = ""[CLS] "" + input_query + "" [SEP]""
tokenized_text = tokenizer.tokenize(marked_text)
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
segments_ids = [1] * len(tokenized_text)
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
with torch.no_grad():
    encoded_layers, _ = model(tokens_tensor, segments_tensors)
    sentence_embedding = torch.mean(encoded_layers[10], 1)

</code></pre>

<p>Using the pytorch_pretrained_bert works perfectly fine with the above code. My <code>encoded_layers</code> object is a list of 12 hidden layer tensors, allowing me to pick and reduce the 11th layer by taking an average, resulting in <code>sentence_embedding</code> object I can run cosine similarities against.</p>

<p>However, when I migrate my code to the <code>pytorch_transformers</code> library, the resulting <code>encoded_layers</code> object is no longer the full list of 12 hidden layers, but a single torch tensor object of shape <code>torch.Size([1, 7, 768])</code>, which results in the following error when I attempt to create the <code>sentence_embedding</code> object:</p>

<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-23-7f877a7d2f9c&gt; in &lt;module&gt;
      9         encoded_layers, _ = model(tokens_tensor, segments_tensors)
     10         test = encoded_layers[0]
---&gt; 11         sentence_embedding = torch.mean(test[10], 1)
     12 

IndexError: index 10 is out of bounds for dimension 0 with size 7
</code></pre>

<p>The migration documentation (<a href=""https://huggingface.co/transformers/migration.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/migration.html</a>) states that I should take the first element of the <code>encoded_layers</code> object as a replacement but that does not provide me with access to the second to last hidden layer of embeddings. </p>

<p>How can I access it?</p>

<p>Thank you!</p>
"
60378466,"If BERT's [CLS] can be retrained for a variety of sentence classification objectives, what about [SEP]?","<p>In BERT pretraining, the [CLS] token is embedded into the input of a classifier tasked with the Next Sentence Prediction task (or, in some BERT variants, with other tasks, such as ALBERT's Sentence Order Prediction); this helps in the pretraining of the entire transformer, and it also helps to make the [CLS] position readily available for retraining to other ""sentence scale"" tasks.</p>

<p>I wonder whether [SEP] could also be retrained in the same manner.
While [CLS] will probably be easier to retrain as the transformer is already trained to imbue its embedding with meaning from across the sentence, while [SEP] does not have these ""connections"" (one would assume), this might still work with sufficient fine-tuning. </p>

<p>With this one could retrain the same model for two different classification tasks,   one using [CLS] and one using [SEP].</p>

<p>Am I missing anything? 
Is there a reason why this would not work? </p>
"
60382793,What are the inputs to the transformer encoder and decoder in BERT?,"<p>I was reading the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a> and was not clear regarding the inputs to the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">transformer</a> encoder and decoder. </p>

<p>For learning masked language model (Cloze task), the paper says that 15% of the tokens are masked and the network is trained to predict the masked tokens. Since this is the case, what are the inputs to the transformer encoder and decoder?</p>

<p><a href=""https://i.stack.imgur.com/jIIuo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jIIuo.png"" alt=""BERT input representation (from the paper)""></a></p>

<p>Is the input to the transformer encoder this input representation (see image above). If so, what is the decoder input?</p>

<p>Further, how is the output loss computed? Is it a softmax for only the masked locations? For this, the same linear layer is used for all masked tokens?</p>
"
60459292,Using past and attention_mask at the same time for gpt2,"<p>I am processing a batch of sentences with different lengths, so I am planning to take advantage of the padding + attention_mask functionality in gpt2 for that. </p>

<p>At the same time, for each sentence I need to add a suffix phrase and run N different inferences. For instance, given the sentence ""I like to drink coke"", I may need to run two different inferences: ""I like to drink coke. Coke is good"" and ""I like to drink coke. Drink is good"". Thus, I am trying to improve the inference time for this by using the ""past"" functionality: <a href=""https://huggingface.co/transformers/quickstart.html#using-the-past"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/quickstart.html#using-the-past</a> so I just process the original sentence (e.g. ""I like to drink coke"") once, and then I somehow expand the result to be able to be used with two other sentences: ""Coke is good"" and ""Drink is good"". </p>

<p>Below you will find a simple code that is trying to represent how I was trying to do this. For simplicity I'm just adding a single suffix phrase per sentence (...but I still hope my original idea is possible though):</p>

<pre><code>from transformers.tokenization_gpt2 import GPT2Tokenizer
from transformers.modeling_gpt2 import GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='&lt;|endoftext|&gt;')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Complete phrases are: ""I like to drink soda without sugar"" and ""Go watch TV alone, I am not going""
docs = [""I like to drink soda"", ""Go watch TV""]
docs_tensors = tokenizer.batch_encode_plus(
    [d for d in docs], pad_to_max_length=True, return_tensors='pt')

docs_next = [""without sugar"", ""alone, I am not going""]
docs_next_tensors = tokenizer.batch_encode_plus(
    [d for d in docs_next], pad_to_max_length=True, return_tensors='pt')

# predicting the first part of each phrase
_, past = model(docs_tensors['input_ids'], attention_mask=docs_tensors['attention_mask'])

# predicting the rest of the phrase
logits, _ = model(docs_next_tensors['input_ids'], attention_mask=docs_next_tensors['attention_mask'], past=past)
logits = logits[:, -1]
_, top_indices_results = logits.topk(30)
</code></pre>

<p>The error I am getting is the following:</p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py"", line 1434, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/damiox/Workspace/xxLtd/yy/stress-test-withpast2.py"", line 26, in &lt;module&gt;
    logits, _ = model(docs_next_tensors['input_ids'], attention_mask=docs_next_tensors['attention_mask'], past=past)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 593, in forward
    inputs_embeds=inputs_embeds,
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 476, in forward
    hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 226, in forward
    self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 189, in forward
    attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 150, in _attn
    w = w + attention_mask
RuntimeError: The size of tensor a (11) must match the size of tensor b (6) at non-singleton dimension 3

Process finished with exit code 1
</code></pre>

<p>Initially I thought this was related to <a href=""https://github.com/huggingface/transformers/issues/3031"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/3031</a> - so I re-built latest master to try the fix, but I still experience the issue. </p>
"
60539758,biobert for keras version of huggingface transformers,"<p>(also posted in <a href=""https://github.com/dmis-lab/biobert/issues/98"" rel=""nofollow noreferrer"">https://github.com/dmis-lab/biobert/issues/98</a>)</p>

<p>Hi, does anyone know how to load biobert as a keras layer using the huggingface transformers (version 2.4.1)? I tried several possibilities but none of these worked. All that I found out is how to use the pytorch version but I am interested in the keras layer version. Below are two of my attempts (I saved the biobert files into folder ""biobert_v1.1_pubmed"").</p>

<h3>Attempt 1:</h3>

<pre><code>biobert_model = TFBertModel.from_pretrained('bert-base-uncased')
biobert_model.load_weights('biobert_v1.1_pubmed/model.ckpt-1000000')
</code></pre>

<p>Error message:</p>

<pre><code>AssertionError: Some objects had attributes which were not restored:
    : ['tf_bert_model_4/bert/embeddings/word_embeddings/weight']
    : ['tf_bert_model_4/bert/embeddings/position_embeddings/embeddings']
   (and many more lines like above...)
</code></pre>

<h3>Attempt 2:</h3>

<pre><code>biobert_model = TFBertModel.from_pretrained(""biobert_v1.1_pubmed/model.ckpt-1000000"", config='biobert_v1.1_pubmed/bert_config.json')
</code></pre>

<p>Error message:</p>

<pre><code>NotImplementedError: Weights may only be loaded based on topology into Models when loading TensorFlow-formatted weights (got by_name=True to load_weights).
</code></pre>

<p>Any help appreciated! My experience with huggingface's transformers library is almost zero. I also tried to load the following two models but it seems they only support the pytorch version.</p>

<ul>
<li><a href=""https://huggingface.co/monologg/biobert_v1.1_pubmed"" rel=""nofollow noreferrer"">https://huggingface.co/monologg/biobert_v1.1_pubmed</a></li>
<li><a href=""https://huggingface.co/adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers"" rel=""nofollow noreferrer"">https://huggingface.co/adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers</a></li>
</ul>
"
60551906,"Tensorflow - HuggingFace - Invalid argument: indices[0,624] = 624 is not in [0, 512)","<p>I'm running a code using Tensorflow's BERT in HuggingFace's transformers based on this tutorial:</p>

<p><a href=""https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/"" rel=""nofollow noreferrer"">Text Classification with BERT Tokenizer and TF 2.0 in Python</a></p>

<p>However, instead of creating my own neural net, I'm using transformers and:</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model0 = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
</code></pre>

<p>I am able to generate the following data for training:</p>

<pre><code>(&lt;tf.Tensor: id=6582, shape=(20, 70), dtype=int32, numpy=
 array([[  191, 19888,  1186,     0, ...,     0,     0,     0,     0],
        [ 7353,  1200,  2180,  1197, ...,     0,     0,     0,     0],
        [  164,   112, 12890,  5589, ...,     0,     0,     0,     0],
        [  164,   112, 21718, 19009, ...,     0,     0,     0,     0],
        ...,
        [ 7998,  3101,   164,   112, ...,     0,     0,     0,     0],
        [  164,   112,   154,  4746, ...,     0,     0,     0,     0],
        [  164,   112,  1842, 23228, ...,  1162,   112,   166,     0],
        [  164,   112,   140,  3814, ...,  7443,   119,   112,   166]], dtype=int32)&gt;,
 &lt;tf.Tensor: id=6583, shape=(20,), dtype=int32, numpy=array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=int32)&gt;)
</code></pre>

<p>But as long as I've seen, there must exist a problem with vocabulary file, that is not defined. I also get the following warning when running:</p>

<pre><code>train2=[]
for i in range(0,train.shape[0]):
    out=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(str(train.iloc[i,1])))
    print(i)
    train2.append(out)

WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (6935 &gt; 512). Running this sequence through the model will result in indexing errors
WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (3574 &gt; 512). Running this sequence through the model will result in indexing errors
</code></pre>

<p>The <code>model0</code> is successfully created:</p>

<pre><code>Model: ""tf_bert_for_sequence_classification""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  108310272 
_________________________________________________________________
dropout_37 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 108,311,810
Trainable params: 108,311,810
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>Then on:</p>

<pre><code>model0.fit(train_data, epochs=2, steps_per_epoch=30,validation_data=test_data, validation_steps=7)
</code></pre>

<p>I get the following error:</p>

<pre><code>Train for 1 steps
Epoch 1/2
1/1 [==============================] - 21s 21s/step
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-53-61d611c37004&gt; in &lt;module&gt;
----&gt; 1 history = model0.fit(train_data, epochs=2, steps_per_epoch=1)#,validation_data=test_data, validation_steps=7)

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--&gt; 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--&gt; 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--&gt; 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---&gt; 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--&gt; 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-&gt; 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--&gt; 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/.local/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  indices[0,624] = 624 is not in [0, 512)
     [[node tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embedding_lookup (defined at /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
  (1) Invalid argument:  indices[0,624] = 624 is not in [0, 512)
     [[node tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embedding_lookup (defined at /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
     [[GroupCrossDeviceControlEdges_0/Adam/Adam/Const/_867]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_36559]

Function call stack:
distributed_function -&gt; distributed_function
</code></pre>

<p>My data consists of a column of 2 classes and the other column are phrases.</p>

<p>What can I do ?</p>
"
60610280,BertForSequenceClassification vs. BertForMultipleChoice for sentence multi-class classification,"<p>I'm working on a text classification problem (e.g. sentiment analysis), where I need to classify a text string into one of five classes.</p>
<p>I just started using the <a href=""https://huggingface.co/transformers/index.html"" rel=""noreferrer"">Huggingface Transformer</a> package and BERT with PyTorch. What I need is a classifier with a softmax layer on top so that I can do 5-way classification. Confusingly, there seem to be two relevant options in the Transformer package: <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""noreferrer"">BertForSequenceClassification</a> and <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice"" rel=""noreferrer"">BertForMultipleChoice</a>.</p>
<p><strong>Which one should I use for my 5-way classification task? What are the appropriate use cases for them?</strong></p>
<p>The documentation for <strong>BertForSequenceClassification</strong> doesn't mention softmax at all, although it does mention cross-entropy. I am not sure if this class is only for 2-class classification (i.e. logistic regression).</p>
<blockquote>
<p><em>Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.</em></p>
<ul>
<li><em><strong>labels</strong> (torch.LongTensor of shape (batch_size,), optional, defaults to None) â€“ Labels for computing the sequence classification/regression loss. Indices should be in [0, ..., config.num_labels - 1]. If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels &gt; 1 a classification loss is computed (Cross-Entropy).</em></li>
</ul>
</blockquote>
<p>The documentation for <strong>BertForMultipleChoice</strong> mentions softmax, but the way the labels are described, it sound like this class is for multi-label classification (that is, a binary classification for multiple labels).</p>
<blockquote>
<p><em>Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.</em></p>
<ul>
<li><em><strong>labels</strong> (torch.LongTensor of shape (batch_size,), optional, defaults to None) â€“ Labels for computing the multiple choice classification loss. Indices should be in [0, ..., num_choices] where num_choices is the size of the second dimension of the input tensors.</em></li>
</ul>
</blockquote>
<p>Thank you for any help.</p>
"
60710606,Why should I call a BERT module instance rather than the forward method?,"<p>I'm trying to extract vector-representations of text using BERT in the transformers libray, and have stumbled on the following part of the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertmodel"" rel=""nofollow noreferrer"">documentation</a> for the ""BERTModel"" class:</p>

<p><a href=""https://i.stack.imgur.com/74fta.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/74fta.png"" alt=""enter image description here""></a></p>

<p>Can anybody explain this in more detail? A forward-pass makes intuitive sense to me (am trying to get final hidden states after all), and I can't find any additional information on what ""pre and post processing"" means in this context.</p>

<p>Thanks up front!</p>
"
60780181,Access the output of several layers of pretrained DistilBERT model,"<p>I am trying to access the output embeddings from several different layers of the pretrained ""DistilBERT"" model. (""distilbert-base-uncased"")</p>

<pre><code>bert_output = model(input_ids, attention_mask=attention_mask)
</code></pre>

<p>The bert_output seems to return only the embedding values of the last layer for the input tokens.</p>
"
60832547,Where is perplexity calculated in the Huggingface gpt2 language model code?,"<p>I see some github comments saying the output of the model() call's loss is in the form of perplexity:
<a href=""https://github.com/huggingface/transformers/issues/473"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/473</a></p>

<p>But when I look at the relevant code...
<a href=""https://huggingface.co/transformers/_modules/transformers/modeling_openai.html#OpenAIGPTLMHeadModel.forward"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/modeling_openai.html#OpenAIGPTLMHeadModel.forward</a></p>

<pre><code>    if labels is not None:
        # Shift so that tokens &lt; n predict n
        shift_logits = lm_logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        # Flatten the tokens
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        outputs = (loss,) + outputs

    return outputs  # (loss), lm_logits, (all hidden states), (all attentions)
</code></pre>

<p>I see cross entropy being calculated, but no transformation into perplexity. Where does the loss finally get transformed? Or is there a transformation already there that I'm not understanding?</p>
"
60833301,Train huggingface's GPT2 from scratch : assert n_state % config.n_head == 0 error,"<p>I am trying to use a GPT2 architecture for musical applications and consequently need to train it from scratch. After a bit of googling I found that the issue #1714 from huggingface's github already had ""solved"" the question. When I try the to run the propose solution :</p>

<pre><code>from transformers import GPT2Config, GPT2Model

NUMLAYER = 4
NUMHEAD = 4
SIZEREDUCTION = 10 #the factor by which we reduce the size of the velocity argument.
VELSIZE = int(np.floor(127/SIZEREDUCTION)) + 1 
SEQLEN=40 #size of data sequences.
EMBEDSIZE = 5 

config = GPT2Config(vocab_size = VELSIZE, n_positions = SEQLEN, n_embd = EMBEDSIZE, n_layer = NUMLAYER, n_ctx = SEQLEN, n_head = NUMHEAD)  
model = GPT2Model(config)
</code></pre>

<p>I get the following error : </p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-7-b043a7a2425f&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py', wdir='C:/Users/cnelias/Desktop/PHD/Swing project/code/script')

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py"", line 191, in &lt;module&gt;
    model = GPT2Model(config)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in __init__
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in &lt;listcomp&gt;
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 223, in __init__
    self.attn = Attention(nx, n_ctx, config, scale)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 109, in __init__
    assert n_state % config.n_head == 0
</code></pre>

<p>What does it mean and how can I solve it ?</p>

<p>Also more generally, is there a documentation on how to do a forward call with the GPT2 ? Can I define my own <code>train()</code> function or do I have to use the model's build-in function ? Am I forced to use a <code>Dataset</code> to do the training or can I feed it individual tensors ? 
I looked for it but couldn't find answer to these on the doc, but maybe I missed something.</p>

<p>PS : I already read the blogpost fron huggingface.co, but it omits too much informations and details to be usefull for my application.</p>
"
60839967,Keyword arguments in BERT call function,"<p>In the HuggingFace TensorFlow 2.0 BERT library, the <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""nofollow noreferrer"">documentation</a> states that:</p>

<blockquote>
  <p>TF 2.0 models accepts two formats as inputs:</p>
  
  <ul>
  <li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
  <li><p>having all inputs as a list, tuple or dict in the first positional
  arguments.</p></li>
  </ul>
</blockquote>

<p>I'm trying to use the first of these two to call a BERT model I created:</p>

<pre><code>from transformers import BertTokenizer, TFBertModel
import tensorflow as tf

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

text = ['This is a sentence', 
        'The sky is blue and the grass is green', 
        'More words are here']
labels = [0, 1, 0]
tokenized_text = bert_tokenizer.batch_encode_plus(batch_text_or_text_pairs=text,
                                                  pad_to_max_length=True,
                                                  return_tensors='tf')
dataset = tf.data.Dataset.from_tensor_slices((tokenized_text['input_ids'],
                                              tokenized_text['attention_mask'],
                                              tokenized_text['token_type_ids'],
                                              tf.constant(labels))).batch(3)
sample = next(iter(dataset))

result1 = bert_model(inputs=(sample[0], sample[1], sample[2]))  # works fine
result2 = bert_model(inputs={'input_ids': sample[0], 
                             'attention_mask': sample[1], 
                             'token_type_ids': sample[2]})  # also fine
result3 = bert_model(input_ids=sample[0], 
                     attention_mask=sample[1], 
                     token_type_ids=sample[2])  # raises an error
</code></pre>

<p>But when I execute the last line, I get an error:</p>

<pre><code>TypeError: __call__() missing 1 required positional argument: 'inputs'
</code></pre>

<p>Could someone please explain how to properly use the keyword argument style of inputs?</p>
"
60847291,Confusion in understanding the output of BERTforTokenClassification class from Transformers library,"<p>It is the example given in the documentation of transformers pytorch library</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForTokenClassification
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForTokenClassification.from_pretrained('bert-base-uncased', 
                      output_hidden_states=True, output_attentions=True)

input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", 
                         add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)

loss, scores, hidden_states,attentions = outputs
</code></pre>

<p>Here <code>hidden_states</code> is a tuple of length 13 and contains hidden-states of the model at the output of each layer plus the initial embedding outputs. I would like to know, <strong>whether hidden_states[0] or hidden_states[12] represent the final hidden state vectors</strong>?</p>
"
60867353,Using LIME for BERT transformer visualization results in memory error,"<p><strong>Situation</strong>:  I am currently working on visualizing the results of a huggingface transformers machine learning model I have been building using the <a href=""https://github.com/marcotcr/lime"" rel=""nofollow noreferrer"">LIME package</a> following <a href=""https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html"" rel=""nofollow noreferrer"">this tutorial</a>. </p>

<p><strong>Complication</strong>: My code is set up and runs well until I create the LIME <em>explainer</em> object. At this point I get a memory error.</p>

<p><strong>Question</strong>: What am I doing wrong? Why am I running into a memory error?</p>

<p><strong>Code</strong>: Here is my code (you should be able to just copy-paste this into google colab and run the whole thing)</p>

<pre class=""lang-py prettyprint-override""><code>########################## LOAD PACKAGES ######################
# Install new packages in our environment
!pip install lime
!pip install wget
!pip install transformers

# Import general libraries
import sklearn
import sklearn.ensemble
import sklearn.metrics
import numpy as np
import pandas as pd

# Import libraries specific to this notebook
import lime
import wget
import os
from __future__ import print_function
from transformers import FeatureExtractionPipeline, BertModel, BertTokenizer, BertConfig
from lime.lime_text import LimeTextExplainer

# Let the notebook know to plot inline
%matplotlib inline

########################## LOAD DATA ##########################
# Get URL
url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'

# Download the file (if we haven't already)
if not os.path.exists('./cola_public_1.1.zip'):
    wget.download(url, './cola_public_1.1.zip')

# Unzip the dataset (if we haven't already)
if not os.path.exists('./cola_public/'):
    !unzip cola_public_1.1.zip

# Load the dataset into a pandas dataframe.
df_cola = pd.read_csv(""./cola_public/raw/in_domain_train.tsv"", delimiter='\t', 
                      header=None, names=['sentence_source', 'label', 
                                          'label_notes', 'sentence'])

# Only look at the first 50 observations for debugging
df_cola = df_cola.head(50)

###################### TRAIN TEST SPLIT ######################
# Apply the train test split
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(
    df_cola.sentence, df_cola.label, test_size=0.2, random_state=42
)

###################### CREATE LIME CLASSIFIER ######################
# Create a function to extract vectors from a single sentence
def vector_extractor(sentence):

    # Create a basic BERT model, config and tokenizer for the pipeline
    configuration = BertConfig()
    configuration.max_len = 64
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',
                                              do_lower_case=True, 
                                              max_length=64,
                                              pad_to_max_length=True)
    model = BertModel.from_pretrained('bert-base-uncased',config=configuration)

    # Create the pipeline
    vector_extractor = FeatureExtractionPipeline(model=model,
                                                 tokenizer=tokenizer,
                                                 device=0)

    # The pipeline gives us all tokens in the final layer - we want the CLS token
    vector = vector_extractor(sentence)
    vector = vector[0][0]

    # Return the vector
    return vector

# Adjust the format of our sentences (from pandas series to python list)
x_train = x_train.values.tolist()
x_test = x_test.values.tolist()

# First we vectorize our train features for the classifier
x_train_vectorized = [vector_extractor(x) for x in x_train]

# Create and fit the random forest classifier
rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100)
rf.fit(x_train_vectorized, y_train)

# Define the lime_classifier function
def lime_classifier(sentences): 

    # Turn all the sentences into vectors
    vectors = [vector_extractor(x) for x in sentences]

    # Get predictions for all 
    predictions = rf.predict_proba(vectors)

    # Return the probabilies as a 2D-array
    return predictions  

########################### APPLY LIME ##########################
# Create the general explainer object
explainer = LimeTextExplainer()

# ""Fit"" the explainer object to a specific observation
exp = explainer.explain_instance(x_test[1], 
                                 lime_classifier, 
                                 num_features=6)
</code></pre>
"
60876394,Does BertForSequenceClassification classify on the CLS vector?,"<p>I'm using the <a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">Huggingface Transformer</a> package and BERT with PyTorch. I'm trying to do 4-way sentiment classification and am using <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> to build a model that eventually leads to a 4-way softmax at the end.</p>

<p>My understanding from reading the BERT paper is that the final dense vector for the input <code>CLS</code> token serves as a representation of the whole text string:</p>

<blockquote>
  <p>The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.</p>
</blockquote>

<p>So, does <code>BertForSequenceClassification</code> actually train and use this vector to perform the final classification?</p>

<p>The reason I ask is because when I <code>print(model)</code>, it is not obvious to me that the <code>CLS</code> vector is being used. </p>

<pre class=""lang-py prettyprint-override""><code>model = BertForSequenceClassification.from_pretrained(
    model_config,
    num_labels=num_labels,
    output_attentions=False,
    output_hidden_states=False
)

print(model)
</code></pre>

<p>Here is the bottom of the output:</p>

<pre><code>        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
</code></pre>

<p>I see that there is a pooling layer <code>BertPooler</code> leading to a <code>Dropout</code> leading to a <code>Linear</code> which presumably performs the final 4-way softmax. However, the use of the <code>BertPooler</code> is not clear to me. Is it operating on only the hidden state of <code>CLS</code>, or is it doing some kind of pooling over hidden states of all the input tokens?</p>

<p>Thanks for any help.</p>
"
60914793,"Argument ""never_split"" not working on bert tokenizer","<p>I used the <code>never_split</code> option and tried to retain some tokens. But the tokenizer still divide them into wordpieces. </p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', never_split=['lol'])
tokenizer.tokenize(""lol That's funny"")
['lo', '##l', 'that', ""'"", 's', 'funny']
</code></pre>

<p>Do I miss anything here?</p>
"
60923314,"I fine tuned a pre-trained BERT for sentence classification, but i cant get it to predict for new sentences","<p>below is the result of my fine-tuning.</p>

<pre><code>Training Loss   Valid. Loss Valid. Accur.   Training Time   Validation Time
epoch                   
1   0.16    0.11    0.96    0:02:11 0:00:05
2   0.07    0.13    0.96    0:02:19 0:00:05
3   0.03    0.14    0.97    0:02:22 0:00:05
4   0.02    0.16    0.96    0:02:21 0:00:05
</code></pre>

<p>next i tried to use the model to predict labels from a csv file. i created a label column, set the type to int64 and run the prediction.</p>

<pre><code>print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))
model.eval()
# Tracking variables 
predictions , true_labels = [], []
# Predict 
for batch in prediction_dataloader:
  # Add batch to GPU
  batch = tuple(t.to(device) for t in batch)

  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels = batch

  # Telling the model not to compute or store gradients, saving memory and 
  # speeding up prediction
  with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]

  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()

  # Store predictions and true labels
  predictions.append(logits)
  true_labels.append(label_ids)


</code></pre>

<p>however, while i am able to print out the predictions[4.235, -4.805] etc, and the true_labels[NaN,NaN.....], i am unable to actually get the predicted labels{0 or 1}. Am i missing something here?</p>
"
61000500,Tensorflow/Keras/BERT MultiClass Text Classification Accuracy,"<p>I'm attempting to fine-tune the HuggingFace TFBertModel to be able to classify some text to a single label. I have the model up and running, however the accuracy is extremely low from the start. My expectation is that the accuracy would be high given that it is using the BERT pre-trained weights as a starting point. I was hoping to get some advice on where I'm going wrong.</p>

<p>I'm using the bbc-text dataset from <a href=""https://www.kaggle.com/yufengdev/bbc-text-categorization"" rel=""nofollow noreferrer"">here</a>:</p>

<p><strong>Load Data</strong></p>

<pre><code>df = pd.read_csv(open(&lt;s3 url&gt;),encoding='utf-8', error_bad_lines=False)
df = df.sample(frac=1)
df = df.dropna(how='any')
</code></pre>

<p><strong>Value Counts</strong></p>

<pre><code>sport            511
business         510
politics         417
tech             401
entertainment    386
Name: label, dtype: int64
</code></pre>

<p><strong>Preprocessing</strong></p>

<pre><code>def preprocess_text(sen):
# Convert html entities to normal
sentence = unescape(sen)

# Remove html tags
sentence = remove_tags(sentence)

# Remove newline chars
sentence = remove_newlinechars(sentence)

# Remove punctuations and numbers
sentence = re.sub('[^a-zA-Z]', ' ', sentence)

# Convert to lowercase
sentence = sentence.lower()

return sentence


def remove_newlinechars(text):
    return "" "".join(text.splitlines()) 

def remove_tags(text):
    TAG_RE = re.compile(r'&lt;[^&gt;]+&gt;')
    return TAG_RE.sub('', text)

df['text_prepd'] = df['text'].apply(preprocess_text)
</code></pre>

<p><strong>Split Data</strong></p>

<pre><code>train, val = train_test_split(df, test_size=0.30, shuffle=True, stratify=df['label'])
</code></pre>

<p><strong>Encode Labels</strong></p>

<pre><code>from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_train = np.asarray(le.fit_transform(train['label']))
y_val = np.asarray(le.fit_transform(val['label']))

</code></pre>

<p><strong>Define BERT input function</strong></p>

<pre><code># Initialise Bert Tokenizer
bert_tokenizer_transformer = BertTokenizer.from_pretrained('bert-base-cased')

def create_input_array(df, tokenizer, args):
    sentences = df.text_prepd.values

    input_ids = []
    attention_masks = []
    token_type_ids = []

    for sent in tqdm(sentences):
        # `encode_plus` will:
        #   (1) Tokenize the sentence.
        #   (2) Prepend the `[CLS]` token to the start.
        #   (3) Append the `[SEP]` token to the end.
        #   (4) Map tokens to their IDs.
        #   (5) Pad or truncate the sentence to `max_length`
        #   (6) Create attention masks for [PAD] tokens.
        encoded_dict = tokenizer.encode_plus(
            sent,  # Sentence to encode.
            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
            max_length=args.max_seq_len,  # Pad &amp; truncate all sentences.
                pad_to_max_length=True,
                return_attention_mask=True,  # Construct attn. masks.
                return_tensors='tf',  # Return tf tensors.
            )

        # Add the encoded sentence to the list.
        input_ids.append(encoded_dict['input_ids'])

        # And its attention mask (simply differentiates padding from non-padding).
        attention_masks.append(encoded_dict['attention_mask'])

        token_type_ids.append(encoded_dict['token_type_ids'])

    input_ids = tf.convert_to_tensor(input_ids)
    attention_masks = tf.convert_to_tensor(attention_masks)
    token_type_ids = tf.convert_to_tensor(token_type_ids)

    return input_ids, attention_masks, token_type_ids

</code></pre>

<p><strong>Convert Data to Bert Inputs</strong></p>

<pre><code>train_inputs = [create_input_array(train[:], tokenizer=tokenizer, args=args)]
val_inputs = [create_input_array(val[:], tokenizer=tokenizer, args=args)]
</code></pre>

<p>For <code>train_inputs, y_train</code> and <code>val_inputs, y_val</code> I then apply the below function which reshapes and converts to numpy arrays. The returned list from this function is then passed as arguments to the keras fit method. I realise this is a bit overkill converting to tf.tensors then to numpy, but I don't think this has an impact. I was originally trying to use tf.datasets but switched to numpy.</p>

<pre><code>def convert_inputs_to_tf_dataset(inputs,y, args):
    # args.max_seq_len = 256
    ids = inputs[0][1]
    masks = inputs[0][1]
    token_types = inputs[0][2]

    ids = tf.reshape(ids, (-1, args.max_seq_len))
    print(""Input ids shape: "", ids.shape)
    masks = tf.reshape(masks, (-1, args.max_seq_len))
    print(""Input Masks shape: "", masks.shape)
    token_types = tf.reshape(token_types, (-1, args.max_seq_len))
    print(""Token type ids shape: "", token_types.shape)

    ids=ids.numpy()
    masks = masks.numpy()
    token_types = token_types.numpy()

    return [ids, masks, token_types, y]
</code></pre>

<p><strong>Keras Model</strong></p>

<pre><code># args.max_seq_len = 256
# n_classes = 6
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', trainable=True, num_labels=n_classes)

input_ids_layer = Input(shape=(args.max_seq_len, ), dtype=np.int32)
input_mask_layer = Input(shape=(args.max_seq_len, ), dtype=np.int32)
input_token_type_layer = Input(shape=(args.max_seq_len,), dtype=np.int32)

bert_layer = model([input_ids_layer, input_mask_layer, input_token_type_layer])[0]
flat_layer = Flatten()(bert_layer)
dropout= Dropout(0.3)(flat_layer)
dense_output = Dense(n_classes, activation='softmax')(dropout)

model_ = Model(inputs=[input_ids_layer, input_mask_layer, input_token_type_layer], outputs=dense_output)

</code></pre>

<p><strong>Compile and Fit</strong></p>

<pre><code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer='adam', loss=loss, metrics=[metric])
model.fit(inputs=..., outputs=..., validation_data=..., epochs=50, batch_size = 32, metrics=metric, verbose=1)


Epoch 32/50
1401/1401 [==============================] - 42s 30ms/sample - loss: 1.6103 - accuracy: 0.2327 - val_loss: 1.6042 -
 val_accuracy: 0.2308
</code></pre>

<p>As I'm using BERT, only a few epochs are necessary, so I was expecting something much higher than 23% after 32 epochs.</p>
"
61019485,Pytorch cross entropy input dimensions,"<p>I'm trying to develop a binary classifier with Huggingface's BertModel and Pytorch. 
The classifier module is something like this:  </p>

<pre><code>class SSTClassifierModel(nn.Module):

  def __init__(self, num_classes = 2, hidden_size = 768):
    super(SSTClassifierModel, self).__init__()
    self.number_of_classes = num_classes
    self.dropout = nn.Dropout(0.01)
    self.hidden_size = hidden_size
    self.bert = BertModel.from_pretrained('bert-base-uncased')
    self.classifier = nn.Linear(hidden_size, num_classes)

  def forward(self, input_ids, att_masks,token_type_ids,  labels):
    _, embedding = self.bert(input_ids, token_type_ids, att_masks)
    output = self.classifier(self.dropout(embedding))
    return output
</code></pre>

<p>The way I train the model is as follows:  </p>

<pre><code>loss_function = BCELoss()
model.train()
for epoch in range(NO_OF_EPOCHS):
  for step, batch in enumerate(train_dataloader):
        input_ids = batch[0].to(device)
        input_mask = batch[1].to(device)
        token_type_ids = batch[2].to(device)
        labels = batch[3].to(device)
        # assuming batch size = 3, labels is something like:
        # tensor([[0],[1],[1]])
        model.zero_grad()        
        model_output = model(input_ids,  
                             input_mask, 
                             token_type_ids,
                             labels)
        # model output is something like: (with batch size = 3) 
        # tensor([[ 0.3566, -0.0333],
                 #[ 0.1154,  0.2842],
                 #[-0.0016,  0.3767]], grad_fn=&lt;AddmmBackward&gt;)

        loss = loss_function(model_output.view(-1,2) , labels.view(-1))
</code></pre>

<p>I'm doing the <code>.view()</code>s because of the Huggingface's source code for <code>BertForSequenceClassification</code> <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForSequenceClassification"" rel=""nofollow noreferrer"">here</a> which uses the exact same way to compute the loss. But I get this error:</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in binary_cross_entropy(input, target, weight, size_average, reduce, reduction)
   2068     if input.numel() != target.numel():
   2069         raise ValueError(""Target and input must have the same number of elements. target nelement ({}) ""
-&gt; 2070                          ""!= input nelement ({})"".format(target.numel(), input.numel()))
   2071 
   2072     if weight is not None:

ValueError: Target and input must have the same number of elements. target nelement (3) != input nelement (6)
</code></pre>

<p>Is there something wrong with my labels? or my model's output? I'm really stuck here. The documentation for Pytorch's BCELoss says:  </p>

<p><em>Input: (N,âˆ—) where âˆ— means, any number of additional dimensions<br>
Target: (N,âˆ—), same shape as the input</em></p>

<p>How should I make my labels the same shape as the model output? I feel like there's something huge that I'm missing but I can't find it. </p>
"
61072673,Reduce the number of hidden units in hugging face transformers (BERT),"<p>I have been given a large csv each line of which is a set of BERT tokens made with hugging face BertTokenizer (<a href=""https://huggingface.co/transformers/main_classes/tokenizer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/tokenizer.html</a>).
1 line of this file looks as follows:</p>

<p><code>101, 108, 31278, 90939, 70325, 196, 199, 71436, 10107, 29190, 10107, 106, 16680, 68314, 10153, 17015, 15934, 10104, 108, 10233, 12396, 14945, 10107, 10858, 11405, 13600, 13597, 169, 57343, 64482, 119, 119, 119, 100, 11741, 16381, 10109, 68830, 10110, 20886, 108, 10233, 11127, 21768, 100, 14120, 131, 120, 120, 188, 119, 11170, 120, 12132, 10884, 10157, 11490, 12022, 10113, 10731, 10729, 11565, 14120, 131, 120, 120, 188, 119, 11170, 120, 162, 11211, 11703, 12022, 11211, 10240, 44466, 100886, 102</code></p>

<p>and there are 9 million lines like this</p>

<p>Now, I am trying to get embeddings from these tokens like this:</p>

<pre><code>def embedding:
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
    model = BertModel.from_pretrained('bert-base-multilingual-cased')
    input_ids = torch.tensor([101, 108, 31278, 90939, 70325, 196, 199, 71436, 10107, 29190, 10107, 106, 16680, 68314, 10153, 17015, 15934, 10104, 108, 10233, 12396, 14945, 10107, 10858, 11405, 13600, 13597, 169, 57343, 64482, 119, 119, 119, 100, 11741, 16381, 10109, 68830, 10110, 20886, 108, 10233, 11127, 21768, 100, 14120, 131, 120, 120, 188, 119, 11170, 120, 12132, 10884, 10157, 11490, 12022, 10113, 10731, 10729, 11565, 14120, 131, 120, 120, 188, 119, 11170, 120, 162, 11211, 11703, 12022, 11211, 10240, 44466, 100886, 102]).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[0][0][0]  # The last hidden-state is the first element of the output tuple
</code></pre>

<p>Output of this is embedding correspending to the line. The size is 768*1 tensor. Semantically, everything is ok. But, when I do this for the full file the output is 768 * 9,0000,000 <code>torch tensors</code>. So I get a memory error even with large machine with a 768 GB of RAM.
Here is how I call this function: 
<code>tokens['embeddings'] = tokens['text_tokens'].apply(lambda x: embedding(x))</code></p>

<p><code>tokens</code> is the pandas data frame with 9 million lines each of which contains BERT tokens.</p>

<p>Is it possible to reduce the default size of the hidden units, which is 768 here: <a href=""https://huggingface.co/transformers/main_classes/model.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/model.html</a></p>

<p>Thank you for your help.</p>
"
61107371,Transformer Pipeline for NER returns partial words with ##s,"<p>How should I interpret the partial words with '##'s in them returned by the Transformer NER pipelines? Other tools like Flair and SpaCy return the word and their tag. I have worked with the CONLL dataset before and never noticed anything like this. Moreover, why are words being divided like this?</p>

<p>Example from the HuggingFace:</p>

<pre><code>from transformers import pipeline

nlp = pipeline(""ner"")

sequence = ""Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very"" \
           ""close to the Manhattan Bridge which is visible from the window.""

print(nlp(sequence))
</code></pre>

<p>Output:</p>

<pre><code>[
    {'word': 'Hu', 'score': 0.9995632767677307, 'entity': 'I-ORG'},
    {'word': '##gging', 'score': 0.9915938973426819, 'entity': 'I-ORG'},
    {'word': 'Face', 'score': 0.9982671737670898, 'entity': 'I-ORG'},
    {'word': 'Inc', 'score': 0.9994403719902039, 'entity': 'I-ORG'},
    {'word': 'New', 'score': 0.9994346499443054, 'entity': 'I-LOC'},
    {'word': 'York', 'score': 0.9993270635604858, 'entity': 'I-LOC'},
    {'word': 'City', 'score': 0.9993864893913269, 'entity': 'I-LOC'},
    {'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},
    {'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},
    {'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},
    {'word': 'Manhattan', 'score': 0.9758241176605225, 'entity': 'I-LOC'},
    {'word': 'Bridge', 'score': 0.990249514579773, 'entity': 'I-LOC'}
]
</code></pre>
"
61121982,Asking gpt-2 to finish sentence with huggingface transformers,"<p>I am currently generating text from left context using the example script <code>run_generation.py</code> of the huggingface transformers library with gpt-2:</p>

<pre class=""lang-sh prettyprint-override""><code>$ python transformers/examples/run_generation.py \
  --model_type gpt2 \
  --model_name_or_path gpt2 \
  --prompt ""Hi, "" --length 5

=== GENERATED SEQUENCE 1 ===
Hi,  could anyone please inform me
</code></pre>

<p>I would like to generate short complete sentences. Is there any way to tell the model to finish a sentence before <code>length</code> words?</p>

<hr>

<p>Note: I don't mind changing model, but would prefer an auto-regressive one.</p>
"
61124443,Using huggingface fill-mask pipeline to get more than 5 suggestions,"<p>The below lets me get 5 suggestions for the masked token, but i'd like to get 10 suggestions - does anyone know if this is possible with hugging face?</p>

<pre><code>!pip install -q transformers
from __future__ import print_function
import ipywidgets as widgets
from transformers import pipeline

nlp_fill = pipeline('fill-mask')
nlp_fill(""I am going to guess &lt;mask&gt; in this sentence"")
</code></pre>
"
61127158,Identifying the word picked by hugging face pipeline fill-mask,"<p>I want to use hugging face's fill-mask pipeline to guess a masked token and then extract just the guessed token as a word.  This code should do that:</p>

<pre><code>!pip install -q transformers
model = pipeline('fill-mask')
outcome = model(""Kubernetes is a container orchestration &lt;mask&gt;"")[0]

#Prints: ""Kubernetes is a container orchestration platform"" 
print(outcome['sequence']) 

token = outcome['token'] 

#Prints: 1761
print(token)

#Prints: Ä platform 
print(model.tokenizer.convert_ids_to_tokens(token))
</code></pre>

<p>But I am finding that it gives me back <code>""Ä platform""</code> instead of <code>""platform""</code> - does anyone know why this is or what can be going on here?</p>
"
61134275,Difficulty in understanding the tokenizer used in Roberta model,"<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel, AutoTokenizer

tokenizer1 = AutoTokenizer.from_pretrained(""roberta-base"")
tokenizer2 = AutoTokenizer.from_pretrained(""bert-base-cased"")

sequence = ""A Titan RTX has 24GB of VRAM""
print(tokenizer1.tokenize(sequence))
print(tokenizer2.tokenize(sequence))
</code></pre>

<p>Output:</p>

<p>['A', 'Ä Titan', 'Ä RTX', 'Ä has', 'Ä 24', 'GB', 'Ä of', 'Ä VR', 'AM']</p>

<p>['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']</p>

<p>Bert model uses WordPiece tokenizer. Any word that does not occur in the WordPiece vocabulary is broken down into sub-words greedily. For example, 'RTX' is broken into 'R', '##T' and '##X' where ## indicates it is a subtoken. </p>

<p>Roberta uses BPE tokenizer but I'm unable to understand </p>

<p>a) how BPE tokenizer works? </p>

<p>b) what does G represents in each of tokens?</p>
"
61134980,Trying to adapt Pre-Trained BERT to another use case of semantic separation of sentences,"<p>i have used huggingface BERT for sentence classification with very good results, but now i want to apply it to another use case. Below is the kind of dataset(not exact) i have in mind.</p>

<pre><code> set_df.head()
</code></pre>

<pre><code>    sentence                                subject                   object
0   my big red dog has a big fat bone       my big red dog          big fat bone
1   The Queen of Spades lives in a Castle   The Queen of spades     lives in a castle

</code></pre>

<p>I have a train dataset with these three columns, and i want it to be able to bisect the test sentences into its constituents. i have looked into the different pre-trained models in BERT, but i havent  gotten any success. Am i using the wrong tool?</p>
"
61157314,RuntimeError: Unknown device when trying to run AlbertForMaskedLM on colab tpu,"<p>I am running the following code on colab taken from the example here: <a href=""https://huggingface.co/transformers/model_doc/albert.html#albertformaskedlm"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/albert.html#albertformaskedlm</a></p>

<pre><code>import os
import torch
import torch_xla
import torch_xla.core.xla_model as xm

assert os.environ['COLAB_TPU_ADDR']

dev = xm.xla_device()

from transformers import AlbertTokenizer, AlbertForMaskedLM
import torch

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForMaskedLM.from_pretrained('albert-base-v2').to(dev)
input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1

data = input_ids.to(dev)

outputs = model(data, masked_lm_labels=data)
loss, prediction_scores = outputs[:2]
</code></pre>

<p>I haven't done anything to the example code except move <code>input_ids</code> and <code>model</code> onto the TPU device using <code>.to(dev)</code>. It seems everything is moved to the TPU no problem as when I input <code>data</code> I get the following output: <code>tensor([[    2, 10975,    15,    51,  1952,    25, 10901,     3]], device='xla:1')</code></p>

<p>However when I run this code I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-f756487db8f7&gt; in &lt;module&gt;()
      1 
----&gt; 2 outputs = model(data, masked_lm_labels=data)
      3 loss, prediction_scores = outputs[:2]

9 frames
/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py in forward(self, hidden_states, attention_mask, head_mask)
    277         attention_output = self.attention(hidden_states, attention_mask, head_mask)
    278         ffn_output = self.ffn(attention_output[0])
--&gt; 279         ffn_output = self.activation(ffn_output)
    280         ffn_output = self.ffn_output(ffn_output)
    281         hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])

RuntimeError: Unknown device
</code></pre>

<p>Anyone know what's going on?</p>
"
61221810,Confusion in Pre-processing text for Roberta Model,"<p>I want to apply Roberta model for  text similarity. Given a pair of sentences,the input should be in the format <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code>. I figure out two possible ways to generate the input ids namely</p>

<p>a)</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')

list1 = tokenizer.encode('Very severe pain in hands')

list2 = tokenizer.encode('Numbness of upper limb')

sequence = list1+[2]+list2[1:]

</code></pre>

<p>In this case, sequence is <code>[0, 12178, 3814, 2400, 11, 1420, 2, 2, 234, 4179, 1825, 9, 2853, 29654, 2]</code></p>

<p>b)</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')

list1 = tokenizer.encode('Very severe pain in hands', add_special_tokens=False)

list2 = tokenizer.encode('Numbness of upper limb', add_special_tokens=False)

sequence = [0]+list1+[2,2]+list2+[2]
</code></pre>

<p>In this case, sequence is <code>[0, 25101, 3814, 2400, 11, 1420, 2, 2, 487, 4179, 1825, 9, 2853, 29654, 2]</code></p>

<p>Here <code>0</code> represents <code>&lt;s&gt;</code> token and 2 represents <code>&lt;/s&gt;</code> token. I'm not sure which is the correct way to encode the given two sentences for calculating sentence similarity using Roberta model.</p>
"
61306391,Running SQuAD script using ALBERT (huggingface-transformers),"<p>I have a question regarding the usage of ALBERT with the SQuAD 2.0 huggingface-transformers script.</p>

<p>In the github page, there are no specific instructions in how to run the script using ALBERT, so I used the same specifications used to run the script with BERT. 
However, the final results achieved are (exact_match = 30.632527583593028, f1 = 36.36948708435092), far from the (f1 = 88.52, exact_match = 81.22) that are achieved by BERT and that are reported on the github page. So I think that I may be doing something wrong.</p>

<p>This is the code that I ran in the command line: </p>

<pre><code>python run_squad.py \
   --model_type albert \
   --model_name_or_path albert-base-v2 \
   --do_train   --do_eval \
   --train_file train-v2.0.json \
   --predict_file dev-v2.0.json \
   --per_gpu_train_batch_size 5 \
   --learning_rate 3e-5 \
   --num_train_epochs 2.0 \
   --max_seq_length 384 \
   --doc_stride 128 \
   --output_dir /aneves/teste2/output/
</code></pre>

<p>The only difference between this one and the one from the transformers page is the model_name, in which they use 'bert_base_uncased', and the per_gpu_train_batch_size which is 12 but I had to use 5 due to memory constrains in my GPU. </p>

<p>Am I forgetting some option when I run the script or are the results achieved because of the per_gpu_train_batch_size being set to 5 instead of 12?</p>

<p>Thanks!</p>
"
61326892,Gradient of the loss of DistilBERT for measuring token importance,"<p>I am trying to access the gradient of the loss in DistilBERT with respect to each attention weight in the first layer. I could access the computed gradient value of the output weight matrix via the following code when <code>requires_grad=True</code> </p>

<pre><code>loss.backward()
for name, param in model.named_parameters():
    if name == 'transformer.layer.0.attention.out_lin.weight':
       print(param.grad)  #shape is [768,768]
</code></pre>

<p>where <code>model</code> is the loaded distilbert model.
My question is how to get the gradient with respect to [SEP] or [CLS] or other tokens' attention? I need it to reproduce the figure about the ""Gradient-based feature importance estimates for attention to [SEP]"" in the following link: 
<a href=""https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062</a></p>

<p>A similar question for the same purpose has been asked in the following, but it is not my issue:
<a href=""https://stackoverflow.com/questions/61286574/bert-token-importance-measuring-issue-grad-is-none"">BERT token importance measuring issue. Grad is none</a> </p>
"
61443480,Huggingface's BERT tokenizer not adding pad token,"<p>It's not entirely clear from the documentation, but I can see that <code>BertTokenizer</code> is initialised with <code>pad_token='[PAD]'</code>, so I assume when you encode with <code>add_special_tokens=True</code> then it would automatically pad it. Given that <code>pad_token_id=0</code>, I can't see any <code>0</code>s in the <code>token_ids</code> however:</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
tokens = tokenizer.tokenize(text)
token_ids = tokenizer.encode(text, add_special_tokens=True, max_length=2048)

# Print the original sentence.
print('Original: ', text)

# Print the sentence split into tokens.
print('\nTokenized: ', tokens)

# Print the sentence mapped to token ids.
print('\nToken IDs: ', token_ids)
</code></pre>

<p>Output:</p>

<pre><code>Original:  Toronto's key stock index ended higher in brisk trading on Thursday, extending Wednesday's rally despite being weighed down by losses on Wall Street.
The TSE 300 Composite Index rose 29.80 points to close at 5828.62, outperforming the Dow Jones Industrial Average which slumped 21.27 points to finish at 6658.60.
Toronto added to Wednesday's 55-point rally while investors took profits in New York after the Dow's 92-point gains, said MMS International analyst Katherine Beattie.
""That shows that the markets are very fragile,"" Beattie said. ""They (investors) want to take advantage of any strength to sell,"" she said.
Toronto was also buoyed by its heavyweight gold group which jumped nearly 2.2 percent, aided by firmer COMEX gold prices. The key June contract rose $1.00 to $344.30.
Ten of Toronto's 14 sub-indices posted gains, led by golds, transportation, forestry products and consumer products.
The weak side included conglomerates, base metals and utilities.
Trading was heavy at 100 million shares worth C$1.54 billion ($1.1 billion).
Advancing stocks outnumbered declines 556 to 395, with 276 issues flat.
Among hot stocks, Bre-X Minerals Ltd. rose 0.13 to 2.30 on 5.0 million shares as investors continued to consider the viability of its Busang gold discovery in Indonesia.
Kenting Energy Services Inc. rose 0.25 to 9.05 after Precision Drilling Corp. amended its takeover offer
Bakery and foodstuffs maker George Weston Ltd. jumped 4.50 to close at 74.50, the TSE's top gainer.


Tokenized:  ['toronto', ""'"", 's', 'key', 'stock', 'index', 'ended', 'higher', 'in', 'brisk', 'trading', 'on', 'thursday', ',', 'extending', 'wednesday', ""'"", 's', 'rally', 'despite', 'being', 'weighed', 'down', 'by', 'losses', 'on', 'wall', 'street', '.', 'the', 'ts', '##e', '300', 'composite', 'index', 'rose', '29', '.', '80', 'points', 'to', 'close', 'at', '58', '##28', '.', '62', ',', 'out', '##per', '##form', '##ing', 'the', 'dow', 'jones', 'industrial', 'average', 'which', 'slumped', '21', '.', '27', 'points', 'to', 'finish', 'at', '66', '##58', '.', '60', '.', 'toronto', 'added', 'to', 'wednesday', ""'"", 's', '55', '-', 'point', 'rally', 'while', 'investors', 'took', 'profits', 'in', 'new', 'york', 'after', 'the', 'dow', ""'"", 's', '92', '-', 'point', 'gains', ',', 'said', 'mm', '##s', 'international', 'analyst', 'katherine', 'beat', '##tie', '.', '""', 'that', 'shows', 'that', 'the', 'markets', 'are', 'very', 'fragile', ',', '""', 'beat', '##tie', 'said', '.', '""', 'they', '(', 'investors', ')', 'want', 'to', 'take', 'advantage', 'of', 'any', 'strength', 'to', 'sell', ',', '""', 'she', 'said', '.', 'toronto', 'was', 'also', 'bu', '##oy', '##ed', 'by', 'its', 'heavyweight', 'gold', 'group', 'which', 'jumped', 'nearly', '2', '.', '2', 'percent', ',', 'aided', 'by', 'firm', '##er', 'come', '##x', 'gold', 'prices', '.', 'the', 'key', 'june', 'contract', 'rose', '$', '1', '.', '00', 'to', '$', '344', '.', '30', '.', 'ten', 'of', 'toronto', ""'"", 's', '14', 'sub', '-', 'indices', 'posted', 'gains', ',', 'led', 'by', 'gold', '##s', ',', 'transportation', ',', 'forestry', 'products', 'and', 'consumer', 'products', '.', 'the', 'weak', 'side', 'included', 'conglomerate', '##s', ',', 'base', 'metals', 'and', 'utilities', '.', 'trading', 'was', 'heavy', 'at', '100', 'million', 'shares', 'worth', 'c', '$', '1', '.', '54', 'billion', '(', '$', '1', '.', '1', 'billion', ')', '.', 'advancing', 'stocks', 'outnumbered', 'declines', '55', '##6', 'to', '395', ',', 'with', '276', 'issues', 'flat', '.', 'among', 'hot', 'stocks', ',', 'br', '##e', '-', 'x', 'minerals', 'ltd', '.', 'rose', '0', '.', '13', 'to', '2', '.', '30', 'on', '5', '.', '0', 'million', 'shares', 'as', 'investors', 'continued', 'to', 'consider', 'the', 'via', '##bility', 'of', 'its', 'bus', '##ang', 'gold', 'discovery', 'in', 'indonesia', '.', 'kent', '##ing', 'energy', 'services', 'inc', '.', 'rose', '0', '.', '25', 'to', '9', '.', '05', 'after', 'precision', 'drilling', 'corp', '.', 'amended', 'its', 'takeover', 'offer', 'bakery', 'and', 'foods', '##tu', '##ffs', 'maker', 'george', 'weston', 'ltd', '.', 'jumped', '4', '.', '50', 'to', 'close', 'at', '74', '.', '50', ',', 'the', 'ts', '##e', ""'"", 's', 'top', 'gain', '##er', '.']

Token IDs:  [101, 4361, 1005, 1055, 3145, 4518, 5950, 3092, 3020, 1999, 28022, 6202, 2006, 9432, 1010, 8402, 9317, 1005, 1055, 8320, 2750, 2108, 12781, 2091, 2011, 6409, 2006, 2813, 2395, 1012, 1996, 24529, 2063, 3998, 12490, 5950, 3123, 2756, 1012, 3770, 2685, 2000, 2485, 2012, 5388, 22407, 1012, 5786, 1010, 2041, 4842, 14192, 2075, 1996, 23268, 3557, 3919, 2779, 2029, 14319, 2538, 1012, 2676, 2685, 2000, 3926, 2012, 5764, 27814, 1012, 3438, 1012, 4361, 2794, 2000, 9317, 1005, 1055, 4583, 1011, 2391, 8320, 2096, 9387, 2165, 11372, 1999, 2047, 2259, 2044, 1996, 23268, 1005, 1055, 6227, 1011, 2391, 12154, 1010, 2056, 3461, 2015, 2248, 12941, 9477, 3786, 9515, 1012, 1000, 2008, 3065, 2008, 1996, 6089, 2024, 2200, 13072, 1010, 1000, 3786, 9515, 2056, 1012, 1000, 2027, 1006, 9387, 1007, 2215, 2000, 2202, 5056, 1997, 2151, 3997, 2000, 5271, 1010, 1000, 2016, 2056, 1012, 4361, 2001, 2036, 20934, 6977, 2098, 2011, 2049, 8366, 2751, 2177, 2029, 5598, 3053, 1016, 1012, 1016, 3867, 1010, 11553, 2011, 3813, 2121, 2272, 2595, 2751, 7597, 1012, 1996, 3145, 2238, 3206, 3123, 1002, 1015, 1012, 4002, 2000, 1002, 29386, 1012, 2382, 1012, 2702, 1997, 4361, 1005, 1055, 2403, 4942, 1011, 29299, 6866, 12154, 1010, 2419, 2011, 2751, 2015, 1010, 5193, 1010, 13116, 3688, 1998, 7325, 3688, 1012, 1996, 5410, 2217, 2443, 22453, 2015, 1010, 2918, 11970, 1998, 16548, 1012, 6202, 2001, 3082, 2012, 2531, 2454, 6661, 4276, 1039, 1002, 1015, 1012, 5139, 4551, 1006, 1002, 1015, 1012, 1015, 4551, 1007, 1012, 10787, 15768, 21943, 26451, 4583, 2575, 2000, 24673, 1010, 2007, 25113, 3314, 4257, 1012, 2426, 2980, 15768, 1010, 7987, 2063, 1011, 1060, 13246, 5183, 1012, 3123, 1014, 1012, 2410, 2000, 1016, 1012, 2382, 2006, 1019, 1012, 1014, 2454, 6661, 2004, 9387, 2506, 2000, 5136, 1996, 3081, 8553, 1997, 2049, 3902, 5654, 2751, 5456, 1999, 6239, 1012, 5982, 2075, 2943, 2578, 4297, 1012, 3123, 1014, 1012, 2423, 2000, 1023, 1012, 5709, 2044, 11718, 15827, 13058, 1012, 13266, 2049, 15336, 3749, 18112, 1998, 9440, 8525, 21807, 9338, 2577, 12755, 5183, 1012, 5598, 1018, 1012, 2753, 2000, 2485, 2012, 6356, 1012, 2753, 1010, 1996, 24529, 2063, 1005, 1055, 2327, 5114, 2121, 1012, 102]
</code></pre>
"
61465223,Roberta Tokenization of multiple sequences,"<p>The <a href=""https://github.com/huggingface/transformers/blob/41750a6cff55e401364568868d619747de3db037/src/transformers/tokenization_roberta.py#L154"" rel=""nofollow noreferrer"">Roberta Tokenizer</a> in huggingface-transformers describes Roberta's tokenization
method as such:  </p>

<pre><code>- single sequence: ``&lt;s&gt; X &lt;/s&gt;``
- pair of sequences: ``&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;``
</code></pre>

<p>I'm curious why the tokenization of multiple sequences is not <code>&lt;s&gt; A &lt;/s&gt;&lt;s&gt; B &lt;/s&gt;</code>?</p>

<p>Building upon the above, if I were to encode more than two sequences manually, should I encode them as <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;&lt;/s&gt; C &lt;/s&gt;</code> or as <code>&lt;s&gt; A &lt;/s&gt;&lt;s&gt; B &lt;/s&gt;&lt;s&gt; C &lt;/s&gt;</code></p>
"
61482810,Fine tuning a pretrained language model with Simple Transformers,"<p>In his article 'Language Model Fine-Tuning For Pre-Trained Transformers' Thilina Rajapakse (<a href=""https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee"" rel=""nofollow noreferrer"">https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee</a>)
provides the following code snippet for fine-tuning a pre-trained model using the library <code>simpletransformers</code>:</p>

<pre><code>from simpletransformers.language_modeling import LanguageModelingModel
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(""transformers"")
transformers_logger.setLevel(logging.WARNING)

train_args = {
    ""reprocess_input_data"": True,
    ""overwrite_output_dir"": True,
}

model = LanguageModelingModel('bert', 'bert-base-cased', args=train_args)

model.train_model(""data/train.txt"", eval_file=""data/text.txt"")

model.eval_model(""data/test.txt"")
</code></pre>

<p>He then adds:</p>

<blockquote>
  <p>We assume that you have combined all the text in your dataset into two
  text files train.txt and test.txt which can be found in the data/
  directory.</p>
</blockquote>

<p>I have 2 questions:</p>

<p><strong>Question 1</strong></p>

<p>Does the highlighted sentence above implies that the entire corpus will be merged into one text file?  So assuming that the Training Corpus is comprised of 1,000,000 text files, are we supposed to merge them all in one text file with code like this?</p>

<pre><code>import fileinput
with open(outfilename, 'w') as fout, fileinput.input(filenames) as fin:
    for line in fin:
        fout.write(line)
</code></pre>

<p><strong>Question 2</strong></p>

<p>I presume that I can use the pretrained model: <code>bert-base-multilingual-cased</code>.  Correct?</p>
"
61513052,attention_mask is missing in the returned dict from tokenizer.encode_plus,"<p>I have a codebase which was working fine but today when I was trying to run, I observed that <code>tokenizer.encode_plus</code> stopped returning <code>attention_mask</code>. Is it removed in the latest release? Or, do I need to do something else?</p>

<p>The following piece of code was working for me.</p>

<pre><code>encoded_dict = tokenizer.encode_plus(
                truncated_query,
                span_doc_tokens,
                max_length=max_seq_length,
                return_overflowing_tokens=True,
                pad_to_max_length=True,
                stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,
                truncation_strategy=""only_second"",
                return_token_type_ids=True,
                return_attention_mask=True
            )
</code></pre>

<p>But now, I get only <code>dict_keys(['input_ids', 'token_type_ids'])</code> from encode_plus. Also, I realized that the returned <code>input_ids</code> are not padded to <code>max_length</code>.</p>
"
61567599,HuggingFace BERT `inputs_embeds` giving unexpected result,"<p>The <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">HuggingFace BERT TensorFlow implementation</a> allows us to feed in a precomputed embedding in place of the embedding lookup that is native to BERT. This is done using the model's <code>call</code> method's optional parameter <code>inputs_embeds</code> (in place of <code>input_ids</code>). To test this out, I wanted to make sure that if I <em>did</em> feed in BERT's embedding lookup, I would get the same result as having fed in the <code>input_ids</code> themselves.</p>

<p>The result of BERT's embedding lookup can be obtained by setting the BERT configuration parameter <code>output_hidden_states</code> to <code>True</code> and extracting the first tensor from the last output of the <code>call</code> method. (The remaining 12 outputs correspond to each of the 12 hidden layers.)</p>

<p>Thus, I wrote the following code to test my hypothesis:</p>

<pre><code>import tensorflow as tf
from transformers import BertConfig, BertTokenizer, TFBertModel

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = tf.constant(bert_tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]
attention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])
token_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])

config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)

result = bert_model(inputs={'input_ids': input_ids, 
                            'attention_mask': attention_mask, 
                             'token_type_ids': token_type_ids})
inputs_embeds = result[-1][0]
result2 = bert_model(inputs={'inputs_embeds': inputs_embeds, 
                            'attention_mask': attention_mask, 
                             'token_type_ids': token_type_ids})

print(tf.reduce_sum(tf.abs(result[0] - result2[0])))  # 458.2522, should be 0
</code></pre>

<p>Again, the output of the <code>call</code> method is a tuple. The first element of this tuple is the output of the last layer of BERT. Thus, I expected <code>result[0]</code> and <code>result2[0]</code> to match. <strong>Why is this not the case?</strong></p>

<p>I am using Python 3.6.10 with <code>tensorflow</code> version 2.1.0 and <code>transformers</code> version 2.5.1.</p>

<p><strong>EDIT</strong>: Looking at some of the <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_bert.html"" rel=""noreferrer"">HuggingFace code</a>, it seems that the raw embeddings that are looked up when <code>input_ids</code> is given or assigned when <code>inputs_embeds</code> is given are added to the positional embeddings and token type embeddings before being fed into subsequent layers. If this is the case, then it <em>may</em> be possible that what I'm getting from <code>result[-1][0]</code> is the raw embedding plus the positional and token type embeddings. This would mean that they are erroneously getting added in again when I feed <code>result[-1][0]</code> as <code>inputs_embeds</code> in order to calculate <code>result2</code>.</p>

<p><strong>Could someone please tell me if this is the case and if so, please explain how to get the positional and token type embeddings, so I can subtract them out?</strong> Below is what I came up with for positional embeddings based on the equations given <a href=""https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3"" rel=""noreferrer"">here</a> (but according to the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a>, the positional embeddings may actually be learned, so I'm not sure if these are valid):</p>

<pre><code>import numpy as np

positional_embeddings = np.stack([np.zeros(shape=(len(sent),768)) for sent in input_ids])
for s in range(len(positional_embeddings)):
    for i in range(len(positional_embeddings[s])):
        for j in range(len(positional_embeddings[s][i])):
            if j % 2 == 0:
                positional_embeddings[s][i][j] = np.sin(i/np.power(10000., j/768.))
            else:
                positional_embeddings[s][i][j] = np.cos(i/np.power(10000., (j-1.)/768.))
positional_embeddings = tf.constant(positional_embeddings)
inputs_embeds += positional_embeddings
</code></pre>
"
61569900,Getting embedding lookup result from BERT,"<p>Prior to passing my tokens through BERT, I would like to perform some processing on their embeddings, (the result of the embedding lookup layer). The <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""nofollow noreferrer"">HuggingFace BERT TensorFlow implementation</a> allows us to access the output of embedding lookup using:</p>

<pre><code>import tensorflow as tf
from transformers import BertConfig, BertTokenizer, TFBertModel

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = tf.constant(bert_tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]
attention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])
token_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])

config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)

result = bert_model(inputs={'input_ids': input_ids, 
                            'attention_mask': attention_mask, 
                            'token_type_ids': token_type_ids})
inputs_embeds = result[-1][0]  # output of embedding lookup
</code></pre>

<p>Subsequently, one can process <code>inputs_embeds</code> and then send this in as an input to the same model using:</p>

<pre><code>inputs_embeds = process(inputs_embeds)  # some processing on inputs_embeds done here (dimensions kept the same)
result = bert_model(inputs={'inputs_embeds': inputs_embeds, 
                            'attention_mask': attention_mask, 
                            'token_type_ids': token_type_ids})
output = result[0]
</code></pre>

<p>where <code>output</code> now contains the output of BERT for the modified input. However, this requires two full passes through BERT. Instead of running BERT all the way through just to perform embedding lookup, I would like to just get the output of the embedding lookup layer. <strong>Is this possible, and if so, how?</strong></p>
"
61580961,Unable to load SpanBert model with transformers package,"<p>I have some questions regarding of SpanBert loading using transformers packages.</p>

<p>I downloaded the pre-trained file from <a href=""https://github.com/facebookresearch/SpanBERT"" rel=""nofollow noreferrer"">SpanBert</a> GitHub Repo and <code>vocab.txt</code> from Bert. Here is the code I used for loading:</p>

<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained(config_file=config_file,
                                  pretrained_model_name_or_path=model_file,
                                  vocab_file=vocab_file)
model.to(""cuda"")
</code></pre>

<p>where </p>

<ul>
<li><code>config_file</code> -> <code>config.json</code></li>
<li><code>model_file</code> -> <code>pytorch_model.bin</code></li>
<li><code>vocab_file</code> -> <code>vocab.txt</code></li>
</ul>

<p>But I got the <code>UnicodeDecoderError</code> with the above code saying that <code>'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte</code></p>

<p>I also tried loading SpanBert with the method mentioned <a href=""https://huggingface.co/SpanBERT/spanbert-large-cased"" rel=""nofollow noreferrer"">here</a>. But it returned <code>OSError: file SpanBERT/spanbert-base-cased not found</code>.</p>

<p>Do you have any suggestions on loading the pre-trained model correctly? Any suggestions are much appreciated. Thanks!</p>
"
61656822,"Tensorflow 2.0 Hugging Face Transformers, TFBertForSequenceClassification, Unexpected Output Dimensions in Inference","<p><strong>Summary:</strong></p>

<p>I want to fine-tune BERT for sentence classification on a custom dataset. I have followed some examples I have found, like <a href=""https://stackoverflow.com/questions/59978959/how-to-use-hugging-face-transformers-library-in-tensorflow-for-text-classificati"">this one</a>, which was very helpful. I have also looked at <a href=""https://gist.github.com/papapabi/124c6ac406e6bbd1f28df732e953ac6d"" rel=""nofollow noreferrer"">this gist</a>. </p>

<p><strong>The problem I have is that when running inference for some samples, the output has other dimensions than I would expect.</strong></p>

<p>When I run inference for 23 samples, I get a tuple with a numpy array of dimensions (1472, 42), where 42 is the number of classes. I would expect dimensions (23, 42).</p>

<p><strong>Code and Other Details:</strong></p>

<p>I run the inference on the trained model using Keras like this:</p>

<pre><code>preds = model.predict(features)
</code></pre>

<p>Where <em>features</em> is tokenized and converted to a Dataset:</p>

<pre><code>for sample, ground_truth in tests:
    test_examples.append(InputExample(text=sample, category_index=ground_truth))

features = convert_examples_to_tf_dataset(test_examples, tokenizer)
</code></pre>

<p>Where <code>sample</code> can be e.g. <code>""A test sentence I want classified""</code> and <code>ground_truth</code> can be e.g. <code>12</code> which is the encoded label. Because I do inference, what I supply as ground truth shouldn't matter of course.</p>

<p>The <code>convert_examples_to_tf_dataset</code>-function looks as follows (which I found in <a href=""https://gist.github.com/papapabi/124c6ac406e6bbd1f28df732e953ac6d"" rel=""nofollow noreferrer"">this gist</a>):</p>

<pre><code>def convert_examples_to_tf_dataset(
    examples: List[Tuple[str, int]],
    tokenizer,
    max_length=64,
):
    """"""
    Loads data into a tf.data.Dataset for finetuning a given model.

    Args:
        examples: List of tuples representing the examples to be fed
        tokenizer: Instance of a tokenizer that will tokenize the examples
        max_length: Maximum string length

    Returns:
        a ``tf.data.Dataset`` containing the condensed features of the provided sentences
    """"""
    features = [] # -&gt; will hold InputFeatures to be converted later

    for e in examples:
        # Documentation is really strong for this method, so please take a look at it
        input_dict = tokenizer.encode_plus(
            e.text,
            add_special_tokens=True,
            max_length=max_length, # truncates if len(s) &gt; max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default
        )

        # input ids = token indices in the tokenizer's internal dict
        # token_type_ids = binary mask identifying different sequences in the model
        # attention_mask = binary mask indicating the positions of padded tokens so the model does not attend to them

        input_ids, token_type_ids, attention_mask = (input_dict[""input_ids""],
            input_dict[""token_type_ids""], input_dict['attention_mask'])

        features.append(
            InputFeatures(
                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.category_index
            )
        )

    def gen():
        for f in features:
            yield (
                {
                    ""input_ids"": f.input_ids,
                    ""attention_mask"": f.attention_mask,
                    ""token_type_ids"": f.token_type_ids,
                },
                f.label,
            )

    return tf.data.Dataset.from_generator(
        gen,
        ({""input_ids"": tf.int32, ""attention_mask"": tf.int32, ""token_type_ids"": tf.int32}, tf.int64),
        (
            {
                ""input_ids"": tf.TensorShape([None]),
                ""attention_mask"": tf.TensorShape([None]),
                ""token_type_ids"": tf.TensorShape([None]),
            },
            tf.TensorShape([]),
        ),
    )

with tf.device('/cpu:0'):
    train_data = convert_examples_to_tf_dataset(train_examples, tokenizer)
    train_data = train_data.shuffle(buffer_size=len(train_examples), reshuffle_each_iteration=True) \
                           .batch(BATCH_SIZE) \
                           .repeat(-1)

    val_data = convert_examples_to_tf_dataset(val_examples, tokenizer)
    val_data = val_data.shuffle(buffer_size=len(val_examples), reshuffle_each_iteration=True) \
                           .batch(BATCH_SIZE) \
                           .repeat(-1)
</code></pre>

<p>It works as I would expect and running <code>print(list(features.as_numpy_iterator())[1])</code> yields the following:</p>

<pre><code>({'input_ids': array([  101, 11639, 19962, 23288, 13264, 35372, 10410,   102,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      dtype=int32), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      dtype=int32)}, 6705)
</code></pre>

<p>So far everything looks like I would expect. And it seems like the tokenizer is working as it should; 3 arrays of length 64 (which corresponds to my set max-length), and a label as an integer.</p>

<p>The model has been trained as follows:</p>

<pre><code>config = BertConfig.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels=len(label_encoder.classes_),
    output_hidden_states=False,
    output_attentions=False
)
model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', config=config)

# train_data is then a tf.data.Dataset we can pass to model.fit()
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-05, epsilon=1e-08)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')
model.compile(optimizer=optimizer,
              loss=loss,
              metrics=[metric])

model.summary()

history = model.fit(train_data,
                    epochs=EPOCHS,
                    steps_per_epoch=train_steps,
                    validation_data=val_data,
                    validation_steps=val_steps,
                    shuffle=True,
                    )
</code></pre>

<p><strong>Results</strong></p>

<p>The problem now is that when running a prediction <code>preds = model.predict(features)</code>, the output dimensions does not correspond to what the <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification"" rel=""nofollow noreferrer"">documentation</a> says: <code>logits (Numpy array or tf.Tensor of shape (batch_size, config.num_labels)):</code>. <strong>What I get is a tuple containing a numpy array with dimensions: (1472,42).</strong></p>

<p>42 makes sense as this is my number of classes. I sent 23 samples for the test, and 23 x 64 = 1472. 64 is my max sentence length, so it kind of sounds familiar. Is this output incorrect? How can I convert this output to an actual class prediction for each input sample? I get 1472 predictions when I would expect 23.</p>

<p>Please let me know if I can provide more details that could help solve this.</p>
"
61667142,huggingface-transformers: Train BERT and evaluate it using different attentions,"<p>This is a clarification question. I am trying to train <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">BERT provided by huggingface</a> using standard attention, and evaluate using a different attention definition.</p>

<p>The operation I was thinking about was change <code>bert-base-uncased</code> to the path of my trained model(using standard attention) in the following command, and run <code>--do_eval</code> under the installation of my customized attention version.</p>

<pre><code>export GLUE_DIR=/path/to/glue
export TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_name_or_path bert-base-uncased \
    --task_name $TASK_NAME \
    --do_eval \
    --data_dir $GLUE_DIR/$TASK_NAME \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/$TASK_NAME/
</code></pre>

<p>However, I was getting unexpected results. So I want to make sure that I was using the right command. Could anyone confirm with me or correct me?</p>

<p>Edited: The version was 2.8.0.</p>
"
61667186,Reformer local and LSH attention in HuggingFace implementation,"<p>The recent implementation of the Reformer in HuggingFace has both what they call LSH Self Attention and Local Self Attention, but the difference is not very clear to me after reading <a href=""https://huggingface.co/transformers/model_doc/reformer.html"" rel=""nofollow noreferrer"">the documentation</a>. Both use bucketing to avoid the quadratic memory requirement of vanilla transformers, but it is not clear how they differ.</p>

<p>Is it the case that local self attention only allows queries to attend to keys sequentially near them (i.e., inside a given window in the sentence), as opposed to the proper LSH hashing that LSH self attention does? Or is it something else?</p>
"
61707371,About get_special_tokens_mask in huggingface-transformers,"<p>I use transformers tokenizer, and created mask using API: get_special_tokens_mask.<br>
<a href=""https://github.com/ishikawa-takumi/transformers-sample/blob/master/tokenizer.ipynb"" rel=""nofollow noreferrer"">My Code</a></p>

<p>In <a href=""https://huggingface.co/transformers/model_doc/roberta.html"" rel=""nofollow noreferrer"">RoBERTa Doc</a>, returns of this API is ""A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token"". But I seem that this API returns ""0 for a sequence token, 1 for a special token"".<br>
Is it all right?</p>
"
61708486,what's difference between tokenizer.encode and tokenizer.encode_plus in Hugging Face,"<p>Here is an example of doing sequence classification using a model to determine if two sequences are paraphrases of each other. The two examples give two different results. Can you help me explain why <code>tokenizer.encode</code> and <code>tokenizer.encode_plus</code> give different results?</p>

<p>Example 1 (with <code>.encode_plus()</code>):</p>

<pre class=""lang-py prettyprint-override""><code>paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=""pt"")
not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=""pt"")

paraphrase_classification_logits = model(**paraphrase)[0]
not_paraphrase_classification_logits = model(**not_paraphrase)[0]
</code></pre>

<p>Example 2 (with <code>.encode()</code>):</p>

<pre class=""lang-py prettyprint-override""><code>paraphrase = tokenizer.encode(sequence_0, sequence_2, return_tensors=""pt"")
not_paraphrase = tokenizer.encode(sequence_0, sequence_1, return_tensors=""pt"")

paraphrase_classification_logits = model(paraphrase)[0]
not_paraphrase_classification_logits = model(not_paraphrase)[0]
</code></pre>
"
61717097,Sequence Labelling with BERT,"<p>I am using a model consisting of an embedding layer and an LSTM to perform sequence labelling, in pytorch + torchtext. I have already tokenised the sentences.</p>

<p>If I use self-trained or other pre-trained word embedding vectors, this is straightforward.</p>

<p>But if I use the Huggingface transformers <code>BertTokenizer.from_pretrained</code> and <code>BertModel.from_pretrained</code> there is a <code>'[CLS]'</code> and <code>'[SEP]'</code> token added to the beginning and end of the sentence, respectively. So the output of the model becomes a sequence that is two elements longer than the label/target sequence.</p>

<p>What I am unsure of is:</p>

<ol>
<li>Are these two tags needed for the <code>BertModel</code> to embed each token of a sentence ""correctly""?</li>
<li>If they are needed, can I take them out after the BERT embedding layer, before the input to the LSTM, so that the lengths are correct in the output?</li>
</ol>
"
61774933,Understanding the Hugging face transformers,"<p>I am new to the Transformers concept and I am going through some tutorials and writing my own code to understand the Squad 2.0 dataset Question Answering using the transformer models. In the hugging face website, I came across 2 different links</p>

<ul>
<li><a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a></li>
<li><a href=""https://huggingface.co/transformers/pretrained_models.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/pretrained_models.html</a></li>
</ul>

<p>I want to know the difference between these 2 websites. Does one link have just a pre-trained model and the other have a pre-trained and fine-tuned model? </p>

<p>Now if I want to use, let's say an Albert Model For Question Answering and train with my Squad 2.0 training dataset on that and evaluate the model, to which of the link should I further?</p>
"
61776977,SMOTE with multiple bert inputs,"<p>I'm building a multiclass text classification model using Keras and Bert (HuggingFace), but I have a very imbalanced dataset. I've used SMOTE from Sklearn in order to generate additional samples for the underbalanced classes (I have 45 in total), which works fine when I use the input ids from the Bert Tokenizer.</p>

<p>However, I would like to be able to also use smote for the input masks ids, in order to allow the model to determine where the padded values are.</p>

<p>My question is how can I use smote for both input ids and mask ids? I've done the following so far, and the model doesn't complain, but I'm not sure if the resampled masks match the resampled input ids row for row.
 Smote requires two inputs, inputs and labels, so I've duplicated the process with the same random state, and just returned the required elements:</p>

<pre><code>def smote(input_ids, input_masks, labels):

    smote = SMOTE(""not majority"", random_state=27)

    input_ids_resampled, labels_resampled = smote.fit_sample(input_ids, labels)
    input_masks_resampled, _ = smote.fit_sample(input_masks, labels)

    return input_ids_resampled, input_masks_resampled, labels_resampled
</code></pre>

<p>Is this acceptable? Is there a better way to do this?</p>
"
61798573,Where does hugging face's transformers save models?,"<p>Running the below code downloads a model - does anyone know what folder it downloads it to?</p>

<pre><code>!pip install -q transformers
from transformers import pipeline
model = pipeline('fill-mask')
</code></pre>
"
61832308,transformers-cli error: the following arguments are required: --model_type,"<p>I am trying to convert a tf checkpoint to a pytorch checkpoint using <code>transformers-cli</code> as following</p>

<p><code>transformers-cli convert model_type bert --tf_checkpoint bio_bert_large_1000k.ckpt --config bert_config_bio_58k_large.json --pytorch_dump_output pytorch_model.bin</code></p>

<p>and am getting the following error</p>

<pre><code>usage: transformers-cli &lt;command&gt; [&lt;args&gt;] convert [-h] --model_type
                                               MODEL_TYPE --tf_checkpoint
                                               TF_CHECKPOINT
                                               --pytorch_dump_output
                                               PYTORCH_DUMP_OUTPUT
                                               [--config CONFIG]
                                               [--finetuning_task_name FINETUNING_TASK_NAME]
transformers-cli &lt;command&gt; [&lt;args&gt;] convert: error: the following arguments are required: --model_type
</code></pre>

<p>What am I doing wrong?</p>
"
61969783,huggingface bert showing poor accuracy / f1 score [pytorch],"<p>I am trying <code>BertForSequenceClassification</code> for a simple article classification task.</p>

<p>No matter how I train it (freeze all layers but the classification layer, all layers trainable, last <code>k</code> layers trainable), I always get an almost randomized accuracy score. My model doesn't go above 24-26% training accuracy (I only have 5 classes in my dataset).</p>

<p>I'm not sure what did I do wrong while designing/training the model. I tried the model with multiple datasets, every time it gives the same random baseline accuracy.</p>

<p>Dataset I used: BBC Articles (5 classes) </p>

<p><a href=""https://github.com/zabir-nabil/pytorch-nlp/tree/master/bbc"" rel=""noreferrer"">https://github.com/zabir-nabil/pytorch-nlp/tree/master/bbc</a></p>

<blockquote>
  <p>Consists of 2225 documents from the BBC news website corresponding to
  stories in five topical areas from 2004-2005. Natural Classes: 5
  (business, entertainment, politics, sport, tech)</p>
</blockquote>

<p>I added the model part and the training part which are the most important portion (to avoid any irrelevant details). I added the full source-code + data too if that's useful for reproducibility.</p>

<p>My guess is there is something wrong with the I way I designed the network or the way I'm passing the attention_masks/ labels to the model. Also, the token length 512 should not be a problem as most of the texts has length &lt; 512 (the mean length is &lt; 300).</p>

<p><strong>Model code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn

class BertClassifier(nn.Module):
    def __init__(self):
        super(BertClassifier, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)
        # as we have 5 classes

        # we want our output as probability so, in the evaluation mode, we'll pass the logits to a softmax layer
        self.softmax = torch.nn.Softmax(dim = 1) # last dimension
    def forward(self, x, attn_mask = None, labels = None):

        if self.training == True:
            # print(x.shape)
            loss = self.bert(x, attention_mask = attn_mask, labels = labels)
            # print(x[0].shape)

            return loss

        if self.training == False: # in evaluation mode
            x = self.bert(x)
            x = self.softmax(x[0])

            return x
    def freeze_layers(self, last_trainable = 1): 
        # we freeze all the layers except the last classification layer + few transformer blocks
        for layer in list(self.bert.parameters())[:-last_trainable]:
            layer.requires_grad = False


# create our model

bertclassifier = BertClassifier()
</code></pre>

<p><strong>Training code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # cuda for gpu acceleration

# optimizer

optimizer = torch.optim.Adam(bertclassifier.parameters(), lr=0.001)


epochs = 15

bertclassifier.to(device) # taking the model to GPU if possible

# metrics

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

train_losses = []

train_metrics = {'acc': [], 'f1': []}
test_metrics = {'acc': [], 'f1': []}

# progress bar

from tqdm import tqdm_notebook

for e in tqdm_notebook(range(epochs)):
    train_loss = 0.0
    train_acc = 0.0
    train_f1 = 0.0
    batch_cnt = 0

    bertclassifier.train()

    print(f'epoch: {e+1}')

    for i_batch, (X, X_mask, y) in tqdm_notebook(enumerate(bbc_dataloader_train)):
        X = X.to(device)
        X_mask = X_mask.to(device)
        y = y.to(device)


        optimizer.zero_grad()

        loss, y_pred = bertclassifier(X, X_mask, y)

        train_loss += loss.item()
        loss.backward()
        optimizer.step()

        y_pred = torch.argmax(y_pred, dim = -1)

        # update metrics
        train_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())
        train_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')
        batch_cnt += 1

    print(f'train loss: {train_loss/batch_cnt}')
    train_losses.append(train_loss/batch_cnt)
    train_metrics['acc'].append(train_acc/batch_cnt)
    train_metrics['f1'].append(train_f1/batch_cnt)


    test_loss = 0.0
    test_acc = 0.0
    test_f1 = 0.0
    batch_cnt = 0

    bertclassifier.eval()
    with torch.no_grad():
        for i_batch, (X, y) in enumerate(bbc_dataloader_test):
            X = X.to(device)
            y = y.to(device)

            y_pred = bertclassifier(X) # in eval model we get the softmax output so, don't need to index


            y_pred = torch.argmax(y_pred, dim = -1)

            # update metrics
            test_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())
            test_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')
            batch_cnt += 1

    test_metrics['acc'].append(test_acc/batch_cnt)
    test_metrics['f1'].append(test_f1/batch_cnt)
</code></pre>

<p>Full source-code with the dataset is available here: <a href=""https://github.com/zabir-nabil/pytorch-nlp/blob/master/bert-article-classification.ipynb"" rel=""noreferrer"">https://github.com/zabir-nabil/pytorch-nlp/blob/master/bert-article-classification.ipynb</a></p>

<p>Update:</p>

<p>After observing the prediction, it seems model almost always predicts 0:</p>

<pre><code>bertclassifier.eval()
with torch.no_grad():
    for i_batch, (X, y) in enumerate(bbc_dataloader_test):
        X = X.to(device)
        y = y.to(device)

        y_pred = bertclassifier(X) # in eval model we get the softmax output so, don't need to index


        y_pred = torch.argmax(y_pred, dim = -1)

        print(y)
        print(y_pred)
        print('--------------------')
</code></pre>

<pre><code>tensor([4, 2, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 4, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 2, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 4, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 4, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 0, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 2, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 1, 2, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 4, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 4, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 1, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 2, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 1, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 4, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 1, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 2, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 2, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 4, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 2, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 2, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 2, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 0, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 2, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 4, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 4, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 1, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 4, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 4, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 4, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 4, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 4, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 4, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
...
...
</code></pre>

<p>Actually, the model is always predicting the same output <code>[0.2270, 0.1855, 0.2131, 0.1877, 0.1867]</code> for any input, it's like it didn't learn anything at all.</p>

<p>It's weird because my dataset is not imbalanced.</p>

<pre><code>Counter({'politics': 417,
         'business': 510,
         'entertainment': 386,
         'tech': 401,
         'sport': 511})
</code></pre>
"
62040309,Why we need the init_weight function in BERT pretrained model in Huggingface Transformers?,"<p>In the code by Hugginface transformers, there are many fine-tuning models have the function <code>init_weight</code>. 
For example(<a href=""https://github.com/huggingface/transformers/blob/a9aa7456ac/src/transformers/modeling_bert.py#L1073-L1082"" rel=""nofollow noreferrer"">here</a>), there is a <code>init_weight</code> function at last.</p>

<pre class=""lang-py prettyprint-override""><code>class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

</code></pre>

<p>As I know, it will call the following <a href=""https://github.com/huggingface/transformers/blob/a9aa7456ac/src/transformers/modeling_bert.py#L520-L530"" rel=""nofollow noreferrer"">code</a></p>

<pre class=""lang-py prettyprint-override""><code>def _init_weights(self, module):
    """""" Initialize the weights """"""
    if isinstance(module, (nn.Linear, nn.Embedding)):
        # Slightly different from the TF version which uses truncated_normal for initialization
        # cf https://github.com/pytorch/pytorch/pull/5617
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, BertLayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if isinstance(module, nn.Linear) and module.bias is not None:
        module.bias.data.zero_()
</code></pre>

<p>My question is <strong>If we are loading the pre-trained model, why do we need to initialize the weight for every module?</strong></p>

<p>I guess I must be misunderstanding something here.</p>
"
62109957,Why does the BERT NSP head linear layer have two outputs?,"<p>Here's the code in question. </p>

<p><a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491</a></p>

<pre><code>class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score
</code></pre>

<p>I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?</p>
"
62125405,RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select site:stackoverflow.com,"<p>I am using the bert on the SMILE dataset. I have written following code can you guide me where I am getting wrong.
I have written training code which is evaluating correctly but the when I try to run evaluate code for validation it is giving error. I tried to pass the parameters directly to cuda. still I am facing the issue</p>

<pre><code>'''

def evaluate(dataloader_val):

  print(""in evaluate"")
  model.eval()

  loss_val_total = 0
  predictions, true_value = [],[]

  for batch in dataloader_val:

    print(""in for loop of dataloader"")
    barch = tuple(b.to(device) for b in batch)

    inputs = {
               'input_ids':  batch[0],
                'attention_mask': batch[1],
                 'labels' : batch[2],
    }

    with torch.no_grad():
      outputs = model(**inputs)

    loss = outputs[0]
    logits = outputs[1]
    loss_val_total += loss.item()

    print(""before logit"")

    logits = logits.to(device)
    print(""in the for batch evaluate: "",logits)
    label_ids = inputs['labels'].to(device)
    true_vals.append(label_ids)

  loss_val_avg = loss_val_total/len(dataloader_val)

  predictions = np.concatenate(predictions, axis = 0)
  true_vals = np.concatenate(true_vals,axis = 0)

  return loss_val_avg, predictions, true_vals
'''
</code></pre>

<p>and another function is</p>

<pre><code>'''
for epoch in tqdm(range(1, epochs+1)):
  model.train()

  loss_train_total = 0

  progress_bar = tqdm(dataloader_train,
                      desc = 'Epoch {:1d}'.format(epoch),
                      leave = False,
                      disable = False)
  for batch in progress_bar:

    model.zero_grad()

    batch = tuple(b.to(device) for b in batch)

    inputs = {

            'input_ids'      : batch[0],
            'attention_mask' : batch[1],
            'labels'         : batch[2]
    }

    outputs = model(**inputs)

    loss = outputs[0]
    loss_train_total += loss.item()
    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()
    scheduler.step()

    progress_bar.set_postfix({'training_loss' : '{:.3f}'.format(loss.item()/len(batch))})

  torch.save(model.state_dict(), f'/content/drive/My Drive/Bert/Coursera/SMILE/Bert_ft_epoch{epoch}.model')

  tqdm.write(f'\n Epoch {epoch}')

  loss_train_avg = loss_train_total / len(dataloader_train)

  tqdm.write(f'Training Loss: {loss_train_avg}')

  val_loss, predictions, true_vals = evaluate(dataloader_val)
  val_f1 = f1_score_func(predictions, true_vals)
  tqdm.write(f'Validation loss : {val_loss}')
  tqdm.write(f'F1 score(weighted): {val_f1}')
'''
</code></pre>
"
62206826,Huggingface Bert: Output Printing,"<p>I'm new to coding, and could use guidance as to why it is printing oddly like it is. While this is related to NLP, I believe this error could most likely be explained by somebody who has greater knowledge in coding than me. I hope this is the right place to ask this question. Thank you for the help! </p>

<pre><code>from transformers import AutoTokenizer, AutoModelWithLMHead
import torch


tokenizer = AutoTokenizer.from_pretrained(""bert-large-cased-whole-word-masking"")

model = AutoModelWithLMHead.from_pretrained(""bert-large-cased-whole-word-masking"")

sentence = """"""While United States [MASK] heed human rights,""""""


token_ids = tokenizer.encode(sentence, return_tensors='pt')
# print(token_ids)
token_ids_tk = tokenizer.tokenize(sentence, return_tensors='pt')
print(token_ids_tk)


masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()
masked_pos = [mask.item() for mask in masked_position ]
print (masked_pos)


with torch.no_grad():
    output = model(token_ids)

last_hidden_state = output[0].squeeze()

print (""\n\n"")
print (""sentence :"", sentence)
print (""\n"")
list_of_list =[]
for mask_index in masked_pos:
    mask_hidden_state = last_hidden_state[mask_index]
    idx = torch.topk(mask_hidden_state, k=25, dim=0)[1]
    words = [tokenizer.decode(i.item()).strip() for i in idx]
    list_of_list.append(words)
    print (words)

best_guess = """"
for j in list_of_list:
    best_guess = best_guess+"" ""+j[0]

print (""\nBest guess for fill in the blank :::"",best_guess)
</code></pre>

<p>OUTPUT:</p>

<pre><code>['While', 'United', 'States', '[MASK]', 'he', '##ed', 'human', 'rights', ',']
</code></pre>

<p>[4]</p>

<pre><code>sentence : While United States [MASK] heed human rights,


['m u s t', 'c i t i z e n s', 's h o u l d', 'c a n n o t', 'l a w s', 'd o e s', 'g e n e r a l l y', 'd i d', 'a l w a y s', 'l a w', ',', 'g o v e r n m e n t', 'd o', 'p o l i t i c i a n s', 'm a y', 'd e f e n d e r s', 'c o u n t r i e s', 'c a n', 'o f f i c i a l s', 'g o v e r n m e n t s', 'w i l l', 'G o v e r n m e n t', 'v a l u e s', 'C o n s t i t u t i o n', 'p e o p l e']

Best guess for fill in the blank :::  m u s t
</code></pre>
"
62235153,huggingface transformers bert model without classification layer,"<p>I want to do a joint-embedding from vgg16 and <code>bert</code> for classification.</p>

<p>The thing with <code>huggingface transformers bert</code> is that it has the classification layer which has <code>num_labels</code> dimension.</p>

<p>But, I want the output from <code>BertPooler</code> (768 dimensions) which I will use as a text-embedding for an extended model.</p>

<pre><code>from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
</code></pre>

<p>This gives the following model:</p>

<pre><code>BertForSequenceClassification(
...
...
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
</code></pre>

<p>How can I get rid of the <code>classifier</code> layer?</p>
"
62317723,Tokens to Words mapping in the tokenizer decode step huggingface?,"<p>Is there a way to know the mapping from the tokens back to the original words in the <code>tokenizer.decode()</code> function?<br>
For example:</p>

<pre class=""lang-py prettyprint-override""><code>from transformers.tokenization_roberta import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)

str = ""This is a tokenization example""
tokenized = tokenizer.tokenize(str) 
## ['this', 'Ä is', 'Ä a', 'Ä token', 'ization', 'Ä example']

encoded = tokenizer.encode_plus(str) 
## encoded['input_ids']=[0, 42, 16, 10, 19233, 1938, 1246, 2]

decoded = tokenizer.decode(encoded['input_ids']) 
## '&lt;s&gt; this is a tokenization example&lt;/s&gt;'
</code></pre>

<p>And the objective is to have a function that maps each token in the <code>decode</code> process to the correct input word, for here it will be:<br>
<code>desired_output = [[1],[2],[3],[4,5],[6]]</code><br> As <code>this</code> corresponds to id <code>42</code>, while <code>token</code> and <code>ization</code> corresponds to ids <code>[19244,1938]</code> which are at indexes <code>4,5</code> of the <code>input_ids</code> array.</p>
"
62386631,Cannot import BertModel from transformers,"<p>I am trying to import BertModel from transformers, but it fails. This is code I am using</p>

<pre><code>from transformers import BertModel, BertForMaskedLM
</code></pre>

<p>This is the error I get</p>

<pre><code>ImportError: cannot import name 'BertModel' from 'transformers'
</code></pre>

<p>Can anyone help me fix this?</p>
"
62405155,BertWordPieceTokenizer vs BertTokenizer from HuggingFace,"<p>I have the following pieces of code and trying to understand the difference between BertWordPieceTokenizer and BertTokenizer.</p>
<h1><strong>BertWordPieceTokenizer (Rust based)</strong></h1>
<pre><code>from tokenizers import BertWordPieceTokenizer

sequence = &quot;Hello, y'all! How are you Tokenizer ðŸ˜ ?&quot;
tokenizer = BertWordPieceTokenizer(&quot;bert-base-uncased-vocab.txt&quot;)
tokenized_sequence = tokenizer.encode(sequence)
print(tokenized_sequence)
&gt;&gt;&gt;Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])

print(tokenized_sequence.tokens)
&gt;&gt;&gt;['[CLS]', 'hello', ',', 'y', &quot;'&quot;, 'all', '!', 'how', 'are', 'you', 'token', '##izer', '[UNK]', '?', '[SEP]']
</code></pre>
<h1>BertTokenizer</h1>
<pre><code>from transformers import BertTokenizer
tokenizer = BertTokenizer(&quot;bert-base-cased-vocab.txt&quot;)
tokenized_sequence = tokenizer.encode(sequence)
print(tokenized_sequence)
#Output: [19082, 117, 194, 112, 1155, 106, 1293, 1132, 1128, 22559, 17260, 100, 136]
</code></pre>
<ol>
<li>Why is encode working differently in both ? In BertWordPieceTokenizer it gives Encoding object while in BertTokenizer it gives the ids of the vocab.</li>
<li>What is the Difference between BertWordPieceTokenizer and BertTokenizer fundamentally, because as I understand BertTokenizer also uses WordPiece under the hood.</li>
</ol>
<p>Thanks</p>
"
62405867,"Error Running ""config = RobertaConfig.from_pretrained( ""/Absolute-path-to/BERTweet_base_transformers/config.json""""","<p>I'm trying to run the code 'transformers' version of <a href=""https://github.com/VinAIResearch/BERTweet#transformers"" rel=""nofollow noreferrer"">this code</a> to use the new pre-trained BERTweet model and I'm getting an error. </p>

<p>The following lines of code ran successfully in my Google Colab notebook:</p>

<pre><code>
!pip install fairseq
import fairseq
!pip install fastBPE
import fastBPE

# download the pre-trained BERTweet model zipped file
!wget https://public.vinai.io/BERTweet_base_fairseq.tar.gz

# unzip the pre-trained BERTweet model files
!tar -xzvf BERTweet_base_fairseq.tar.gz

!pip install transformers
import transformers

import torch
import argparse

from transformers import RobertaConfig
from transformers import RobertaModel

from fairseq.data.encoders.fastbpe import fastBPE
from fairseq.data import Dictionary

</code></pre>

<p>Then I tried to run the following code:</p>

<pre><code># Load model
config = RobertaConfig.from_pretrained(
    ""/Absolute-path-to/BERTweet_base_transformers/config.json""
)
BERTweet = RobertaModel.from_pretrained(
    ""/Absolute-path-to/BERTweet_base_transformers/model.bin"",
    config=config
)
</code></pre>

<p>...and an error was displayed:</p>

<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    242             if resolved_config_file is None:
--&gt; 243                 raise EnvironmentError
    244             config_dict = cls._dict_from_json_file(resolved_config_file)

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
2 frames
/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    250                 f""- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n""
    251             )
--&gt; 252             raise EnvironmentError(msg)
    253 
    254         except json.JSONDecodeError:

OSError: Can't load config for '/Absolute-path-to/BERTweet_base_transformers/config.json'. Make sure that:

- '/Absolute-path-to/BERTweet_base_transformers/config.json' is a correct model identifier listed on 'https://huggingface.co/models'

- or '/Absolute-path-to/BERTweet_base_transformers/config.json' is the correct path to a directory containing a config.json file

</code></pre>

<p>I'm guessing the issue is that I need to replace '/Absolute-path-to' with something else but if that's the case what should it be replaced with? It's likely a very simple answer and I feel stupid for asking but I need help.</p>
"
62422590,Do I need to pre-tokenize the text first before using HuggingFace's RobertaTokenizer? (Different undersanding),"<p>I feel confused when using the Roberta tokenizer in Huggingface. </p>

<pre><code>&gt;&gt;&gt; tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ (big) than the dog."")
['The', 'Ä tiger', 'Ä is', 'Ä ___', 'Ä (', 'big', ')', 'Ä than', 'Ä the', 'Ä dog', '.']
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ ( big ) than the dog."")
['The', 'Ä tiger', 'Ä is', 'Ä ___', 'Ä (', 'Ä big', 'Ä )', 'Ä than', 'Ä the', 'Ä dog', '.']
&gt;&gt;&gt; x = tokenizer.encode(""The tiger is ___ (big) than the dog."")
[0, 20, 23921, 16, 2165, 36, 8527, 43, 87, 5, 2335, 4, 2]
&gt;&gt;&gt; x = tokenizer.encode(""The tiger is ___ ( big ) than the dog."")
[0, 20, 23921, 16, 2165, 36, 380, 4839, 87, 5, 2335, 4, 2]
&gt;&gt;&gt;
</code></pre>

<p><strong>Question</strong>: <code>(big)</code> and <code>( big )</code> have different tokenization results, which result in different token id as well. Which one I should use? Does it mean that I should pre-tokenize the input first to make it <code>( big )</code> and go for RobertaTokenization?  Or it doesn't really matter?</p>

<p>Secondly, it seems <code>BertTokenizer</code> has no such confusion:</p>

<pre><code>&gt;&gt;&gt; tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ (big) than the dog."")
['the', 'tiger', 'is', '_', '_', '_', '(', 'big', ')', 'than', 'the', 'dog', '.']
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ ( big ) than the dog."")
['the', 'tiger', 'is', '_', '_', '_', '(', 'big', ')', 'than', 'the', 'dog', '.']
&gt;&gt;&gt;
</code></pre>

<p><code>BertTokenizer</code> gives me the same results using the wordpieces. </p>

<p>Any thoughts to help me better understand the RobertaTokenizer, which I know is using Byte-Pair Encoding?</p>
"
62434075,Getting started: Huggingface Model Cards,"<p>I just recently started looking into the huggingface transformer library.
When I tried to get started using the model card code at e.g. <a href=""https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"" rel=""nofollow noreferrer"">community model</a> </p>

<pre><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
model = AutoModel.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
</code></pre>

<p>However, I got the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 2, in &lt;module&gt;
    tokenizer = AutoTokenizer.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
  File ""/Users/Lukas/miniconda3/envs/nlp/lib/python3.7/site-packages/transformers/tokenization_auto.py"", line 124, in from_pretrained
    ""'xlm', 'roberta', 'ctrl'"".format(pretrained_model_name_or_path))
ValueError: Unrecognized model identifier in emilyalsentzer/Bio_ClinicalBERT. Should contains one of 'bert', 'openai-gpt', 'gpt2', 'transfo-xl', 'xlnet', 'xlm', 'roberta', 'ctrl'
</code></pre>

<p>If I try a different tokenizer such as ""baykenney/bert-base-gpt2detector-topp92"" I get the following error:</p>

<pre><code>OSError: Model name 'baykenney/bert-base-gpt2detector-topp92' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'baykenney/bert-base-gpt2detector-topp92' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
</code></pre>

<p>Did I miss anything to get started? I feel like the model cards indicate that these three lines of code should should be enough to get started.</p>

<p>I am using Python 3.7 and the transformer library version 2.1.1 and pytorch 1.5. </p>
"
62435022,"Where in the code of pytorch or huggingface/transformer label gets ""renamed"" into labels?","<p>My question concerns the example, available in the great huggingface/transformers library.</p>
<p>I am using a <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/trainer/01_text_classification.ipynb#scrollTo=uBzDW1FO63pK"" rel=""nofollow noreferrer"">notebook</a>, provided by library creators as a starting point for my pipeline. It presents a pipeline of finetuning a BERT for Sentence Classification on Glue dataset.</p>
<p>When getting into the code, I noticed a very weird thing, which I cannot explain.</p>
<p>In the example, input data is introduced to the model as the instances of the <code>InputFeatures</code> class from <a href=""https://github.com/huggingface/transformers/blob/011cc0be51cf2eb0a91333f1a731658361e81d89/src/transformers/data/processors/utils.py"" rel=""nofollow noreferrer"">here</a>:</p>
<p>This class has 4 attributes, including the <strong>label</strong> attribute:</p>
<pre class=""lang-py prettyprint-override""><code>class InputFeatures:
    ...
    input_ids: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    label: Optional[Union[int, float]] = None
</code></pre>
<p>which are later passed as a dictionary of inputs to the <code>forward()</code> method of the model. This is done by the <code>Trainer</code> class, for example in the lines 573-576 <a href=""https://github.com/huggingface/transformers/blob/edcb3ac59ab05d9afbc6b4f7bebfb2e5dfc662d2/src/transformers/trainer.py"" rel=""nofollow noreferrer"">here</a>:</p>
<pre class=""lang-py prettyprint-override""><code>    def _training_step(
        self, model: nn.Module, inputs: Dict[str, torch.Tensor], optimizer: torch.optim.Optimizer
    ) -&gt; float:
        model.train()
        for k, v in inputs.items():
            inputs[k] = v.to(self.args.device)

        outputs = model(**inputs)  
</code></pre>
<p>However, the <code>forward()</code> method expects <strong>labels</strong> (note the plural form) input parameter (taken from <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_distilbert.html#DistilBertForSequenceClassification"" rel=""nofollow noreferrer"">here</a>):</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
    ):
</code></pre>
<p>So my question is <strong>where does the label become labels</strong> in this pipeline?</p>
<p>To give some extra info on the issue, I created my own pipeline, which uses nothing, related, with Glue data and pipe, basically it relies only on the <code>Trainer</code> class of transformers. I even use another model (Flaubert). I replicated the InputFeature class and my code works for both cases below:</p>
<pre class=""lang-py prettyprint-override""><code>class InputFeature:
    def __init__(self, text, label):
        self.input_ids = text
        self.label = label

class InputFeaturePlural:
    def __init__(self, text, label):
        self.input_ids = text
        self.labels = label
</code></pre>
<p>But it does not work if I name the second attribute as <code>self.labe</code> or by any other names. <strong>Why is it possible to use both attribute names?</strong></p>
<p>It's not like it is extremely important in my case, but I feel uncomfortable passing around the data in the variable, which &quot;changes name&quot; somewhere along the way.</p>
"
62462878,Customize the encode module in huggingface bert model,"<p>I am working on a text classification project using <a href=""https://huggingface.co/transformers/glossary.html#token-type-ids"" rel=""nofollow noreferrer"">Huggingface transformers module</a>. The encode_plus function provides the users with a convenient way of generating the input ids, attention masks, token type ids, etc. For instance:</p>

<pre><code>from transformers import BertTokenizer

pretrained_model_name = 'bert-base-cased'
bert_base_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

sample_text = 'Bamboo poles, â€installation by an unknown building constructor #discoverhongkong #hongkonginsta'

encoding = bert_base_tokenizer.encode_plus(
        cleaned_tweet, hashtag_string,
        max_length=70,
        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
        return_token_type_ids=True,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',  # Return PyTorch tensors
    )

print('*'*20)
print(encoding['input_ids'])
print(encoding['attention_mask'])
print(encoding['token_type_ids'])
print('*'*20)
</code></pre>

<p>However, my current project requires me to generate <strong>customized ids</strong> for a given text. For instance, for a list of words <code>[HK, US, UK]</code>, I want to generate ids for these words and let other words' ids which do not exist in this list as zero. These ids are used to find embedding in another customized embedding matrix, not from pretrained bert module.</p>

<p>How can I achieve this kind of customized encoder? Any suggestions and solutions are welcomed! Thanks~</p>
"
62466514,Shall we lower case input data for (pre) training a BERT uncased model using huggingface?,"<p>Shall we lower case input data for (pre) training a BERT uncased model using huggingface? I looked into this response from Thomas Wolf (<a href=""https://github.com/huggingface/transformers/issues/92#issuecomment-444677920"" rel=""noreferrer"">https://github.com/huggingface/transformers/issues/92#issuecomment-444677920</a>) but not entirely sure if he meant that. </p>

<p>What happens if we lowercase the text ? </p>
"
62472438,"With the HuggingFace transformer, how can I return multiple samples when generating text?","<p>I'm going off of <a href=""https://github.com/cortexlabs/cortex/blob/master/examples/pytorch/text-generator/predictor.py"" rel=""nofollow noreferrer"">https://github.com/cortexlabs/cortex/blob/master/examples/pytorch/text-generator/predictor.py</a></p>

<p>But if I pass <code>num_samples=5</code>, I get:</p>

<pre><code>    generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)
RuntimeError: Sizes of tensors must match except in dimension 1. Got 5 and 1 in dimension 0
</code></pre>

<p>the code is:</p>

<pre><code>def sample_sequence(
    model,
    length,
    context,
    num_samples=1,
    temperature=1,
    top_k=0,
    top_p=0.9,
    repetition_penalty=1.0,
    device=""cpu"",
):
    context = torch.tensor(context, dtype=torch.long, device=device)
    context = context.unsqueeze(0).repeat(num_samples, 1)
    print('context_size', context.shape)
    generated = context
    print('context', context)
    with torch.no_grad():
        for _ in trange(length):
            inputs = {""input_ids"": generated}
            outputs = model(
                **inputs
            )  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)
            next_token_logits = outputs[0][0, -1, :] / (temperature if temperature &gt; 0 else 1.0)

            # reptition penalty from CTRL (https://arxiv.org/abs/1909.05858)
            for _ in set(generated.view(-1).tolist()):
                next_token_logits[_] /= repetition_penalty

            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)
            if temperature == 0:  # greedy sampling:
                next_token = torch.argmax(filtered_logits).unsqueeze(0)
            else:
                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)
            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)
    return generated
</code></pre>
"
62487267,Fine Tuning Bert on Medical Dataset,"<p>I would like to use a language model such as Bert to get a feature vector for a certain text describing a medical condition.</p>
<p>As there are many words in the text unknown to most pre-trained models and tokenizers, I wonder which steps are required to achieve this task?</p>
<p>Using a pre-trained model seems beneficial to me since the dataset describing the medical conditions is quite small.</p>
"
62525680,Save only best weights with huggingface transformers,"<p>Currently, I'm building a new transformer-based model with huggingface-transformers, where attention layer is different from the original one. I used <code>run_glue.py</code> to check performance of my model on GLUE benchmark. However, I found that Trainer class of huggingface-transformers saves all the checkpoints that I set, where I can set the maximum number of checkpoints to save. However, I want to save only the weight (or other stuff like optimizers) with <strong>best</strong> performance on validation dataset, and current Trainer class doesn't seem to provide such thing. (If we set the maximum number of checkpoints, then it removes older checkpoints, not ones with worse performances). <a href=""https://github.com/huggingface/transformers/issues/2675"" rel=""noreferrer"">Someone already asked about same question on Github</a>, but I can't figure out how to modify the script and do what I want. Currently, I'm thinking about making a custom Trainer class that inherits original one and change the <code>train()</code> method, and it would be great if there's an easy and simple way to do this. Thanks in advance.</p>
"
62538079,Hugginface transformers module not recognized by anaconda,"<p>I am using Anaconda, python 3.7, windows 10.</p>
<p>I tried to install transformers by <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/</a> on my env.
I am aware that I must have either pytorch or TF installed, I have pytorch installed - as seen in anaconda navigator environments.</p>
<p>I would get many kinds of errors, depending on where (anaconda / prompt) I uninstalled and reinstalled pytorch and transformers. Last attempt using
conda install pytorch torchvision cpuonly -c pytorch and
conda install -c conda-forge transformers
I get an error:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

def tok(dataset):
    input_ids = []
    attention_masks = []
    sentences = dataset.Answer2EN.values
    labels = dataset.Class.values
    for sent in sentences:
        encoded_sent = bert_tokenizer.encode(sent, 
                                             add_special_tokens=True,
                                             max_length = 64,
                                             pad_to_max_length =True)
</code></pre>
<blockquote>
<p>TypeError: _tokenize() got an unexpected keyword argument
'pad_to_max_length'</p>
</blockquote>
<p><strong>Does anyone know a secure installation of transformers using Anaconda?</strong>
Thank you</p>
"
62591068,NLP : Get 5 best candidates from QuestionAnsweringPipeline,"<p>I am working on a French Question-Answering model using huggingface transformers library. I'm using a pre-trained CamemBERT model which is very similar to RoBERTa but is adapted to french.</p>
<p>Currently, i am able to get the best answer candidate for a question on a text of my own, using the QuestionAnsweringPipeline from the transformers library.</p>
<p>Here is an extract of my code.</p>
<pre><code>QA_model = &quot;illuin/camembert-large-fquad&quot;
CamTokQA = CamembertTokenizer.from_pretrained(QA_model)
CamQA = CamembertForQuestionAnswering.from_pretrained(QA_model)

device_pipeline = 0 if torch.cuda.is_available() else -1
q_a_pipeline = QuestionAnsweringPipeline(model=CamQA,
                                         tokenizer=CamTokQA,
                                         device=device_pipeline)

ctx = open(&quot;text/Sample.txt&quot;, &quot;r&quot;).read()
question = 'Quel est la taille de la personne ?'
res = q_a_pipeline({'question': question, 'context': ctx})
print(res)
</code></pre>
<p>I am currently getting this :<code>{'score': 0.9630325870663725, 'start': 2421, 'end': 2424, 'answer': '{21'} </code>, which is wrong.</p>
<p>Therefore, i would like to get the 5 best candidates for the answer. Does anyone have an idea how to do that ?</p>
"
62592468,learning rate AdamW Optimizer,"<p>I train with BERT (from huggingface) sentiment analysis which is a NLP task.</p>
<p>My question refers to the learning rate.</p>
<pre><code>EPOCHS = 5                                                                                                                                                                                
optimizer = AdamW(model.parameters(), lr=1e-3, correct_bias=True)                  
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(                                    
  optimizer,
  num_warmup_steps=0,                                                          
  num_training_steps=total_steps
)
loss_fn = nn.CrossEntropyLoss().to(device)
</code></pre>
<p>Can you please explain how to read 1e-3?</p>
<p>Is this the density of steps or is this a value to decay.</p>
<p>If the latter, is it a linear decay?</p>
<p>If I train with a value 3e-5, which is a recommended value of huggingface for NLP tasks, my model overfits very quickly: loss for training decreases to a minimum, loss for validation increases.</p>
<p>Learning rate 3e-5:</p>
<p><img src=""https://i.stack.imgur.com/yPkqa.png"" alt=""3e-5"" /></p>
<p>If I train with a value of 1e-2, I get a steady improvement in the loss value of validation. but the validation accuracy does not improve after the first epoch. See picture. Why does the validation value not increase, even though the loss falls. Isn't that a contradiction? I thought these two values were an interpretation of each other.</p>
<p>Learning rate 1e-2:</p>
<p><img src=""https://i.stack.imgur.com/lEqAc.png"" alt=""1e-2"" /></p>
<p>What would you recommend?</p>
"
62327803,Having 6 labels instead of 2 in Hugging Face BertForSequenceClassification,"<p>I was just wondering if it is possibel to extend the HuggingFace <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> model to more than 2 labels. The docs say, we can pass positional arguments, but it seems like ""labels"" is not working. Does anybody has an idea?</p>

<h2>Model assignment</h2>

<pre class=""lang-py prettyprint-override""><code>labels = th.tensor([0,0,0,0,0,0], dtype=th.long).unsqueeze(0)
print(labels.shape)
modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', 
    labels=labels
    )

l = [module for module in modelBERTClass.modules()]
l
</code></pre>

<h2>Console Output</h2>

<pre><code>torch.Size([1, 6])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-122-fea9a36402a6&gt; in &lt;module&gt;()
      3 modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
      4     'bert-base-uncased',
----&gt; 5     labels=labels
      6     )
      7 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    653 
    654         # Instantiate model.
--&gt; 655         model = cls(config, *model_args, **model_kwargs)
    656 
    657         if state_dict is None and not from_tf:

TypeError: __init__() got an unexpected keyword argument 'labels'
</code></pre>
"
62302499,"Huggingface Bert, Which Bert flavor is the fastest to train for debugging?","<p>I am working with Bert and the library <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a> hugginface.
I was wondering which of the models available you would choose for debugging?</p>

<p>In other words which models trains/loads the fast on my GPU, to get runs as fast as possible?
Albert, distillbert or?</p>
"
62069350,Transformer-XL: Input and labels for Language Modeling,"<p>I'm trying to finetune the pretrained Transformer-XL model <code>transfo-xl-wt103</code> for a language modeling task. Therfore, I use the model class <code>TransfoXLLMHeadModel</code>.</p>

<p>To iterate over my dataset I use the <code>LMOrderedIterator</code> from the file <a href=""https://github.com/huggingface/transformers/blob/5e737018e1fcb22c8b76052058279552a8d6c806/src/transformers/tokenization_transfo_xl.py#L467"" rel=""nofollow noreferrer"">tokenization_transfo_xl.py</a> which yields a tensor with the <code>data</code> and its <code>target</code> for each batch (and the sequence length).</p>

<p>Let's assume the following data with <code>batch_size = 1</code> and <code>bptt = 8</code>:</p>

<pre><code>data = tensor([[1,2,3,4,5,6,7,8]])
target = tensor([[2,3,4,5,6,7,8,9]])
mems # from the previous output
</code></pre>

<p><strong>My question is:</strong> I currently pass this data into the model like this:</p>

<pre><code>output = model(input_ids=data, labels=target, mems=mems)
</code></pre>

<p>Is this correct?</p>

<p>I am wondering because the documentation says for the <code>labels</code> parameter:</p>

<blockquote>
  <p>labels (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(batch_size, sequence_length)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
              Labels for language modeling.
              Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set <code>lm_labels = input_ids</code></p>
</blockquote>

<p>So what is it about the parameter <code>lm_labels</code>? I only see <code>labels</code> defined in the <code>forward</code> method.</p>

<p>And when the labels ""are shifted"" inside the model, does this mean I have to pass <code>data</code> twice (additionally instead of <code>targets</code>) because its shifted inside? But how does the model then know the next token to predict?</p>

<p>I also read through <a href=""https://github.com/huggingface/transformers/issues/3711"" rel=""nofollow noreferrer"">this bug</a> and the fix in <a href=""https://github.com/huggingface/transformers/pull/3716"" rel=""nofollow noreferrer"">this pull request</a> but I don't quite understand how to treat the model now (before vs. after fix)</p>

<p>Thanks in advance for some help!</p>

<p><strong>Edit</strong>: <a href=""https://github.com/huggingface/transformers/issues/4698"" rel=""nofollow noreferrer"">Link</a> to issue on Github</p>
"
61916760,Using huggingface transformers with a non English language,"<p>I have installed the latest version of transformers and I was able to use its simple syntax to make sentiment prediction of English phrases:</p>

<pre><code>from transformers import pipeline
sentimentAnalysis = pipeline(""sentiment-analysis"")
print(sentimentAnalysis(""Transformers piplines are easy to use""))
HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_â€¦

HBox(children=(FloatProgress(value=0.0, description='Downloading', max=629.0, style=ProgressStyle(description_â€¦

HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_â€¦

HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267844284.0, style=ProgressStyle(descriâ€¦

[{'label': 'POSITIVE', 'score': 0.9305251240730286}]

print(sentimentAnalysis(""Transformers piplines are extremely easy to use""))

[{'label': 'POSITIVE', 'score': 0.9820092916488647}]
</code></pre>

<p>However, when I tried it on a non-English language (here is Greek) I did not get the results I expected.</p>

<p>The following phrase translates in English as: <code>'This food is disgusting'</code> and I would expect I very low sentiment score which is not what I got:</p>

<pre><code>print(sentimentAnalysis(""Î‘Ï…Ï„ÏŒ Ï„Î¿ Ï†Î±Î³Î·Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Î±Î·Î´Î¹Î±ÏƒÏ„Î¹ÎºÏŒ""))
[{'label': 'POSITIVE', 'score': 0.7899578213691711}]
</code></pre>

<p>Here is an attempt to use the best multilingual model:</p>

<p><a href=""https://i.stack.imgur.com/X35N3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X35N3.png"" alt=""enter image description here""></a></p>

<p>Somewhat better but still widely out of target.</p>

<p>Is there something I can do about it?</p>
"
61913010,Can not import pipeline from transformers,"<p>I have installed <code>pytorch</code> with <code>conda</code> and <code>transformers</code> with <code>pip</code>.</p>

<p>I can <code>import transformers</code> without a problem but when I try to <code>import pipeline from transformers</code> I get an exception:</p>

<pre><code>from transformers import pipeline
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-4-69a9fd07ccac&gt; in &lt;module&gt;
----&gt; 1 from transformers import pipeline

ImportError: cannot import name 'pipeline' from 'transformers' (C:\Users\Alienware\Anaconda3\envs\tf2\lib\site-packages\transformers\__init__.py)
</code></pre>

<p>This is a view of the directory where it searches for the <strong>init</strong>.py file:</p>

<p><a href=""https://i.stack.imgur.com/cDIK7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cDIK7.png"" alt=""enter image description here""></a></p>

<p>What is causing the problem and how can I resolve it?</p>
"
62703391,Estimate token probability/logits given a sentence without computing the entire sentence,"<p>I have a sentence like:  <code>&quot;I like sitting in my new chair and _____ about life&quot;</code>.</p>
<p>And I have a SPECIFIC set of tokens like <code>[&quot;watch&quot;, &quot;run&quot;, &quot;think&quot;, &quot;apple&quot;, &quot;light&quot;]</code></p>
<p>I would like to calculate the probability of each of those tokens to appear as the next word in that incomplete sentence. Hopefully I should get that the probability of <code>&quot;think&quot;</code> is higher that <code>&quot;apple&quot;</code> for instance.</p>
<p>I am working with pytorch-transformers (GPT2LMHeadModel specifically), and a possible solution is to evaluate the score of the full sentence with each of the tokens, but when number of tokens to evaluate is on the order of 100 or 1000 then the computation time starts to be too long.</p>
<p>It must be possible to process the sentence only once and somehow use the hidden states to calculate the probabilities of the set of tokens, but I don't know how to do it.</p>
<p>Any ideas? Thanks in advance</p>
<hr />
<p>EDIT:</p>
<p>The actual code looks like the one below (estimating the probability for the full sentence every time). For every sentence it takes about 0.1 seconds to run the <code>score()</code> method, which turns into hours if I want to evaluate some thousands of words.</p>
<pre><code>from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel
import pandas as pd

model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
model.eval()
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)


def score(sentence):
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    loss = model(tensor_input, labels=tensor_input)
    return -loss[0].item()


candidates = [&quot;watch&quot;, &quot;run&quot;, &quot;think&quot;, &quot;apple&quot;, &quot;light&quot;]
sent_template = &quot;I like sitting in my new chair and {} about life&quot;
print({candidate: score(sent_template.format(candidate)) for candidate in candidates})
</code></pre>
"
62746180,ImportError: cannot import name 'hf_bucket_url' in HuggingFace Transformers,"<p>So I installed the latest version of transformers on Google Colab</p>
<pre><code>!pip install transformers 
</code></pre>
<p>When trying to invoke the conversion file using</p>
<pre><code>!python /usr/local/lib/python3.6/dist-packages/transformers/convert_pytorch_checkpoint_to_tf2.py .py --help  
</code></pre>
<p>Or trying to use</p>
<pre><code>from transformers.file_utils import hf_bucket_url.                                 // works 
from transformers.convert_pytorch_checkpoint_to_tf2 import *.                      // fails

convert_pytorch_checkpoint_to_tf(&quot;gpt2&quot;, pytorch_file, config_file, tf_file).      
</code></pre>
<p>I get this error</p>
<pre><code> ImportError                               Traceback (most recent call last)

&lt;ipython-input-3-dadaf83ecea0&gt; in &lt;module&gt;()
      1 from transformers.file_utils import hf_bucket_url
----&gt; 2 from transformers.convert_pytorch_checkpoint_to_tf2 import *
      3 
      4 convert_pytorch_checkpoint_to_tf(&quot;gpt2&quot;, pytorch_file, config_file, tf_file)
/usr/local/lib/python3.6/dist-packages/transformers/convert_pytorch_checkpoint_to_tf2.py in &lt;module&gt;()
     20 import os
     21 
---&gt; 22 from transformers import (
     23     ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,
     24     BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,

ImportError: cannot import name 'hf_bucket_url'
</code></pre>
<p>What's going on?</p>
"
62782001,Using Hugging Face Transformers library how can you POS_TAG French text,"<p>I am trying to POS_TAG French using the Hugging Face Transformers library. In English I was able to do so given a sentence like e.g:</p>
<blockquote>
<p>The weather is really great. So let us go for a walk.</p>
</blockquote>
<p>the result is:</p>
<pre><code>    token   feature
0   The     DET
1   weather NOUN
2   is      AUX
3   really  ADV
4   great   ADJ
5   .       PUNCT
6   So      ADV
7   let     VERB
8   us      PRON
9   go      VERB
10  for     ADP
11  a       DET
12  walk    NOUN
13  .       PUNCT
</code></pre>
<p>Does anyone have an idea how a similar thing could be achieved for French?</p>
<p>This is the code I used for the English version in a Jupyter notebook:</p>
<pre><code>!git clone https://github.com/bhoov/spacyface.git
!python -m spacy download en_core_web_sm

from transformers import pipeline
import numpy as np
import pandas as pd

nlp = pipeline('feature-extraction')
sequence = &quot;The weather is really great. So let us go for a walk.&quot;
result = nlp(sequence)
# Just displays the size of the embeddings. The sequence
# In this case there are 16 tokens and the embedding size is 768
np.array(result).shape

import sys
sys.path.append('spacyface')

from spacyface.aligner import BertAligner

alnr = BertAligner.from_pretrained(&quot;bert-base-cased&quot;)
tokens = alnr.meta_tokenize(sequence)
token_data = [{'token': tok.token, 'feature': tok.pos} for tok in tokens]
pd.DataFrame(token_data)
</code></pre>
<p>The output of this notebook is above.</p>
"
62830783,"Scripts missing for GPT-2 fine tune, and inference in Hugging-face GitHub?","<p>I am following the <a href=""https://huggingface.co/transformers/v2.0.0/examples.html"" rel=""nofollow noreferrer"">documentation</a> on the hugging face website, in there they say that to fine-tune GPT-2 I should use the script
<a href=""https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"" rel=""nofollow noreferrer"">run_lm_finetuning.py</a> for fine-tuning, and the script <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_generation.py"" rel=""nofollow noreferrer"">run_generation.py</a>
for inference.
However, both scripts don't actually exist on GitHub anymore.</p>
<p>Does anybody know whether the documentation is outdated? or where to find those two scripts?</p>
<p>Thanks</p>
"
62978957,Sliding window for long text in BERT for Question Answering,"<p>I've read post which explains how the sliding window works but I cannot find any information on how it is actually implemented.</p>
<p>From what I understand if the input are too long, sliding window can be used to process the text.</p>
<p>Please correct me if I am wrong.
Say I have a text <em><strong>&quot;In June 2017 Kaggle announced that it passed 1 million registered users&quot;</strong></em>.</p>
<p>Given some <code>stride</code> and <code>max_len</code>, the input can be split into chunks with over lapping words (not considering padding).</p>
<pre><code>In June 2017 Kaggle announced that # chunk 1
announced that it passed 1 million # chunk 2
1 million registered users # chunk 3
</code></pre>
<p>If my questions were <em><strong>&quot;when did Kaggle make the announcement&quot;</strong></em> and <em><strong>&quot;how many registered users&quot;</strong></em> I can use <code>chunk 1</code> and <code>chunk 3</code> and <strong>not use</strong> <code>chunk 2</code> <strong>at all</strong> in the model. Not quiet sure if I should still use <code>chunk 2</code> to train the model</p>
<p>So the input will be:
<code>[CLS]when did Kaggle make the announcement[SEP]In June 2017 Kaggle announced that[SEP]</code>
and
<code>[CLS]how many registered users[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>Then if I have a question with no answers do I feed it into the model with all chunks like and indicate the starting and ending index as <strong>-1</strong>? For example <em><strong>&quot;can pigs fly?&quot;</strong></em></p>
<p><code>[CLS]can pigs fly[SEP]In June 2017 Kaggle announced that[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]announced that it passed 1 million[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>As suggested in the comments, II tried to run <code>squad_convert_example_to_features</code> (<a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L134"" rel=""noreferrer"">source code</a>) to investigate the problem I have above, but it doesn't seem to work, nor there are any documentation. It seems like <code>run_squad.py</code> from huggingface uses <code>squad_convert_example_to_features</code> with the <code>s</code> in <code>example</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_example_to_features
from transformers import AutoTokenizer, AutoConfig, squad_convert_examples_to_features

FILE_DIR = &quot;.&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
processor = SquadV2Processor()
examples = processor.get_train_examples(FILE_DIR)

features = squad_convert_example_to_features(
    example=examples[0],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
</code></pre>
<p>I get the error.</p>
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00&lt;00:00, 159.95it/s]
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 25, in &lt;module&gt;
    sub_tokens = tokenizer.tokenize(token)
NameError: name 'tokenizer' is not defined
</code></pre>
<p>The error indicates that there are no <code>tokenizers</code> but it does not allow us to pass a <code>tokenizer</code>. Though it does work if I add a tokenizer while I am inside the function in debug mode. So how exactly do I use the <code>squad_convert_example_to_features</code> function?</p>
"
63020991,cannot import name 'pipline' from 'transformers' (unknown location),"<p>I am getting this error when trying following code in the jupyter-lab:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
</code></pre>
<p>Amazingly, if I copy that line of code in a <code>code_test.py</code> file, and execute it using <code>python3 code_test.py</code>(both in the terminal and jupyter-lab itself) everything will work fine.</p>
<p>I am using jupyter-lab and which is configured to use a virtual-env(the one containing transformers module).</p>
<p>I have searched for similar problems, but none of proposed solutions worked(such as reinstalling the <strong>transformers</strong> module).</p>
<p><strong>Edited</strong>:</p>
<p>Output of <code>sys.path</code> in jupyter-lab:</p>
<pre class=""lang-py prettyprint-override""><code>['/Users/{my_username}/{path_to_my_project}/code',
 '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip',
 '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7',
 '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload',
 '',
 '/Users/{my_username}/Library/Python/3.7/lib/python/site-packages',
 '/usr/local/lib/python3.7/site-packages',
 '/Users/{my_username}/Library/Python/3.7/lib/python/site-packages/IPython/extensions',
 '/Users/{{my_username}/.ipython']
</code></pre>
<p>Output of sys.path in <code>code_test.py</code>:</p>
<pre><code>['/Users/{my_username}/{path_to_my_project}/code',
'/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip',
'/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7',
'/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload',
'/Users/{my_username}/{path_to_my_project}/code/env/lib/python3.7/site-packages']
</code></pre>
"
63141267,ImportError: cannot import name 'AutoModelWithLMHead' from 'transformers',"<p>This is literally all the code that I am trying to run:</p>
<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
</code></pre>
<p>I am getting this error:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-14-aad2e7a08a74&gt; in &lt;module&gt;
----&gt; 1 from transformers import AutoModelWithLMHead, AutoTokenizer
      2 import torch
      3 
      4 tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
      5 model = AutoModelWithLMHead.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)

ImportError: cannot import name 'AutoModelWithLMHead' from 'transformers' (c:\python38\lib\site-packages\transformers\__init__.py)
</code></pre>
<p>What do I do about it?</p>
"
63152188,HuggingFace Transformers: BertTokenizer changing characters,"<p>I have downloaded the Norwegian BERT-model from <a href=""https://github.com/botxo/nordic_bert"" rel=""nofollow noreferrer"">https://github.com/botxo/nordic_bert</a>, and loaded it in using:</p>
<pre><code>import transformers as t

model_class = t.BertModel
tokenizer_class = t.BertTokenizer

tokenizer = tokenizer_class.from_pretrained(/PATH/TO/MODEL/FOLDER)
model = model_class.from_pretrained(/PATH/TO/MODEL)
model.eval()
</code></pre>
<p>This works very well, however when i try to tokenize a given sentence, some nordic characters such as &quot;Ã¸&quot; and &quot;Ã¦&quot; remain the same, whereas all words having the char &quot;Ã¥&quot; is replaced with &quot;a&quot;.
For instance:</p>
<pre><code>s = &quot;Ã¦ Ã¸ Ã¥ lÃ¸pe fÃ¥ Ã¦rfugl&quot;
print(tokenizer.tokenize(s))
</code></pre>
<p>Yields:</p>
<pre><code>['Ã¦', 'Ã¸', 'a', 'lÃ¸p', '##e', 'fa', 'Ã¦r', '##fugl']
</code></pre>
<p>Thanks</p>
"
63201036,Add additional layers to the Huggingface transformers,"<p>I want to add additional <code>Dense</code> layer after pretrained <code>TFDistilBertModel</code>, <code>TFXLNetModel</code> and <code>TFRobertaModel</code> Huggingface models. I have already seen how I can do this with the <code>TFBertModel</code>, e.g. <a href=""https://www.kaggle.com/dhruv1234/huggingface-tfbertmodel"" rel=""noreferrer"">in this notebook</a>:</p>
<pre><code>output = bert_model([input_ids,attention_masks])
output = output[1]
output = tf.keras.layers.Dense(32,activation='relu')(output)
</code></pre>
<p>So, here I need to use the second item(i.e. item with index <code>1</code>) of the <code>BERT</code> output tuple. According to the <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">docs</a> <code>TFBertModel</code> has <code>pooler_output</code> at this tuple index. But the other three models don't have <code>pooler_output</code>.</p>
<p>So, how can I add additional layers to the other three model outputs?</p>
"
63211463,Error Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select,"<p>I have the following code taken directly from <a href=""https://towardsdatascience.com/simple-abstractive-text-summarization-with-pretrained-t5-text-to-text-transfer-transformer-10f6d602c426"" rel=""nofollow noreferrer"">here</a> with some pretty little modifications:</p>
<pre><code>import pandas as pd
import torch
import json 
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config
from torch import cuda

df = pd.read_pickle('df_final.pkl')

model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')
device = 'cuda' if cuda.is_available() else 'cpu'

text = ''.join(df[(df['col1'] == 'type') &amp; (df['col2'] == 2)].col3.to_list())

preprocess_text = text.strip().replace(&quot;\n&quot;,&quot;&quot;)
t5_prepared_Text = &quot;summarize: &quot;+preprocess_text
#print (&quot;original text preprocessed: \n&quot;, preprocess_text)

tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=&quot;pt&quot;, max_length = 500000).to(device)


# summmarize 
summary_ids = model.generate(tokenized_text,
                                    num_beams=4,
                                    no_repeat_ngram_size=2,
                                    min_length=30,
                                    max_length=100,
                                    early_stopping=True)

output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print (&quot;\n\nSummarized text: \n&quot;,output)
</code></pre>
<p>When executing the <code>model_generate()</code> part i get an error like this:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-12-e8e9819a85dc&gt; in &lt;module&gt;
     12                                     min_length=30,
     13                                     max_length=100,
---&gt; 14                                     early_stopping=True).to(device)
     15 
     16 output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

~\Anaconda3\lib\site-packages\torch\autograd\grad_mode.py in decorate_no_grad(*args, **kwargs)
     47         def decorate_no_grad(*args, **kwargs):
     48             with self:
---&gt; 49                 return func(*args, **kwargs)
     50         return decorate_no_grad
     51 

~\Anaconda3\lib\site-packages\transformers\generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)
    383             encoder = self.get_encoder()
    384 
--&gt; 385             encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)
    386 
    387         # Expand input ids if num_beams &gt; 1 or num_return_sequences &gt; 1

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~\Anaconda3\lib\site-packages\transformers\modeling_t5.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states, return_dict)
    701         if inputs_embeds is None:
    702             assert self.embed_tokens is not None, &quot;You have to intialize the model with valid token embeddings&quot;
--&gt; 703             inputs_embeds = self.embed_tokens(input_ids)
    704 
    705         batch_size, seq_length = input_shape

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~\Anaconda3\lib\site-packages\torch\nn\modules\sparse.py in forward(self, input)
    112         return F.embedding(
    113             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 114             self.norm_type, self.scale_grad_by_freq, self.sparse)
    115 
    116     def extra_repr(self):

~\Anaconda3\lib\site-packages\torch\nn\functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1482         # remove once script supports set_grad_enabled
   1483         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1484     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1485 
   1486 

RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select
â€‹
</code></pre>
<p>I've searched this error and fouund some other threads like <a href=""https://stackoverflow.com/questions/55278566/runtimeerror-expected-object-of-backend-cuda-but-got-backend-cpu-for-argument"">this</a> one and <a href=""https://stackoverflow.com/questions/58799486/expected-object-of-device-type-cuda-but-got-device-type-cpu-in-pytorch"">this</a> one but they didn't help me much since their case seems to be completely different. In my case there are no custom instances or classes created, so i don't know how to fix this or where the error come from.</p>
<p>Could you please tell me where is the error coming from and how could i fix it?</p>
<p>Thank you very much in advance.</p>
"
63221913,"Named Entity Recognition with Huggingface transformers, mapping back to complete entities","<p>I'm looking at the documentation for <a href=""https://huggingface.co/transformers/task_summary.html#named-entity-recognition"" rel=""noreferrer"">Huggingface pipeline for Named Entity Recognition</a>, and it's not clear to me how these results are meant to be used in an actual entity recognition model.</p>
<p>For instance, given the example in documentation:</p>
<pre><code>&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; nlp = pipeline(&quot;ner&quot;)

&gt;&gt;&gt; sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot;
...            &quot;close to the Manhattan Bridge which is visible from the window.&quot;

This outputs a list of all words that have been identified as an entity from the 9 classes     defined above. Here is the expected results:

print(nlp(sequence))

[
{'word': 'Hu', 'score': 0.9995632767677307, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9915938973426819, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9982671737670898, 'entity': 'I-ORG'},
{'word': 'Inc', 'score': 0.9994403719902039, 'entity': 'I-ORG'},
{'word': 'New', 'score': 0.9994346499443054, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9993270635604858, 'entity': 'I-LOC'},
{'word': 'City', 'score': 0.9993864893913269, 'entity': 'I-LOC'},
{'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},
{'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},
{'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},
{'word': 'Manhattan', 'score': 0.9758241176605225, 'entity': 'I-LOC'},
{'word': 'Bridge', 'score': 0.990249514579773, 'entity': 'I-LOC'}
]
</code></pre>
<p>While this alone is impressive, it isn't clear to me the correct way to get  &quot;DUMBO&quot; from:</p>
<pre><code>{'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},
{'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},
{'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},
</code></pre>
<p>---or even to the cleaner multiple token matches, like distinguishing &quot;New York City&quot; from simply the city of &quot;York.&quot;</p>
<p>While I can imagine heuristic methods, what's the correct intended way to join these tokens back into correct labels given your inputs?</p>
"
63280435,huggingface transformers: truncation strategy in encode_plus,"<p><a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"" rel=""nofollow noreferrer""><code>encode_plus</code></a> in huggingface's transformers library allows truncation of the input sequence. Two parameters are relevant: <code>truncation</code> and <code>max_length</code>. I'm passing a paired input sequence to <code>encode_plus</code> and need to truncate the input sequence simply in a &quot;cut off&quot; manner, i.e., if the whole sequence consisting of both inputs <code>text</code> and <code>text_pair</code> is longer than <code>max_length</code> it should just be truncated correspondingly from the right.</p>
<p>It seems that neither of the truncation strategies allows to do this, instead <code>longest_first</code> removes tokens from the longest sequence (which could be either text or text_pair, but not just simply from the right or end of the sequence, e.g., if text is longer that text_pair, it seems this would remove tokens from text first), <code>only_first</code> and <code>only_second</code> remove tokens from only the first or second (hence, also not simply from the end), and <code>do_not_truncate</code> does not truncate at all. Or did I misunderstood this and actually <code>longest_first</code> might be what I'm looking for?</p>
"
63387831,Memory Issue while following LM tutorial,"<p>SPECS:
OS: Windows 10
CUDA: 10.1
GPU: RTX 2060 6G VRAM (x2)
RAM: 32GB
tutorial: <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a></p>
<p>Hello I am trying to train my own language model and I have had some memory issues. I have tried to run some of this code in Pycharm on my computer and then trying to replicate in my Collab Pro Notebook.</p>
<h2>First, my code</h2>
<pre><code>from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM, LineByLineTextDataset
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments

config = RobertaConfig(vocab_size=60000, max_position_embeddings=514, num_attention_heads=12, num_hidden_layers=6,
                       type_vocab_size=1)

tokenizer = RobertaTokenizerFast.from_pretrained(&quot;./MODEL DIRECTORY&quot;, max_len=512)

model = RobertaForMaskedLM(config=config)

print(&quot;making dataset&quot;)

dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=&quot;./total_text.txt&quot;, block_size=128)

print(&quot;making c&quot;)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

training_args = TrainingArguments(output_dir=&quot;./MODEL DIRECTORY&quot;, overwrite_output_dir=True, num_train_epochs=1,
                                  per_gpu_train_batch_size=64, save_steps=10000, save_total_limit=2)
print(&quot;Building trainer&quot;)
trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=dataset,
                  prediction_loss_only=True)
trainer.train()

trainer.save_model(&quot;./MODEL DIRECTORY&quot;)

</code></pre>
<p><code>&quot;./total_text.txt&quot;</code> being a 1.7GB text file.</p>
<h2>PyCharm Attempt</h2>
<p>This code on pycharm builds the dataset and then would throw an error saying that my preferred gpu was running out of memory, and that Torch was already using 3.7GiB of memory.</p>
<p>I tried:</p>
<ul>
<li>import gc doing a gc clear to try to flush what ever was going on my gpu</li>
<li>Decreasing my batch size for my gpu (training only happened on a batch size of 8 resulting in 200,000+ epochs that all took 1.17 seconds)</li>
<li>Setting my <code>os.environ[&quot;CUDA_VISIBLE_OBJECTS&quot;] =&quot;&quot;</code> so that torch would have to use my CPU and not my GPU. Still threw same gpu memory error...</li>
</ul>
<p>So succumbing to the fact that torch, for the time being, was forcing itself to use my gpu, I decided to go to Collab.</p>
<h2>Collab Attempt</h2>
<p>Collab has different issues with my code. It does not have the memory to build the dataset, and crashes due to RAM shortages. I purchased a Pro account and then increased the usable RAM to 25GB, still memory shortages.</p>
<p>Cheers!</p>
"
63413414,Is there a way to get the location of the substring from which a certain token has been produced in BERT?,"<p>I am feeding sentences to a BERT model (Hugging Face library). These sentences get tokenized with a pretrained tokenizer. I know that you can use the decode function to go back from tokens to strings.</p>
<pre><code>string = tokenizer.decode(...)
</code></pre>
<p>However, the reconstruction is not perfect. If you use an uncased pretrained model, the uppercase letters get lost. Also, if the tokenizer splits a word into 2 tokens, the second token will start with '##'. For example, the word 'coronavirus' gets split into 2 tokens: 'corona' and '##virus'.</p>
<p>So my question is: is there a way to get the indices of the substring from which every token is created?
For example, take the string &quot;Tokyo to report nearly 370 new coronavirus cases, setting new single-day record&quot;. The 9th token is the token corresponding to 'virus'.</p>
<pre><code>['[CLS]', 'tokyo', 'to', 'report', 'nearly', '370', 'new', 'corona', '##virus', 'cases', ',', 'setting', 'new', 'single', '-', 'day', 'record', '[SEP]']
</code></pre>
<p>I want something that tells me that the token '##virus' comes from the 'virus' substring in the original string, which is located between the indices 37 and 41 of the original string.</p>
<pre><code>sentence = &quot;Tokyo to report nearly 370 new coronavirus cases, setting new single-day record&quot;
print(sentence[37:42]) # --&gt; outputs 'virus
</code></pre>
"
63461262,BERT sentence embeddings from transformers,"<p>I'm trying to get sentence vectors from hidden states in a BERT model.  Looking at the huggingface BertModel instructions <a href=""https://huggingface.co/bert-base-multilingual-cased?text=This%20sentence%20etc"" rel=""noreferrer"">here</a>, which say:</p>
<pre><code>from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
text = &quot;Replace me by any text you'd like.&quot;
encoded_input = tokenizer(text, return_tensors='pt') 
output = model(**encoded_input)
</code></pre>
<p>So first note, as it is on the website, this does /not/ run. You get:</p>
<pre><code>&gt;&gt;&gt; Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: 'BertTokenizer' object is not callable
</code></pre>
<p>But it looks like a minor change fixes it, in that you don't call the tokenizer directly, but ask it to encode the input:</p>
<pre><code>encoded_input = tokenizer.encode(text, return_tensors=&quot;pt&quot;)
output = model(encoded_input)
</code></pre>
<p>OK, that aside, the tensors I get, however, have a different shape than I expected:</p>
<pre><code>&gt;&gt;&gt; output[0].shape
torch.Size([1,11,768])
</code></pre>
<p>This is a lot of layers.  Which is the correct layer to use for sentence embeddings?  <code>[0]</code>?  <code>[-1]</code>?  Averaging several?  I have the goal of being able to do cosine similarity with these, so I need a proper 1xN vector rather than an NxK tensor.</p>
<p>I see that the popular <a href=""https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes"" rel=""noreferrer"">bert-as-a-service project</a> appears to use <code>[0]</code></p>
<p>Is this correct? Is there documentation for what each of the layers are?</p>
"
63461821,What features are used in the default transformers pipeline?,"<p>I'm looking <a href=""https://huggingface.co/transformers/main_classes/pipelines.html#featureextractionpipeline"" rel=""nofollow noreferrer"">here at the feature extraction pipeline</a>.</p>
<p>I initialize with the following:</p>
<pre><code>from transformers import pipeline 
pipe = pipeline(&quot;feature-extraction&quot;) 
features = pipe(&quot;test&quot;)
</code></pre>
<p>And I get a bunch of features.  What model is this using by default?  How can I initialize this pipeline to use a particular pre-trained model?</p>
<pre><code>len(features)
1
&gt;&gt;&gt; features
[[[0.4122459590435028, 0.10175584256649017, 0.09342928230762482, -0.3119196593761444, -0.3226662278175354, -0.16414110362529755, 0.06356583535671234, -0.03167172893881798, -0.010002809576690197, -1.1153486967086792, -0.3304346203804016, 0.1727224737405777, -0.0904250368475914, -0.04243310168385506, -0.4745883047580719, 0.09118127077817917, 0.4240476191043854, 0.2237153798341751, 0.12108077108860016, -0.16883963346481323, 0.055300742387771606, -0.07225772738456726, 0.4521999955177307, -0.31655701994895935, 0.05917530879378319, -0.0343029648065567, 0.4157347083091736, 0.10791877657175064, -0
...etc
</code></pre>
<p>While the document tells me:</p>
<blockquote>
<p>All models may be used for this pipeline. See a list of all models, including community-contributed models on huggingface.co/models.</p>
</blockquote>
<p>It's not clear to me where to initialize the models in this link.  The API is very terse.</p>
"
63478947,Correct Way to Fine-Tune/Train HuggingFace's Model from scratch (PyTorch),"<p>For example, I want to train a BERT model from scratch but using the existing configuration. Is the following code the correct way to do so?</p>
<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained('bert-base-cased')
model.init_weights()
</code></pre>
<p>Because I think the <code>init_weights</code> method will re-initialize all the weights.</p>
<p>Second question, if I want to change a bit the configuration, such as the number of hidden layers.</p>
<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained('bert-base-cased', num_hidden_layers=10)
model.init_weights()
</code></pre>
<p>I wonder if the above is the correct way to do so. Because they don't appear to have an error when I run the above code.</p>
"
63607919,Tokens returned in transformers Bert model from encode(),"<p>I have a small dataset for sentiment analysis. The classifier will be a simple KNN but I wanted to get the word embedding with the <code>Bert</code> model from the <code>transformers</code> library. Note that I just found out about this library - I am still learning.</p>
<p>So looking at online example, I am trying to understand the dimensions that are returned from the model.</p>
<p>Example:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;)
print(tokens)

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;)
print(tokens)
</code></pre>
<p>The output is the following:</p>
<pre><code>[101, 100, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 2002, 2003, 2428, 3835, 102]

[101, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]
</code></pre>
<p>I can't seem to find the docs for <code>encode()</code> - I have no idea why it returns different stuff when the input is passed as a list. What is this doing?</p>
<p>Additionally, is there a method to pass a word token and get the actual word back - to troubleshoot the above?</p>
<p>Thank you in advance</p>
"
63608183,"Torch Tensor & Input Conflicting: ""Tensor Object Is Not Callable""","<p>Due to the code &quot;torch.tensor,&quot; I am getting the error &quot;Tensor object is not callable&quot; when I add &quot;input.&quot; Does anyone know how I can fix this?</p>
<pre><code>import torch
from torch.nn import functional as F
from transformers import GPT2Tokenizer, GPT2LMHeadModel


tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

text0 = &quot;In order to&quot;
text = tokenizer.encode(&quot;In order to&quot;)
input, past = torch.tensor([text]), None


logits, past = model(input, past = past)
logits = logits[0,-1]
probabilities = torch.nn.functional.softmax(logits)
best_logits, best_indices = logits.topk(5)
best_words = [tokenizer.decode([idx.item()]) for idx in best_indices]
text.append(best_indices[0].item())
best_probabilities = probabilities[best_indices].tolist()

for i in range(5):
    f = ('Generated {}: {}'.format(i, best_words[i]))
    print(f)


option = input(&quot;Pick a Option:&quot;)
z = text0.append(option)
print(z)
</code></pre>
<p>Error stacktrace:</p>
<pre><code>TypeError                                 Traceback (most recent call last)

&lt;ipython-input-2-82e8d88e81c1&gt; in &lt;module&gt;()
     25 
     26 
---&gt; 27 option = input(&quot;Pick a Option:&quot;)
     28 z = text0.append(option)
     29 print(z)

TypeError: 'Tensor' object is not callable
</code></pre>
"
63662548,GCP AI Platform Notebook driver too old?,"<p>I am trying to run the following <a href=""https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"" rel=""nofollow noreferrer"">Hugging Face Transformers tutorial</a> on GCP's AI Platform Notebook with 32 vCPUs, 208 GB RAM, and 2 NVIDIA Tesla T4s.</p>
<p>However, when I try to run the part</p>
<p><code>model = DistillBERTClass()</code></p>
<p><code>model.to(device)</code></p>
<p>I get the following Assertion Error:</p>
<pre><code>AssertionError: The NVIDIA driver on your system is too old (found version 10010).
Please update your GPU driver by downloading and installing a new
version from the URL: http://www.nvidia.com/Download/index.aspx
Alternatively, go to: https://pytorch.org to install
a PyTorch version that has been compiled with your version
of the CUDA driver.
</code></pre>
<p>However, when I run
!nvidia-smi</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   38C    P0    22W /  70W |     10MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |
| N/A   39C    P8    10W /  70W |     10MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
</code></pre>
<p>The version on the NVIDIA driver is compatible with the latest PyTorch version, which I am using.
Has anyone else ran into this error, and is there a way around it?</p>
"
63676307,Hugging face - RuntimeError: Caught RuntimeError in replica 0 on device 0 on Azure Databricks,"<p>How do I run the run_language_modeling.py script from hugging face using the pretrained roberta case model to fine-tune  using my own data on the Azure databricks with a GPU cluster.</p>
<p>Using Transformer version 2.9.1 and 3.0 .
Python 3.6
Torch `1.5.0
torchvision 0.6</p>
<p>This is the script I ran below on Azure databricks</p>
<pre><code>%run '/dbfs/FileStore/tables/dev/run_language_modeling.py' \
  --output_dir='/dbfs/FileStore/tables/final_train/models/roberta_base_reduce_n' \
  --model_type=roberta \
  --model_name_or_path=roberta-base \
  --do_train \
  --num_train_epochs 5 \
  --train_data_file='/dbfs/FileStore/tables/final_train/train_data/all_data_desc_list_full.txt' \
  --mlm 
</code></pre>
<p>This is the error I get after running the above command.</p>
<pre><code>/dbfs/FileStore/tables/dev/run_language_modeling.py in &lt;module&gt;
   279 
   280 if __name__ == &quot;__main__&quot;:
--&gt; 281     main()

/dbfs/FileStore/tables/dev/run_language_modeling.py in main()
   243             else None
   244         )
--&gt; 245         trainer.train(model_path=model_path)
   246         trainer.save_model()
   247         # For convenience, we also re-save the tokenizer to the same directory,

/databricks/python/lib/python3.7/site-packages/transformers/trainer.py in train(self, model_path)
   497                     continue
   498 
--&gt; 499                 tr_loss += self._training_step(model, inputs, optimizer)
   500 
   501                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (

/databricks/python/lib/python3.7/site-packages/transformers/trainer.py in _training_step(self, model, inputs, optimizer)
   620             inputs[&quot;mems&quot;] = self._past
   621 
--&gt; 622         outputs = model(**inputs)
   623         loss = outputs[0]  # model outputs are always tuple in transformers (see doc)
   624 

/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
   548             result = self._slow_forward(*input, **kwargs)
   549         else:
--&gt; 550             result = self.forward(*input, **kwargs)
   551         for hook in self._forward_hooks.values():
   552             hook_result = hook(self, input, result)

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)
   153             return self.module(*inputs[0], **kwargs[0])
   154         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
--&gt; 155         outputs = self.parallel_apply(replicas, inputs, kwargs)
   156         return self.gather(outputs, self.output_device)
   157 

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in parallel_apply(self, replicas, inputs, kwargs)
   163 
   164     def parallel_apply(self, replicas, inputs, kwargs):
--&gt; 165         return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
   166 
   167     def gather(self, outputs, output_device):

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py in parallel_apply(modules, inputs, kwargs_tup, devices)
    83         output = results[i]
    84         if isinstance(output, ExceptionWrapper):
---&gt; 85             output.reraise()
    86         outputs.append(output)
    87     return outputs

/databricks/python/lib/python3.7/site-packages/torch/_utils.py in reraise(self)
   393             # (https://bugs.python.org/issue2651), so we work around it.
   394             msg = KeyErrorMessage(msg)
--&gt; 395         raise self.exc_type(msg)

RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 60, in _worker
   output = module(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_roberta.py&quot;, line 239, in forward
   output_hidden_states=output_hidden_states,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 762, in forward
   output_hidden_states=output_hidden_states,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 439, in forward
   output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 371, in forward
   hidden_states, attention_mask, head_mask, output_attentions=output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 315, in forward
   hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 240, in forward
   attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 11.17 GiB total capacity; 10.68 GiB already allocated; 95.31 MiB free; 10.77 GiB reserved in total by PyTorch)```

Please how do I resolve this
</code></pre>
"
63774619,Enhance a MarianMT pretrained model from HuggingFace with more training data,"<p>I am using a pretrained MarianMT machine translation model from <a href=""https://huggingface.co/Helsinki-NLP/opus-mt-en-de"" rel=""nofollow noreferrer"">English to German</a>. I also have a large set of high quality English-to-German sentence pairs that I would like to use to enhance the performance of the model, which is trained on the OPUS corpus, but <strong>without</strong> making the model <em>forget</em> the OPUS training data. Is there a way to do that? Thanks.</p>
"
63785319,PyTorch torch.no_grad() versus requires_grad=False,"<p>I'm following a <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"" rel=""noreferrer"">PyTorch tutorial</a> which uses the BERT NLP model (feature extractor) from the Huggingface Transformers library. There are two pieces of interrelated code for gradient updates that I don't understand.</p>
<p>(1) <code>torch.no_grad()</code></p>
<p>The tutorial has a class where the <code>forward()</code> function creates a <code>torch.no_grad()</code> block around a call to the BERT feature extractor, like this:</p>
<pre class=""lang-py prettyprint-override""><code>bert = BertModel.from_pretrained('bert-base-uncased')

class BERTGRUSentiment(nn.Module):
    
    def __init__(self, bert):
        super().__init__()
        self.bert = bert
        
    def forward(self, text):
        with torch.no_grad():
            embedded = self.bert(text)[0]
</code></pre>
<p>(2) <code>param.requires_grad = False</code></p>
<p>There is another portion in the same tutorial where the BERT parameters are frozen.</p>
<pre class=""lang-py prettyprint-override""><code>for name, param in model.named_parameters():                
    if name.startswith('bert'):
        param.requires_grad = False
</code></pre>
<p><strong>When would I need (1) and/or (2)?</strong></p>
<ul>
<li>If I want to train with a frozen BERT, would I need to enable both?</li>
<li>If I want to train to let BERT be updated, would I need to disable both?</li>
</ul>
<p>Additionaly, I ran all four combinations and found:</p>
<pre><code>   with torch.no_grad   requires_grad = False  Parameters  Ran
   ------------------   ---------------------  ----------  ---
a. Yes                  Yes                      3M        Successfully
b. Yes                  No                     112M        Successfully
c. No                   Yes                      3M        Successfully
d. No                   No                     112M        CUDA out of memory
</code></pre>
<p><strong>Can someone please explain what's going on?</strong> Why am I getting <code>CUDA out of memory</code> for (d) but not (b)? Both have 112M learnable parameters.</p>
"
63845748,Train Model with Token Features,"<p>I want to train a BERT like model for Hebrew, where fore very word I know:</p>
<ol>
<li>Lemma</li>
<li>Gender</li>
<li>Number</li>
<li>Voice</li>
</ol>
<p>And I would like to train a model where for each token these features are concatenated
Embedding(Token) = E1(Lemma):E2(Gender):E3(Number):E4(Voice)</p>
<p>Is there a way to do such a thing with the current huggingface transformers library?</p>
"
63876450,Reduce the output layer size from XLTransformers,"<p>I'm running the following using the huggingface implementation:</p>
<pre><code>t1 = &quot;My example sentence is really great.&quot;

tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')
model = TransfoXLLMHeadModel.from_pretrained(&quot;transfo-xl-wt103&quot;)

encoded_input = tokenizer(t1, return_tensors='pt', add_space_before_punct_symbol=True) 
output = model(**encoded_input)
tmp = output[0].detach().numpy()
print(tmp.shape)

&gt;&gt;&gt; (1, 7, 267735)
</code></pre>
<p>With the goal of getting output embeddings that I'll use downstream.</p>
<p>The last dimension is /substantially/ larger than I expected, and it looks like it is the size of the entire <code>vocab_size</code> rather than a reduction based on the <a href=""https://arxiv.org/pdf/1901.02860.pdf."" rel=""nofollow noreferrer"">ECL from the paper</a> (which potentially I am misinterpreting).</p>
<p>What argument would I provide the <code>model</code> to reduce this layer size to a smaller dimensional space, something more like the basic BERT at 400 or 768 and still obtain good performance based on the pretrained embeddings?</p>
"
63899303,Pytorch NLP model doesnâ€™t use GPU when making inference,"<p>I have a NLP model trained on Pytorch to be run in Jetson Xavier. I installed Jetson stats to monitor usage of CPU and GPU. When I run the Python script, only CPU cores work on-load, GPU bar does not increase. I have searched on Google about that with keywords of &quot; How to check if pytorch is using the GPU?&quot; and checked results on stackoverflow.com etc. According to their advices to someone else facing similar issue, cuda is available and there is cuda device in my Jetson Xavier. However, I donâ€™t understand why GPU bar does not change, CPU core bars go to the ends.</p>
<p>I donâ€™t want to use CPU, it takes so long to compute. In my opinion, it uses CPU, not GPU. How can I be sure and if it uses CPU, how can I change it to GPU?</p>
<p><strong>Note:</strong> Model is taken from huggingface transformers library. I have tried to use cuda() method on the model. (model.cuda()) In this scenario, GPU is used but I can not get an output from model and raises exception.</p>
<p>Here is the code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
import torch

BERT_DIR = &quot;savasy/bert-base-turkish-squad&quot;    

tokenizer = AutoTokenizer.from_pretrained(BERT_DIR)
model = AutoModelForQuestionAnswering.from_pretrained(BERT_DIR)
nlp=pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)


def infer(question,corpus):
    try:
        ans = nlp(question=question, context=corpus)
        return ans[&quot;answer&quot;], ans[&quot;score&quot;]
    except:
        ans = None
        pass

    return None, 0
</code></pre>
"
63899305,Docker error when containerizing app in Google Cloud Run,"<p>I am trying to run <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transformers</a> from <a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">huggingface</a> in Google Cloud Run.</p>
<p>My first idea was to run one of the dockerfiles provided by huggingface, but it seems that is not possible.</p>
<p>Any ideas on how to get around this error?</p>
<pre><code>Step 6/9 : WORKDIR /workspace
 ---&gt; Running in xxx
Removing intermediate container xxx
 ---&gt; xxx
Step 7/9 : COPY . transformers/
 ---&gt; xxx
Step 8/9 : RUN cd transformers/ &amp;&amp;     python3 -m pip install --no-cache-dir .
 ---&gt; Running in xxx
â†[91mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.
The command '/bin/sh -c cd transformers/ &amp;&amp;     python3 -m pip install --no-cache-dir .' returned a non-zero code: 1
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1
â†[0m
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

ERROR: (gcloud.builds.submit) build xxx completed with status &quot;FAILURE&quot;
</code></pre>
<p>Dockerfile from <a href=""https://github.com/huggingface/transformers/tree/master/docker/transformers-tensorflow-gpu"" rel=""nofollow noreferrer"">huggingface</a>:</p>
<pre><code>FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04
LABEL maintainer=&quot;Hugging Face&quot;
LABEL repository=&quot;transformers&quot;

RUN apt update &amp;&amp; \
    apt install -y bash \
                   build-essential \
                   git \
                   curl \
                   ca-certificates \
                   python3 \
                   python3-pip &amp;&amp; \
    rm -rf /var/lib/apt/lists

RUN python3 -m pip install --no-cache-dir --upgrade pip &amp;&amp; \
    python3 -m pip install --no-cache-dir \
    mkl \
    tensorflow

WORKDIR /workspace
COPY . transformers/
RUN cd transformers/ &amp;&amp; \
    python3 -m pip install --no-cache-dir .

CMD [&quot;/bin/bash&quot;]
</code></pre>
<p>.dockerignore file from Google Cloud Run <a href=""https://cloud.google.com/run/docs/quickstarts/build-and-deploy"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>Dockerfile
README.md
*.pyc
*.pyo
*.pyd
__pycache__
.pytest_cache
</code></pre>
<p>---- Edit:</p>
<p>Managed to get working based on the answer from Dustin. I basically:</p>
<ul>
<li>left the Dockerfile in the root folder, together with the transformers folder.</li>
<li>updated the COPY line from the dockerfile to:</li>
</ul>
<pre><code>COPY . ./
</code></pre>
"
63904821,Using Transformer for Text-Summarization,"<p>I am using huggingface transformer models for <strong>text-summarization</strong>.
Currently I am testing different models such as <strong>T5</strong> and <strong>Pegasus</strong>.
Now these models were trained for summarizing Big Texts into very short like a maximum of two sentences. Now I have the task, that I want summarizations, that are about half the size of the text, ergo the generated summaries are too small for my purpose.</p>
<p>My question now is, if there is a way to tell the model that another sentence came before?
Kind of similar to the logic inside stateful RNNs (although I know they work completly different).
If yes, I could summarize small windows over the sentences always with the information which content came before.</p>
<p>Is that just a thing of my mind? I cant believe that I am the only one, who wants to create shorter summaries, but not only 1 or two sentence long ones.</p>
<p>Thank you</p>
"
63907100,ValueError: Can't convert non-rectangular Python sequence to Tensor when using tf.data.Dataset.from_tensor_slices,"<p>This issue has been posted a handful of times in SO, but I still can't figure out what is the problem with my code, especially because it comes from a tutorial in <a href=""https://medium.com/atheros/text-classification-with-transformers-in-tensorflow-2-bert-2f4f16eff5ad"" rel=""nofollow noreferrer"">medium</a> and the author makes the code available on google <a href=""https://colab.research.google.com/drive/1934Mm2cwSSfT5bvi78-AExAl-hSfxCbq#scrollTo=kVlHGAbfjvjM"" rel=""nofollow noreferrer"">colab</a></p>
<p>I have seen other users having problem with wrong variable types <a href=""https://stackoverflow.com/questions/52582275/tf-data-with-multiple-inputs-outputs-in-keras"">#56304986</a> (which is not my case, as my model input is the output of <code>tokenizer</code>) and even seen the function I am trying to use (<code>tf.data.Dataset.from_tensor_slices</code>) being suggested as a solution <a href=""https://stackoverflow.com/questions/56304986/valueerror-cant-convert-non-rectangular-python-sequence-to-tensor"">#56304986</a>.</p>
<p>The line yielding error is:</p>
<pre><code># train dataset
ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)
</code></pre>
<p>where the method <code>encode_examples</code> is defined as (I have inserted an <code>assert</code> line into the <code>encode_examples</code> method to be sure my problem was not unmatching lenghts):</p>
<pre><code>def encode_examples(ds, limit=-1):
    # prepare list, so that we can build up final TensorFlow dataset from slices.
    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    if (limit &gt; 0):
        ds = ds.take(limit)

    for review, label in tfds.as_numpy(ds):

            bert_input = convert_example_to_feature(review.decode())

            ii = bert_input['input_ids']
            tti = bert_input['token_type_ids']
            am = bert_input['attention_mask']

            assert len(ii) == len(tti) == len(am), &quot;unmatching lengths!&quot;

            input_ids_list.append(ii)
            token_type_ids_list.append(tti)
            attention_mask_list.append(am)
            label_list.append([label])

    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)
</code></pre>
<p>The data is loaded like this (here i changed the dataset to get only 10% of the training data so I could speed up the debugging)</p>
<pre><code>(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split = ['train[:10%]','test[10%:15%]'], as_supervised=True, with_info=True)
</code></pre>
<p>And the other two calls(<code>convert_example_to_feature</code> and <code>map_example_to_dict</code>) and the tokenizer are as follow:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
def convert_example_to_feature(text):
    # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length
    return tokenizer.encode_plus(text,
                                 add_special_tokens = True, # add [CLS], [SEP]
                                 #max_length = max_length, # max length of the text that can go to BERT
                                 pad_to_max_length = True, # add [PAD] tokens
                                 return_attention_mask = True,)# add attention mask to not focus on pad tokens

def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):
    return ({&quot;input_ids&quot;: input_ids,
            &quot;token_type_ids&quot;: token_type_ids,
            &quot;attention_mask&quot;: attention_masks,
            }, label)
</code></pre>
<p>I suspect the error might have something to do with different versions of TensorFlow (I am using 2.3), but unfortunately I couldn't run the snippets in the google.colab notebook for memory reasons.</p>
<p>Does anyone know where what is the problem with my code? Thanks for your time and attention.</p>
"
63920887,"Whitelist tokens for text generation (XLNet, GPT-2) in huggingface-transformers","<p>In the documentation on text generation (<a href=""https://huggingface.co/transformers/main_classes/model.html#generative-models"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/model.html#generative-models</a>) there is the option to put</p>
<pre><code>bad_words_ids (List[int], optional) â€“ List of token ids that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use tokenizer.encode(bad_word, add_prefix_space=True).
</code></pre>
<p>Is there also the option to put something along the lines of &quot;allowed_words_ids&quot;? The idea would be to restrict the language of the generated texts.</p>
"
63924567,GPT2 on Hugging face(pytorch transformers) RuntimeError: grad can be implicitly created only for scalar outputs,"<p>I am trying to fine-tune gpt2 with a custom dataset of mine. I created a basic example with the documentation from hugging-face transformers. I receive the mentioned error. I know what it means: (basically it is calling backward on a non-scalar tensor) but since I almost use only API calls, I have no idea how to fix this issue. Any suggestions?</p>
<pre><code>from pathlib import Path
from absl import flags, app
import IPython
import torch
from transformers import GPT2LMHeadModel, Trainer,  TrainingArguments
from data_reader import GetDataAsPython

# this is my custom data, but i get the same error for the basic case below
# data = GetDataAsPython('data.json')
# data = [data_point.GetText2Text() for data_point in data]
# print(&quot;Number of data samples is&quot;, len(data))

data = [&quot;this is a trial text&quot;, &quot;this is another trial text&quot;]

train_texts = data

from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

special_tokens_dict = {'pad_token': '&lt;PAD&gt;'}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
train_encodigs = tokenizer(train_texts, truncation=True, padding=True)


class BugFixDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    
    def __getitem__(self, index):
        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = BugFixDataset(train_encodigs)

training_args = TrainingArguments(
    output_dir='./results',          
    num_train_epochs=3,              
    per_device_train_batch_size=1,  
    per_device_eval_batch_size=1,   
    warmup_steps=500,                
    weight_decay=0.01,               
    logging_dir='./logs',
    logging_steps=10,
)

model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)
model.resize_token_embeddings(len(tokenizer))

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()
</code></pre>
"
63939072,Loading saved NER transformers model causes AttributeError?,"<p>I have trained and saved some NER models using</p>
<pre><code>torch.save(model)
</code></pre>
<p>I need to load these model files (extension <code>.pt</code>) for evaluation using</p>
<pre><code>torch.load('PATH_TO_MODEL.pt')
</code></pre>
<p>And I get the following error: <code>'BertConfig' object has no attribute 'return_dict'</code></p>
<p>For the same, I updated my transformer package to the latest one, but the error persists.</p>
<p>This is the stack trace:</p>
<pre><code>Traceback (most recent call last):
File &quot;/home/systematicReviews/train_mtl_3.py&quot;, line 523, in &lt;module&gt;
test_loss, test_cr, test_cr_fine = evaluate_i(test_model, optimizer, scheduler, validation_dataloader, args, device)
File &quot;/home/systematicReviews/train_mtl_3.py&quot;, line 180, in evaluate_i
e_loss_coarse, e_output, e_labels, e_loss_fine, e_f_output, e_f_labels, mask, e_cumulative_loss  = defModel(args, e_input_ids, attention_mask=e_input_mask, P_labels=e_labels, P_f_labels=e_f_labels)
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__
result = self.forward(*input, **kwargs)
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 150, in forward
return self.module(*inputs[0], **kwargs[0])
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__
result = self.forward(*input, **kwargs)
File &quot;/home/systematicReviews/models/mtl/model.py&quot;, line 122, in forward
attention_mask = attention_mask
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__
result = self.forward(*input, **kwargs)
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/modeling_bert.py&quot;, line 784, in forward
return_dict = return_dict if return_dict is not None else self.config.use_return_dict
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/configuration_utils.py&quot;, line 219, in use_return_dict
return self.return_dict and not self.torchscript
AttributeError: 'BertConfig' object has no attribute 'return_dict'
</code></pre>
<p>Here is some more information about my system:</p>
<pre><code>- `transformers` version: 3.1.0
- Platform: Linux-4.4.0-186-generic-x86_64-with-debian-stretch-sid
- Python version: 3.6.9
- PyTorch version (GPU?): 1.3.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No
</code></pre>
<p>It worked pretty fine until now, but suddenly this bug appears. Any help or hint is appreciated.</p>
"
63948641,List Index Out of Range: Can I Pad My Text To Avoid?,"<p>I completely understand why I would be getting this error, but I am interested to know if there is a way to pad the text to fourteen words. For context purposes, this is a textual heatmap using GPT-2. If you have a simpler idea in mind, I would greatly appreciate that as well. To test the code out for yourself: <a href=""https://colab.research.google.com/drive/1NFwEdkQdQkDQAwAGwhP_fw4_zfyHWvMU#scrollTo=aNXp69VxE6lf"" rel=""nofollow noreferrer"">Google Colaboratory</a>. Thank you in advance for your assistance!</p>
<pre><code>def apply(f):
    text = f
    text = re.sub(r'\W+', ' ', text)
    res = LM().check_probabilities(text, topk=50)
    
    word_list = f.split()
    one = word_list[0]
    two = word_list[1]
    three = word_list[2]
    four = word_list[3]
    five = word_list[4]
    six = five = word_list[5]
    seven = word_list[6]
    eight = word_list[7]
    nine = word_list[8]
    ten = word_list[9]
    eleven = word_list[10]
    twelve = word_list[11]
    thirteen = word_list[12]
    fourteen = word_list[13]

    data = [[
{'token': '[CLR]',
 'meta': ['', '', ''],
 'heat': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},
{'token': ' ',
 'format': True},
{'token': one,
 'meta': res['pred_topk'][0],
 'heat': [0.13271349668502808, 0.4047139883041382, 0.23314827680587769, 1.0, 0.5698219537734985, 0.20001010596752167, 0.41732218861579895, 0.2375192940235138, 0.12837326526641846, 0.3011391758918762, 0.2920743227005005, 0.15121395885944366, 0.4707326292991638, 0.141720250248909, 0.1146061047911644, 0.3309290111064911, 0.2721664309501648, 0.38880598545074463, 0.28752031922340393, 0.30476102232933044, 0.40849509835243225, 0.12109626829624176, 0.236867755651474, 0.15692873299121857, 0.08568184077739716, 0.28222283720970154, 0.10787433385848999, 0.09868176281452179, 0.11645302921533585, 0.27660083770751953, 0.1150846853852272, 0.13137750327587128, 0.2834398150444031, 0.1425863653421402, 0.7729436159133911, 0.15550559759140015, 0.3342195451259613, 0.2743198275566101]},
{'token': ' ',
 'format': True},
{'token': two,
 'meta': res['pred_topk'][1],
 'heat': [0.11053311824798584, 1.0, 0.3417408764362335, 0.5805244445800781, 0.596860408782959, 0.18530210852622986, 0.2305091768503189, 0.19138814508914948, 0.08227257430553436, 0.19505015015602112, 0.10965480655431747, 0.07133453339338303, 0.21702361106872559, 0.07083487510681152, 0.05262206494808197, 0.09487571567296982, 0.07871642708778381, 0.09568451344966888, 0.10381820052862167, 0.11150145530700684, 0.08054117858409882, 0.06160977482795715, 0.13430000841617584, 0.07046942412853241, 0.04503295198082924, 0.10039176791906357, 0.07321848720312119, 0.04508531466126442, 0.04002087190747261, 0.1304282695055008, 0.05149686336517334, 0.05910608172416687, 0.1943625509738922, 0.05612911283969879, 0.2365487962961197, 0.0644913837313652, 0.08357883244752884, 0.10955799371004105]},
{'token': ' ',
 'format': True},
{'token': three,
 'meta': res['pred_topk'][2],
 'heat': [0.13794338703155518, 0.7412312626838684, 0.2688325345516205, 0.3519371747970581, 1.0, 0.3511815071105957, 0.6799001097679138, 0.23039610683918, 0.10480885207653046, 0.29196831583976746, 0.24283158779144287, 0.08086933195590973, 0.3110826909542084, 0.16006161272525787, 0.07783187925815582, 0.23599569499492645, 0.2036796659231186, 0.25475823879241943, 0.39147695899009705, 0.4029639661312103, 0.16113890707492828, 0.08008856326341629, 0.4354044497013092, 0.14515410363674164, 0.05876074731349945, 0.21267741918563843, 0.11644049733877182, 0.08587612956762314, 0.08814962208271027, 0.363741010427475, 0.07122389227151871, 0.07023804634809494, 0.1380654275417328, 0.1375676840543747, 0.7550925016403198, 0.10494624823331833, 0.23596565425395966, 0.12745369970798492]},
{'token': ' ',
 'format': True},
{'token': four,
 'meta': res['pred_topk'][3],
 'heat': [0.09374084323644638, 0.27613726258277893, 0.19584566354751587, 1.0, 0.2668629586696625, 0.12618684768676758, 0.5485848784446716, 0.10671643167734146, 0.05578231066465378, 0.16895149648189545, 0.14708179235458374, 0.08301705121994019, 0.2549331486225128, 0.05449998006224632, 0.0407552570104599, 0.09658133238554001, 0.08113130927085876, 0.10979730635881424, 0.09126582741737366, 0.16856855154037476, 0.10670913755893707, 0.049128126353025436, 0.12720689177513123, 0.10207141935825348, 0.040946654975414276, 0.14924436807632446, 0.07131370157003403, 0.05912680923938751, 0.057828083634376526, 0.2358609288930893, 0.05285044014453888, 0.03720799833536148, 0.08448022603988647, 0.05244402214884758, 0.2379569709300995, 0.07916100323200226, 0.06218649446964264, 0.10799198597669601]},
{'token': ' ',
 'format': True},
{'token': five,
 'meta': res['pred_topk'][4],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949, 0.08953408151865005, 0.13667654991149902, 0.1143374964594841, 0.11026952415704727, 0.05795498564839363, 0.12386422604322433, 0.08859734237194061, 0.042766354978084564, 0.3162827491760254, 0.07349050790071487, 0.09265555441379547, 0.08770584315061569, 0.2039150893688202, 0.05270526185631752, 0.06614900380373001, 0.16070793569087982, 0.05872023105621338, 0.3202408254146576, 0.062171820551157, 0.14679910242557526, 0.08074744045734406]},
{'token': ' ',
 'format': True},
 {'token': six,
 'meta': res['pred_topk'][5],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
 {'token': seven,
 'meta': res['pred_topk'][6],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
 {'token': eight,
 'meta': res['pred_topk'][7],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
 {'token': nine,
 'meta': res['pred_topk'][8],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': ten,
 'meta': res['pred_topk'][9],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': eleven,
 'meta': res['pred_topk'][10],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': twelve,
 'meta': res['pred_topk'][11],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': thirteen,
 'meta': res['pred_topk'][12],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': fourteen,
 'meta': res['pred_topk'][13],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
{'token': '[SEP]',
 'meta': ['', '', ''],
 'heat': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}
]]

    from textualheatmap import TextualHeatmap
    heatmap = TextualHeatmap(facet_titles = ['BERT'], show_meta=True, width=3800)
    heatmap.set_data(data)
    print(&quot;    &quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/3JT6t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3JT6t.png"" alt=""enter image description here"" /></a></p>
"
63953597,Using Huggingface zero-shot text classification with large data set,"<p>I'm trying to use Huggingface zero-shot text classification using 12 labels with large data set (57K sentences) read from a CSV file as follows:</p>
<pre><code>csv_file = tf.keras.utils.get_file('batch.csv', filename)
df = pd.read_csv(csv_file)
classifier = pipeline('zero-shot-classification')
results = classifier(df['description'].to_list(), labels, multi_class=True)
</code></pre>
<p>This keeps crashing as python runs out of memory.
I tried to create a dataset instead as follows:</p>
<pre><code>dataset = load_dataset('csv', data_files=filename)
</code></pre>
<p>But not sure how to use it with Huggingface's classifier. What is the best way to batch process classification?</p>
<p>I eventually would like to feed it over 1M sentences for classification.</p>
"
64023547,Inconsistent vector representation using transformers BertModel and BertTokenizer,"<p>I have a <code>BertTokenizer</code> (<code>tokenizer</code>) and a <code>BertModel</code> (<code>model</code>) from the <code>transformers</code> library.
<strong>I have pre-trained the model from scratch</strong> with a few wikipedia articles, just to test how it works.</p>
<p>Once the model is pre-trained, <strong>I want to extract a layer vector representation for a given sentence</strong>. For that, I calculate the average of the 11 hidden (768-sized) vectors. I do this as follows (<code>line</code> is a single <code>String</code>):</p>
<pre><code>padded_sequence = tokenizer(line, padding=True)
        
indexed_tokens = padded_sequence['input_ids']
attention_mask = padded_sequence[&quot;attention_mask&quot;]

tokens_tensor = torch.tensor([indexed_tokens])
attention_mask_tensor = torch.tensor([attention_mask])

outputs = model(tokens_tensor, attention_mask_tensor)
hidden_states = outputs[0]

line_vectorized = hidden_states[0].data.numpy().mean(axis=0)
</code></pre>
<p>So far so good. <strong>I can do this for every sentence individually. But now I want to do it in batch</strong>, ie. I have a bunch of sentences and instead of iterating each sentence I send the appropiate tensor representations to get all vectors at once. I do this as follows (<code>lines</code> is a <code>list of Strings</code>):</p>
<pre><code>padded_sequences = self.tokenizer_PYTORCH(lines, padding=True)
        
indexed_tokens_list = padded_sequences['input_ids']
attention_mask_list = padded_sequences[&quot;attention_mask&quot;]
        
tokens_tensors_list = [torch.tensor([indexed_tokens]) for indexed_tokens in indexed_tokens_list]
attention_mask_tensors_list = [torch.tensor([attention_mask ]) for attention_mask in attention_mask_list ]
        
tokens_tensors = torch.cat((tokens_tensors_list), 0)
attention_mask_tensors = torch.cat((attention_mask_tensors_list ), 0)

outputs = model(tokens_tensors, attention_mask_tensors)
hidden_states = outputs[0]

lines_vectorized = [hidden_states[i].data.numpy().mean(axis=0) for i in range(0, len(hidden_states))]
</code></pre>
<p>The problem is the following: <strong>I have to use padding so that I can appropiately concatenate the token tensors</strong>. That means that the indexed tokens and the attention masks can be larger than in the previous case where the sentences were evaluated individually. <strong>But when I use padding, I get different results for the sentences which have been padded</strong>.</p>
<p><em>EXAMPLE</em>:
I have two sentences (in French but it doesn't matter):</p>
<p><code>sentence_A</code> = &quot;appareil digestif un article de wikipedia l encyclopedie libre&quot;</p>
<p><code>sentence_B</code> = &quot;sauter a la navigation sauter a la recherche cet article est une ebauche concernant la biologie&quot;</p>
<p>When I evaluate the two sentences <strong>individually</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.9304411   0.53798294 -1.6231083 ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>But when I evaluate the two sentences <strong>in batch</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000, 10004, 10004, 10004, 10004, 10004, 10004, 10004]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
line_vectorized =  [-1.0473819   0.6090186  -1.727466  ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>That is, <strong>since <code>sentence_B</code> is larger than <code>sentence_A</code>, <code>sentence_A</code> has been padded and the attention mask has been padded with zeros as well</strong>. The indexed tokens contain now extra tokens (<code>10004</code> which I assume <code>empty</code>).
The vector representation of <code>sentence_B</code> has NOT changed. But <strong>the vector representation of <code>sentence_A</code> HAS CHANGED</strong>.</p>
<p>I would like to know if this is working as intended or not (I assume not).
And I guess I am doing something wrong but I can't figure out what.</p>
<p>Any ideas?</p>
"
64044200,Pytorch BERT: Misshaped inputs,"<p>I am running into issues of evaluating huggingface's BERT model ('bert-base-uncased') on large input sequences.</p>
<pre><code>model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
token_ids = [101, 1014, 1016, ...] # len(token_ids) == 33286
token_tensors = torch.tensor([token_ids]) # shape == [1, 33286]
segment_tensors = torch.tensor([[1] * len(token_ids)]) # shape == [1, 33286]
model(token_tensors, segment_tensors)

Traceback
self.model(token_tensors, segment_tensors)
  File &quot;/home/.../python3.8/site-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/.../python3.8/site-packages/transformers/modeling_bert.py&quot;, line 824, in forward
    embedding_output = self.embeddings(
  File &quot;/home/.../python3.8/site-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/.../python3.8/site-packages/transformers/modeling_bert.py&quot;, line 211, in forward
    embeddings = inputs_embeds + position_embeddings + token_type_embeddings
RuntimeError: The size of tensor a (33286) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>I noticed that <code>model.embeddings.positional_embeddings.weight.shape == (512, 768)</code>. I.e. when I restrict the input size to <code>model(token_tensors[:, :10], segment_tensors[:, :10])</code> it works. I am misunderstanding how the the <code>token_tensors</code> and <code>segment_tensors</code> should be shaped. I thought they should be sized <code>(batch_size, sequence_length)</code></p>
<p>Thanks for the help</p>
"
64106747,Loading saved NER back into HuggingFace pipeline?,"<p>I am doing some research into HuggingFace's functionalities for transfer learning (specifically, for named entity recognition). To preface, I am a bit new to transformer architectures. I briefly walked through their example off of their website:</p>
<pre><code>from transformers import pipeline

nlp = pipeline(&quot;ner&quot;)

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge which is visible from the window.&quot;

print(nlp(sequence))
</code></pre>
<p>What I would like to do is save and run this locally without having to download the &quot;ner&quot; model every time (which is over 1 GB in size). In their documentation, I see that you can save the pipeline using the &quot;pipeline.save_pretrained()&quot; function to a local folder. The results of this are various files which I am storing into a specific folder.</p>
<p>My question would be how can I load this model back up into a script to continue classifying as in the example above after saving? The output of &quot;pipeline.save_pretrained()&quot; is multiple files.</p>
<p>Here is what I have tried so far:</p>
<p>1: Following the documentation about pipeline</p>
<pre><code>pipe = transformers.TokenClassificationPipeline(model=&quot;pytorch_model.bin&quot;, tokenizer='tokenizer_config.json')
</code></pre>
<p>The error I got was: 'str' object has no attribute &quot;config&quot;</p>
<p>2: Following HuggingFace example on ner:</p>
<pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

model = AutoModelForTokenClassification.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)

label_list = [
&quot;O&quot;,       # Outside of a named entity
&quot;B-MISC&quot;,  # Beginning of a miscellaneous entity right after another miscellaneous entity
&quot;I-MISC&quot;,  # Miscellaneous entity
&quot;B-PER&quot;,   # Beginning of a person's name right after another person's name
&quot;I-PER&quot;,   # Person's name
&quot;B-ORG&quot;,   # Beginning of an organisation right after another organisation
&quot;I-ORG&quot;,   # Organisation
&quot;B-LOC&quot;,   # Beginning of a location right after another location
&quot;I-LOC&quot;    # Location
]

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge.&quot;

# Bit of a hack to get the tokens with the special tokens
tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))
inputs = tokenizer.encode(sequence, return_tensors=&quot;pt&quot;)

outputs = model(inputs)[0]
predictions = torch.argmax(outputs, dim=2)

print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())])
</code></pre>
<p>This yields an error: list index out of range</p>
<p>I also tried printing out just predictions which is not returning the text format of the tokens along with their entities.</p>
<p>Any help would be much appreciated!</p>
"
64119623,Flask app continuously restarting after downloading huggingface models,"<p>In my docker container I have a flask app (behind nginx and uwsgi) which instantiates a model from huggingface/transformers.
For some reason, the app continuously restarts when trying to after downloading the models</p>
<p><strong>App:</strong></p>
<pre><code>### app.py
server = Flask(__name__)
cors = CORS(server)
server.config[&quot;CORS_HEADERS&quot;] = &quot;Content-Type&quot;
log.info(&quot;Instantiating model&quot;)
model = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
pipe = pipeline('ner', model=model, tokenizer=tokenizer)

if __name__ == &quot;__main__&quot;:
    server.run(host=appconf.host, port=appconf.port, debug=appconf.isdev, use_reloader=False)
</code></pre>
<p><strong>Logs:</strong></p>
<pre><code>[2020-09-29 14:13:11,704] {./app.py:14} INFO - Instantiating model
[2020-09-29 14:13:11,708] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): s3.amazonaws.com:443
[2020-09-29 14:13:12,170] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://s3.amazonaws.com:443 &quot;HEAD /models.huggingface.co/bert/dbmdz/bert-large-cased-finetuned-conll03-english/config.json HTTP/1.1&quot; 200 0
[2020-09-29 14:13:12,176] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): cdn.huggingface.co:443
[2020-09-29 14:13:12,317] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://cdn.huggingface.co:443 &quot;HEAD /dbmdz/bert-large-cased-finetuned-conll03-english/pytorch_model.bin HTTP/1.1&quot; 200 0
[uWSGI] getting INI configuration from /etc/uwsgi/uwsgi.ini
*** Starting uWSGI 2.0.19.1 (64bit) on [Tue Sep 29 14:13:55 2020] ***
...
your server socket listen backlog is limited to 100 connections
your mercy for graceful operations on workers is 60 seconds
mapped 1727064 bytes (1686 KB) for 16 cores
*** Operational MODE: preforking ***
mounting app:server on /
Comment: FROM HERE IT REPEATS
[2020-09-29 14:13:57,254] {./app.py:14} INFO - Instantiating model &lt;-- AGAIN!
[2020-09-29 14:13:57,257] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): s3.amazonaws.com:443
[2020-09-29 14:13:57,686] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://s3.amazonaws.com:443 &quot;HEAD /models.huggingface.co/bert/dbmdz/bert-large-cased-finetuned-conll03-english/config.json HTTP/1.1&quot; 200 0
[2020-09-29 14:13:57,693] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): cdn.huggingface.co:443
[2020-09-29 14:13:57,790] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://cdn.huggingface.co:443 &quot;HEAD /dbmdz/bert-large-cased-finetuned-conll03-english/pytorch_model.bin HTTP/1.1&quot; 200 0
</code></pre>
<p>I am sure to have <code>http_proxy</code> and <code>https_proxy</code> set within the Docker container.</p>
<p>Thanks for the help</p>
"
64138426,Why can't I use Cross Entropy Loss for multilabel?,"<p>I'm in the process of finetuning a BERT model to the long answer task in the Natural Questions dataset. I'm training the model just like a SQuAD model (predicting start and end tokens).</p>
<p>I use Huggingface and PyTorch.</p>
<p>So the targets and labels have a shape/size of <em><strong>[batch, 2]</strong></em>. My problem is that I can't input &quot;multi-targets&quot; which I think is refering to the fact that the last shape is <em><strong>2</strong></em>.</p>
<blockquote>
<p>RuntimeError: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:18</p>
</blockquote>
<p>Should I choose another loss function or is there another way to bypass this problem?</p>
<p>This code I'm using:</p>
<pre><code>def loss_fn(preds, targets):
    return nn.CrossEntropyLoss()(preds,labels)
</code></pre>
<pre><code>class DecoderModel(nn.Module):

    def __init__(self, model_args, encoder_config, loss_fn):
        super(DecoderModel, self).__init__()
        # ...

    def forward(self, pooled_output, labels):   
        pooled_output = self.dropout(pooled_output)
        logits = self.linear(pooled_output)

        start_logits, end_logits = logits.split(1, dim = -1)
        start_logit = torch.squeeze(start_logits, axis=-1)
        end_logit = torch.squeeze(end_logits, axis=-1)

        # Concatenate into a &quot;label&quot;
        preds = torch.cat((start_logits, end_logits), -1)

        # Calculate loss
        loss = self.loss_fn(
            preds = preds, 
            labels = labels)

        return loss, preds
</code></pre>
<p>The targets properties are:
<em><strong>torch.int64</strong></em> &amp; <em><strong>[3,2]</strong></em></p>
<p>The predictions properties are:
<em><strong>torch.float32</strong></em> &amp; <em><strong>[3,2]</strong></em></p>
<h1>SOLVED - this is my solution</h1>
<pre><code>def loss_fn(preds:list, labels):
    start_token_labels, end_token_labels = labels.split(1, dim = -1)
    start_token_labels = start_token_labels.squeeze(-1)
    end_token_labels = end_token_labels.squeeze(-1)

    print('*'*50)
    print(preds[0].shape) # preds [0] and [1] has the same shape and dtype
    print(preds[0].dtype) # preds [0] and [1] has the same shape and dtype
    print(start_token_labels.shape) # labels [0] and [1] has the same shape and dtype
    print(start_token_labels.dtype) # labels [0] and [1] has the same shape and dtype

    start_loss = nn.CrossEntropyLoss()(preds[0], start_token_labels)
    end_loss = nn.CrossEntropyLoss()(preds[1], end_token_labels)

    avg_loss = (start_loss + end_loss) / 2
    return avg_loss
</code></pre>
<p>Basically I'm splitting the logits (just not concatinating them) and the labels. I then do Cross Entropy loss on both of them and at last taking the average loss between the two. Hope this gives you an idea to solve your own problem!</p>
"
64156202,Add dense layer on top of Huggingface BERT model,"<p>I want to add a dense layer on top of the bare BERT Model transformer outputting raw hidden-states, and then fine tune the resulting model. Specifically, I am using <a href=""https://huggingface.co/dbmdz/bert-base-italian-xxl-cased"" rel=""noreferrer"">this</a> base model. This is what the model should do:</p>
<ol>
<li>Encode the sentence (a vector with 768 elements for each token of the sentence)</li>
<li>Keep only the first vector (related to the first token)</li>
<li>Add a dense layer on top of this vector, to get the desired transformation</li>
</ol>
<p>So far, I have successfully encoded the sentences:</p>
<pre><code>from sklearn.neural_network import MLPRegressor

import torch

from transformers import AutoModel, AutoTokenizer

# List of strings
sentences = [...]
# List of numbers
labels = [...]

tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/bert-base-italian-xxl-cased&quot;)
model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-italian-xxl-cased&quot;)

# 2D array, one line per sentence containing the embedding of the first token
encoded_sentences = torch.stack([model(**tokenizer(s, return_tensors='pt'))[0][0][0]
                                 for s in sentences]).detach().numpy()

regr = MLPRegressor()
regr.fit(encoded_sentences, labels)
</code></pre>
<p>In this way I can train a neural network by feeding it with the encoded sentences. However, this approach clearly does not fine tune the base BERT model. Can anybody help me? How can I build a model (possibly in pytorch or using the Huggingface library) that can be entirely fine tuned?</p>
"
64383443,"What's difference RobertaModel, RobertaSequenceClassification (hugging face)","<p>I try to use hugging face transformers api.
As I import library , I have some questions. If anyone who know the answer, please tell me your knowledge.</p>
<p>transformers library have several models that are trained. transformers provide not only bare model like 'BertModel, RobertaModel, ... but also convenient heads like 'ModelForMultipleChoice' , 'ModelForSequenceClassification', 'ModelForTokenClassification' , ModelForQuestionAnswering.</p>
<p>I wonder what's difference between bare model adding new linear transformation myself and modelforsequenceclassification.
what's different custom model (pretrained model with random intialized linear) and transformers modelforsequenceclassification.</p>
<p>is ModelforSequenceClassification trained from glue data?
I look forward to someone's reply Thanks.</p>
"
63672169,HuggingFace Transformers model for German news classification,"<p>I've been trying to find a suitable model for my project (multiclass German text classification) but got a little confused with the models offered <a href=""https://huggingface.co/models?search=german"" rel=""nofollow noreferrer"">here</a>. There are models with <code>text-classification</code> tag, but they are for binary classification. Most of the other models are for <code>[MASK]</code> word predicting. I am not sure, which one to choose and if it will work with multiple classes at all</p>
<p>Would appreciate any advice!</p>
"
63358768,Why is there no pooler layer in huggingfaces' FlauBERT model?,"<p>BERT model for Language Model and Sequence classification includes an extra projection layer between the last transformer and the classification layer (it contains a linear layer of size <code>hidden_dim x hidden_dim</code>, a dropout layer and a <code>tanh</code> activation). This was not described in the paper originally but was clarified <a href=""https://github.com/google-research/bert/issues/43"" rel=""nofollow noreferrer"">here</a>. This intermediate layer is pre-trained together with the rest of the transformers.</p>
<p>In huggingface's <code>BertModel</code>, this layer is called <code>pooler</code>.</p>
<p>According to <a href=""https://arxiv.org/pdf/1912.05372.pdf#subsection.5.1"" rel=""nofollow noreferrer"">the paper</a>, FlauBERT model (XLMModel fine-tuned on French corpus) also includes this pooler layer: &quot;The classification head is composed of the following layers, in order: dropout, linear,tanhactivation, dropout, and linear.&quot;. However, when loading a FlauBERT model with huggingface (<em>e.g</em>, with <code>FlaubertModel.from_pretrained(...)</code>, or <code>FlaubertForSequenceClassification.from_pretrained(...)</code>), the model seem to include no such layer.</p>
<p>Hence the question: why is there no pooler layer in huggingfaces' FlauBERT model ?</p>
"
64446355,huggingface - save fine tuned model locally - and tokenizer too?,"<p>I just wonder if the tokenizer is somehow affected or changed if fine tune a BERT model and save it. Do I need to save the tokenizer locally too to reload it when using the saved BERT model later?</p>
<p>I just do:</p>
<pre><code>bert_model.save_pretrained('./Fine_tune_BERT/')
</code></pre>
<p>then later</p>
<pre><code>bert_model = TFBertModel.from_pretrained('./Fine_tune_BERT/')
</code></pre>
<p>But do i need to saver the tokenizer too? Or could I just use it in the normal way like:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
</code></pre>
"
64550503,Huggingface saving tokenizer,"<p>I am trying to save the tokenizer in huggingface so that I can load it later from a container where I don't need access to the internet.</p>
<pre class=""lang-py prettyprint-override""><code>BASE_MODEL = &quot;distilbert-base-multilingual-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.save_vocabulary(&quot;./models/tokenizer/&quot;)
tokenizer2 = AutoTokenizer.from_pretrained(&quot;./models/tokenizer/&quot;)
</code></pre>
<p>However, the last line is giving the error:</p>
<pre class=""lang-py prettyprint-override""><code>OSError: Can't load config for './models/tokenizer3/'. Make sure that:

- './models/tokenizer3/' is a correct model identifier listed on 'https://huggingface.co/models'

- or './models/tokenizer3/' is the correct path to a directory containing a config.json file
</code></pre>
<p><strong>transformers version: 3.1.0</strong></p>
<p><a href=""https://stackoverflow.com/questions/58417374/how-to-load-the-saved-tokenizer-from-pretrained-model-in-pytorch"">How to load the saved tokenizer from pretrained model in Pytorch</a> didn't help unfortunately.</p>
<h2>Edit 1</h2>
<p>Thanks to @ashwin's answer below I tried <code>save_pretrained</code> instead, and I get the following error:</p>
<pre class=""lang-py prettyprint-override""><code>OSError: Can't load config for './models/tokenizer/'. Make sure that:

- './models/tokenizer/' is a correct model identifier listed on 'https://huggingface.co/models'

- or './models/tokenizer/' is the correct path to a directory containing a config.json file
</code></pre>
<p>the contents of the tokenizer folder is below:
<a href=""https://i.stack.imgur.com/hNYVy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hNYVy.png"" alt=""enter image description here"" /></a></p>
<p>I tried renaming <code>tokenizer_config.json</code> to <code>config.json</code> and then I got the error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Unrecognized model in ./models/tokenizer/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: retribert, t5, mobilebert, distilbert, albert, camembert, xlm-roberta, pegasus, marian, mbart, bart, reformer, longformer, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl, electra, encoder-decoder
</code></pre>
"
64564545,BERT tokenize URLs,"<p>I want to classify a bunch of tweets and therefore I'm using the huggingface implementation of BERT. However I noticed that the deafult BertTokenizer does not use special tokens for urls.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; from transformers import BertTokenizer
&gt;&gt;&gt; tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
&gt;&gt;&gt; tokenizer.tokenize(&quot;https://stackoverflow.com/questions/ask&quot;)
['https', ':', '/', '/', 'stack', '##over', '##flow', '.', 'com', '/', 'questions', '/', 'ask']
</code></pre>
<p>This seems quite inefficent to me. What would be the best way, to encode URLs?</p>
"
64606333,BERT embeddings in SPARKNLP or BERT for token classification in huggingface,"<p>Currently I am working on productionize a NER model on Spark. I have a current implementation that is using Huggingface DISTILBERT with the TokenClassification head, but as the performance is a bit slow and costly, I am trying to find ways to optimize.</p>
<p>I have checked SPARKNLP implementation, which lacks a pretrained DISTILBERT and has I think a different approach, so some questions regarding this arose:</p>
<ol>
<li>Huggingface uses the entire BERT model and adds a head for token classification. Is this the same as obtaining the BERT embeddings and just feeding them to another NN?</li>
<li>I ask this because this is the SPARKNLP approach, a class that helps obtaim those embeddings and use it as a feature for another complex NN. Doesnt this lose some of the knowledge inside BERT?</li>
<li>Does SPARKNLP have any optimization regarding SPARK that helps in inference time or is it just another BERT implementation.</li>
</ol>
"
64610841,BERT-based NER model giving inconsistent prediction when deserialized,"<p>I am trying to train an NER model using the HuggingFace transformers library on Colab cloud GPUs, pickle it and load the model on my own CPU to make predictions.</p>
<p><strong>Code</strong></p>
<p>The model is the following:</p>
<pre><code>from transformers import BertForTokenClassification

model = BertForTokenClassification.from_pretrained(
    &quot;bert-base-cased&quot;,
    num_labels=NUM_LABELS,
    output_attentions = False,
    output_hidden_states = False
)
</code></pre>
<p>I am using this snippet to save the model on Colab</p>
<pre><code>import torch

torch.save(model.state_dict(), FILENAME)
</code></pre>
<p>Then load it on my local CPU using</p>
<pre><code># Initiating an instance of the model type

model_reload = BertForTokenClassification.from_pretrained(
    &quot;bert-base-cased&quot;,
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)

# Loading the model
model_reload.load_state_dict(torch.load(FILENAME, map_location='cpu'))
model_reload.eval()

</code></pre>
<p>The code snippet used to tokenize the text and make actual predictions is the same both on the Colab GPU notebook instance and my CPU notebook instance.</p>
<p><strong>Expected Behavior</strong></p>
<p>The GPU-trained model behaves correctly and classifies the following tokens perfectly:</p>
<pre><code>O       [CLS]
O       Good
O       morning
O       ,
O       my
O       name
O       is
B-per   John
I-per   Kennedy
O       and
O       I
O       am
O       working
O       at
B-org   Apple
O       in
O       the
O       headquarters
O       of
B-geo   Cupertino
O       [SEP]
</code></pre>
<p><strong>Actual Behavior</strong></p>
<p>When loading the model and use it to make predictions on my CPU, the predictions are totally wrong:</p>
<pre><code>I-eve   [CLS]
I-eve   Good
I-eve   morning
I-eve   ,
I-eve   my
I-eve   name
I-eve   is
I-geo   John
B-eve   Kennedy
I-eve   and
I-eve   I
I-eve   am
I-eve   working
I-eve   at
I-gpe   Apple
I-eve   in
I-eve   the
I-eve   headquarters
I-eve   of
B-org   Cupertino
I-eve   [SEP]
</code></pre>
<p>Does anyone have ideas why it doesn't work? Did I miss something?</p>
"
64631665,What is the difference in RobertaTokenizer() and from_pretrained() way of initialising RobertaTokenizer?,"<p>I am a newbie to <strong>huggingface transformers</strong> and facing the below issue in training a <code>RobertaForMaskedLM</code> LM from scratch:</p>
<p>First, I have trained and saved a <code>ByteLevelBPETokenizer</code> as follows:</p>
<pre><code>tokenizer = ByteLevelBPETokenizer()
print('Saving tokenizer at:', training_file)
tokenizer.train(files=training_file, vocab_size=VOCAB_SIZE, min_frequency=2, 
special_tokens=[&quot;&lt;s&gt;&quot;,&quot;&lt;pad&gt;&quot;,&quot;&lt;/s&gt;&quot;,&quot;&lt;unk&gt;&quot;,&quot;&lt;mask&gt;&quot;])
tokenizer.save_model(tokenizer_mdl_dir)
</code></pre>
<p>Then, trained <code>RobertaForMaskedLM</code> using this tokenizer by creating a <code>RobertaTokenizer</code> as follows:</p>
<pre><code>roberta_tokenizer = RobertaTokenizer(tokenizer_mdl + &quot;/vocab.json&quot;, tokenizer_mdl + &quot;/merges.txt&quot;)
</code></pre>
<p>But now, when I try to test the trained LM using a fill-mask pipeline,</p>
<pre><code>fill_mask_pipeline = pipeline(&quot;fill-mask&quot;, model=roberta_model, tokenizer=roberta_tokenizer)
</code></pre>
<p>I got the below error:</p>
<blockquote>
<p>PipelineException: No mask_token () found on the input</p>
</blockquote>
<p>So, I realized, the tokenizer that I have loaded, is tokenizing the <code>&lt;mask&gt;</code> token as well. But I couldn't understand why it is doing so. Please help me understand this.</p>
<p>After trying several things, I loaded the tokenizer differently,</p>
<pre><code>roberta_tokenizer = RobertaTokenizer.from_pretrained(tokenizer_mdl)
</code></pre>
<p>And, now the <code>fill_mask_pipeline</code> runs without errors. So, what is the difference between loading a tokenizer using <code>RobertaTokenizer()</code> and using the <code>.from_pretrained()</code> method?</p>
"
64646867,Downloading huggingface pre-trained models,"<p>Once I have downloaded a pre-trained model on a Colab Notebook, it disappears after I reset the notebook variables.
Is there a way I can download the model to use it for a second occasion?</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
</code></pre>
"
64646890,Train BERT with CLI commands,"<p>I have downloaded the HuggingFace BERT model from the transformer repository found <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">here</a> and would like to train the model on custom NER labels by using the run_ner.py script as it is referenced <a href=""https://huggingface.co/transformers/task_summary.html"" rel=""nofollow noreferrer"">here</a> in the section &quot;Named Entity Recognition&quot;.</p>
<p>I define model (&quot;bert-base-german-cased&quot;), data_dir (&quot;Data/sentence_data.txt&quot;) and labels (&quot;Data/labels.txt)&quot; as defaults in the code.</p>
<p>Now I'm using this input for the command line:</p>
<pre><code>python run_ner.py --output_dir=&quot;Models&quot; --num_train_epochs=3 --logging_steps=100 --do_train --do_eval --do_predict
</code></pre>
<p>But all it does is telling me:</p>
<pre><code>Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.w
eight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>After that it just stops, not ending the script, but simply waiting.</p>
<p>Does anyone know what could be the problem here? Am I missing a parameter?</p>
<p>My sentence_data.txt in CoNLL format looks like this (small snippet):</p>
<pre><code>Strafverfahren O
gegen O
; O
wegen O
Diebstahls O
hat O
das O
Amtsgericht Ort
Leipzig Ort
- O
Strafrichter O
</code></pre>
<p>And that's how I defined my labels in labels.txt:</p>
<pre><code>&quot;Date&quot;, &quot;Delikt&quot;, &quot;Strafe_Tatbestand&quot;, &quot;Schadensbetrag&quot;, &quot;GestÃ¤ndnis_ja&quot;, &quot;Vorstrafe_ja&quot;, &quot;Vorstrafe_nein&quot;, &quot;Ort&quot;,
&quot;Strafe_Gesamtfreiheitsstrafe_Dauer&quot;, &quot;Strafe_Gesamtsatz_Dauer&quot;, &quot;Strafe_Gesamtsatz_Betrag&quot;
</code></pre>
"
64675655,BERT always predicts same class (Fine-Tuning),"<p>I am fine-tuning BERT on a financial news dataset.
Unfortunately BERT seems to be trapped in a local minimum. It is content with learning to always predict the same class.</p>
<ul>
<li>balancing the dataset didnt work</li>
<li>tuning parameters didnt work as well</li>
</ul>
<p>I am honestly not sure what is causing this problem. With the simpletransformers library I am getting very good results. I would really appreciate if somebody could help me. thanks a lot!</p>
<p>Full code on github:
<a href=""https://github.com/Bene939/BERT_News_Sentiment_Classifier"" rel=""nofollow noreferrer"">https://github.com/Bene939/BERT_News_Sentiment_Classifier</a></p>
<p>Code:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
import pandas as pd
from pathlib import Path
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
from torch.nn import functional as F
from collections import defaultdict
import random


#defining tokenizer, model and optimizer
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)


if torch.cuda.is_available():
  print(&quot;\nUsing: &quot;, torch.cuda.get_device_name(0))
  device = torch.device('cuda')
else:
  print(&quot;\nUsing: CPU&quot;)
  device = torch.device('cpu')
model = model.to(device)


#loading dataset
labeled_dataset = &quot;news_headlines_sentiment.csv&quot;
labeled_dataset_file = Path(labeled_dataset)
file_loaded = False
while not file_loaded:
  if labeled_dataset_file.exists():
    labeled_dataset = pd.read_csv(labeled_dataset_file)
    file_loaded = True
    print(&quot;Dataset Loaded&quot;)
  else:
    print(&quot;File not Found&quot;)
print(labeled_dataset)

#counting sentiments
negative = 0
neutral = 0
positive = 0
for idx, row in labeled_dataset.iterrows():
  if row[&quot;sentiment&quot;] == 0:
    negative += 1
  elif row[&quot;sentiment&quot;] == 1:
    neutral += 1
  else:
    positive += 1
print(&quot;Unbalanced Dataset&quot;)
print(&quot;negative: &quot;, negative)
print(&quot;neutral: &quot;, neutral)
print(&quot;positive: &quot;, positive)

#balancing dataset to 1/3 per sentiment
for idx, row in labeled_dataset.iterrows():
  if row[&quot;sentiment&quot;] == 0:
    if negative - neutral != 0:
      index_name = labeled_dataset[labeled_dataset[&quot;news&quot;] == row[&quot;news&quot;]].index
      labeled_dataset.drop(index_name, inplace=True)
      negative -= 1
  elif row[&quot;sentiment&quot;] == 2:
    if positive - neutral != 0:
      index_name = labeled_dataset[labeled_dataset[&quot;news&quot;] == row[&quot;news&quot;]].index
      labeled_dataset.drop(index_name, inplace=True)
      positive -= 1

#custom dataset class
class NewsSentimentDataset(torch.utils.data.Dataset):
  def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

  def __getitem__(self, idx):
      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
      item['labels'] = torch.tensor(self.labels[idx])
      return item

  def __len__(self):
      return len(self.labels)

#method for tokenizing dataset list
def tokenize_headlines(headlines, labels, tokenizer):

  encodings = tokenizer.batch_encode_plus(
      headlines,
      add_special_tokens = True,
      truncation = True,
      padding = 'max_length',
      return_attention_mask = True,
      return_token_type_ids = True
  )

  dataset = NewsSentimentDataset(encodings, labels)
  return dataset

#splitting dataset into training and validation set
#load news sentiment dataset
all_headlines = labeled_dataset['news'].tolist()
all_labels = labeled_dataset['sentiment'].tolist()

train_headlines, val_headlines, train_labels, val_labels = train_test_split(all_headlines, all_labels, test_size=.2)

val_dataset = tokenize_headlines(val_headlines, val_labels, tokenizer)
train_dataset = tokenize_headlines(train_headlines, val_labels, tokenizer)

#data loader
train_batch_size = 8
val_batch_size = 8

train_data_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle=True)
val_data_loader = DataLoader(val_dataset, batch_size = val_batch_size, sampler=SequentialSampler(val_dataset))

#optimizer and scheduler
num_epochs = 1
num_steps = len(train_data_loader) * num_epochs
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_steps*0.06, num_training_steps=num_steps)

#training and evaluation
seed_val = 64

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

for epoch in range(num_epochs):

  print(&quot;\n###################################################&quot;)
  print(&quot;Epoch: {}/{}&quot;.format(epoch+1, num_epochs))
  print(&quot;###################################################\n&quot;)

  #training phase
 
  average_train_loss = 0
  average_train_acc = 0
  model.train() 
  for step, batch in enumerate(train_data_loader):
      
      
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['labels'].to(device)
      token_type_ids = batch['token_type_ids'].to(device)


      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids = token_type_ids)

      loss = F.cross_entropy(outputs[0], labels)
      average_train_loss += loss

      if step % 40 == 0:
        print(&quot;Training Loss: &quot;, loss)

      logits = outputs[0].detach().cpu().numpy()
      label_ids = labels.to('cpu').numpy()

      average_train_acc += sklearn.metrics.accuracy_score(label_ids, np.argmax(logits, axis=1))
      print(&quot;predictions: &quot;,np.argmax(logits, axis=1))
      print(&quot;labels:      &quot;,label_ids)
      print(&quot;#############&quot;)
      optimizer.zero_grad()
      loss.backward()
      #maximum gradient clipping
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
      
      optimizer.step()
      scheduler.step()
      model.zero_grad()

  average_train_loss = average_train_loss / len(train_data_loader)
  average_train_acc = average_train_acc / len(train_data_loader)
  print(&quot;======Average Training Loss: {:.5f}======&quot;.format(average_train_loss))
  print(&quot;======Average Training Accuracy: {:.2f}%======&quot;.format(average_train_acc*100))

  #validation phase
  average_val_loss = 0
  average_val_acc = 0
  model.eval()
  for step,batch in enumerate(val_data_loader):
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)
    token_type_ids = batch['token_type_ids'].to(device)

    pred = []
    with torch.no_grad():
      

      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

      loss = F.cross_entropy(outputs[0], labels)
      average_val_loss += loss

      logits = outputs[0].detach().cpu().numpy()
      label_ids = labels.to('cpu').numpy()
      print(&quot;predictions: &quot;,np.argmax(logits, axis=1))
      print(&quot;labels:      &quot;,label_ids)
      print(&quot;#############&quot;)

      average_val_acc += sklearn.metrics.accuracy_score(label_ids, np.argmax(logits, axis=1))

  average_val_loss = average_val_loss / len(val_data_loader)
  average_val_acc = average_val_acc / len(val_data_loader)

  print(&quot;======Average Validation Loss: {:.5f}======&quot;.format(average_val_loss))
  print(&quot;======Average Validation Accuracy: {:.2f}%======&quot;.format(average_val_acc*100))
###################################################
Epoch: 1/1
###################################################

Training Loss:  tensor(1.1006, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [1 0 2 0 0 0 2 0]
labels:       [2 0 1 1 0 1 0 1]
#############
predictions:  [2 2 0 0 0 2 0 0]
labels:       [1 2 1 0 2 0 1 2]
#############
predictions:  [0 0 0 0 1 0 0 1]
labels:       [0 1 1 0 1 1 2 0]
#############
predictions:  [0 0 0 2 0 1 0 0]
labels:       [0 0 0 2 0 0 2 1]
#############
predictions:  [1 0 0 0 0 0 2 0]
labels:       [0 2 2 1 0 0 0 0]
#############
predictions:  [0 0 0 0 0 1 0 0]
labels:       [1 0 2 2 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 2 2 0 2 0]
#############
predictions:  [0 1 0 0 0 0 0 0]
labels:       [2 2 0 2 0 0 0 1]
#############
predictions:  [0 0 0 0 0 2 0 1]
labels:       [0 1 0 2 2 0 1 2]
#############
predictions:  [0 0 2 0 0 0 1 0]
labels:       [0 0 0 1 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 1 0 1 0 1 1]
#############
predictions:  [0 2 0 0 0 0 0 0]
labels:       [2 2 0 1 0 1 2 1]
#############
predictions:  [0 1 0 0 0 0 1 2]
labels:       [2 2 1 0 2 0 0 2]
#############
predictions:  [0 0 1 1 1 1 0 1]
labels:       [1 2 1 1 1 1 2 2]
#############
predictions:  [1 0 0 0 0 1 2 1]
labels:       [1 0 1 1 0 0 0 2]
#############
predictions:  [0 1 1 1 1 0 2 1]
labels:       [2 2 1 2 2 1 1 2]
#############
predictions:  [0 0 1 0 1 1 0 0]
labels:       [1 0 0 1 0 1 0 2]
#############
predictions:  [1 2 0 0 1 2 0 0]
labels:       [0 2 2 1 2 0 1 0]
#############
predictions:  [0 2 1 1 0 1 1 0]
labels:       [2 2 0 1 1 0 1 2]
#############
predictions:  [1 0 1 1 1 1 1 0]
labels:       [0 2 0 1 0 1 2 2]
#############
predictions:  [0 2 1 2 0 0 1 1]
labels:       [2 1 1 1 1 2 2 0]
#############
predictions:  [0 1 2 2 2 1 1 2]
labels:       [2 2 1 1 2 1 0 1]
#############
predictions:  [2 2 2 1 2 1 1 1]
labels:       [0 1 1 0 0 2 2 1]
#############
predictions:  [1 2 2 2 1 2 1 2]
labels:       [0 0 0 0 2 0 1 2]
#############
predictions:  [2 1 1 1 2 2 2 2]
labels:       [1 0 2 2 1 0 0 0]
#############
predictions:  [2 1 2 2 2 1 2 2]
labels:       [2 1 1 1 1 1 2 2]
#############
predictions:  [1 1 0 2 1 2 1 2]
labels:       [2 2 0 2 0 1 2 0]
#############
predictions:  [0 1 1 2 0 1 2 1]
labels:       [2 2 2 1 2 2 0 1]
#############
predictions:  [2 1 1 1 1 2 1 1]
labels:       [0 1 1 2 1 0 0 2]
#############
predictions:  [1 2 2 0 1 1 1 2]
labels:       [0 1 2 1 2 1 0 1]
#############
predictions:  [0 1 1 1 1 1 1 0]
labels:       [0 2 0 1 1 2 2 2]
#############
predictions:  [1 2 1 1 2 1 1 0]
labels:       [0 2 2 2 0 0 1 0]
#############
predictions:  [2 2 2 1 2 1 1 2]
labels:       [2 2 1 2 1 0 0 0]
#############
predictions:  [2 2 1 2 2 2 1 2]
labels:       [1 1 2 2 2 0 2 1]
#############
predictions:  [2 2 2 2 2 0 2 2]
labels:       [2 2 1 2 0 1 1 2]
#############
predictions:  [1 1 2 1 2 2 0 1]
labels:       [2 1 1 1 0 0 2 2]
#############
predictions:  [2 1 2 2 2 2 1 0]
labels:       [0 2 0 2 0 0 0 0]
#############
predictions:  [2 2 2 2 2 2 2 2]
labels:       [1 1 0 2 0 1 2 1]
#############
predictions:  [2 2 2 2 1 2 2 2]
labels:       [1 0 0 1 1 0 0 0]
#############
predictions:  [2 2 2 1 2 2 2 2]
labels:       [1 0 1 1 0 2 2 0]
#############
Training Loss:  tensor(1.1104, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [2 0 1 2 1 2 2 0]
labels:       [2 2 0 0 1 0 0 2]
#############
predictions:  [0 2 2 0 2 1 1 1]
labels:       [0 0 0 1 0 0 1 0]
#############
predictions:  [0 2 2 0 1 1 1 2]
labels:       [2 1 1 1 2 2 1 0]
#############
predictions:  [2 1 1 2 2 0 2 0]
labels:       [1 2 1 2 1 0 2 1]
#############
predictions:  [0 2 2 0 0 2 1 2]
labels:       [0 0 2 2 0 0 2 0]
#############
predictions:  [0 0 1 2 2 0 2 2]
labels:       [0 0 0 0 0 0 0 0]
#############
predictions:  [1 1 2 1 2 0 1 2]
labels:       [0 0 2 0 0 0 1 1]
#############
predictions:  [0 0 2 1 0 2 0 1]
labels:       [1 1 2 1 1 0 2 0]
#############
predictions:  [0 0 0 0 1 0 0 0]
labels:       [2 2 1 1 2 1 1 1]
#############
predictions:  [0 0 0 0 1 0 0 0]
labels:       [1 1 2 2 1 1 2 0]
#############
predictions:  [0 0 0 0 0 1 1 1]
labels:       [2 0 1 1 0 1 2 2]
#############
predictions:  [0 0 1 0 0 1 2 1]
labels:       [1 2 0 2 2 0 2 1]
#############
predictions:  [1 1 1 1 0 1 0 1]
labels:       [2 0 1 0 1 0 1 2]
#############
predictions:  [1 2 2 0 0 0 1 1]
labels:       [2 0 0 2 1 2 2 2]
#############
predictions:  [1 0 2 1 0 2 2 0]
labels:       [0 0 2 1 2 1 1 1]
#############
predictions:  [0 0 0 1 1 1 1 1]
labels:       [1 2 1 0 0 0 1 0]
#############
predictions:  [1 1 1 0 1 1 0 1]
labels:       [0 2 1 2 1 2 2 0]
#############
predictions:  [2 1 0 1 1 2 0 0]
labels:       [0 1 0 0 1 2 0 2]
#############
predictions:  [0 1 1 0 0 1 0 1]
labels:       [1 0 0 2 2 1 1 2]
#############
predictions:  [1 1 1 1 1 1 1 1]
labels:       [2 0 1 0 2 0 0 2]
#############
predictions:  [1 0 0 1 0 1 0 2]
labels:       [1 0 0 1 1 2 2 1]
#############
predictions:  [1 1 1 1 1 1 0 0]
labels:       [1 1 0 2 1 0 2 0]
#############
predictions:  [1 1 2 1 0 1 0 0]
labels:       [0 2 1 2 1 1 0 2]
#############
predictions:  [1 1 0 0 1 2 1 1]
labels:       [0 2 1 0 2 2 0 1]
#############
predictions:  [0 1 1 0 0 1 0 1]
labels:       [0 0 1 2 2 0 1 2]
#############
predictions:  [1 0 2 2 2 1 1 0]
labels:       [2 2 1 0 0 1 1 2]
#############
predictions:  [1 2 2 1 1 2 1 1]
labels:       [1 0 0 1 0 0 0 0]
#############
predictions:  [0 2 0 2 2 0 2 2]
labels:       [2 0 0 0 2 1 1 2]
#############
predictions:  [0 0 1 0 1 0 2 2]
labels:       [0 0 1 0 1 0 2 0]
#############
predictions:  [0 2 0 1 1 2 2 0]
labels:       [0 2 0 2 0 2 0 0]
#############
predictions:  [2 2 2 2 2 2 2 1]
labels:       [2 2 1 1 0 0 2 2]
#############
predictions:  [2 0 0 2 2 1 1 0]
labels:       [1 0 0 1 0 2 1 2]
#############
predictions:  [2 0 0 2 0 2 2 0]
labels:       [2 2 2 2 0 1 1 1]
#############
predictions:  [0 2 2 0 2 2 0 0]
labels:       [1 0 1 2 0 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 2]
labels:       [2 1 1 0 0 0 1 2]
#############
predictions:  [2 0 2 0 2 1 0 2]
labels:       [2 1 1 2 1 1 0 0]
#############
predictions:  [1 1 2 0 2 0 2 2]
labels:       [0 2 1 2 1 2 1 0]
#############
predictions:  [2 0 1 1 0 2 0 0]
labels:       [2 1 0 1 1 0 2 0]
#############
predictions:  [2 0 0 2 0 2 1 0]
labels:       [0 0 0 0 2 1 0 1]
#############
predictions:  [1 2 1 0 0 2 0 2]
labels:       [2 0 2 1 0 0 1 1]
#############
Training Loss:  tensor(1.1162, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [2 0 0 1 1 1 0 1]
labels:       [0 1 1 1 1 2 2 1]
#############
predictions:  [0 2 0 1 2 0 0 1]
labels:       [2 2 1 0 1 0 0 0]
#############
predictions:  [0 0 1 0 0 0 0 1]
labels:       [1 0 2 0 0 2 2 0]
#############
predictions:  [2 1 2 2 0 1 2 0]
labels:       [2 0 1 0 2 1 0 1]
#############
predictions:  [1 0 0 2 0 0 1 1]
labels:       [2 2 0 2 0 2 0 0]
#############
predictions:  [0 0 1 0 0 0 0 0]
labels:       [2 2 2 1 2 2 2 2]
#############
predictions:  [0 0 1 1 0 1 1 0]
labels:       [2 1 1 1 0 2 1 0]
#############
predictions:  [0 0 0 1 0 0 1 0]
labels:       [2 0 2 2 0 0 1 2]
#############
predictions:  [1 0 1 0 0 2 0 0]
labels:       [1 1 2 0 0 1 0 0]
#############
predictions:  [2 1 0 0 0 1 0 0]
labels:       [1 2 0 0 0 0 0 0]
#############
predictions:  [0 2 0 0 0 0 0 0]
labels:       [2 0 1 1 2 2 1 1]
#############
predictions:  [0 1 0 0 0 1 0 2]
labels:       [0 2 1 1 0 0 1 2]
#############
predictions:  [0 2 1 0 0 1 1 1]
labels:       [1 1 0 2 0 1 1 0]
#############
predictions:  [0 1 1 0 0 0 1 0]
labels:       [0 0 1 0 1 2 1 1]
#############
predictions:  [0 1 1 0 1 0 0 0]
labels:       [0 1 1 1 2 2 2 0]
#############
predictions:  [0 0 0 0 1 1 0 0]
labels:       [2 0 2 2 1 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 0 2 2 0 1 1]
#############
predictions:  [0 1 0 0 0 0 0 0]
labels:       [0 2 0 1 1 2 0 2]
#############
predictions:  [1 1 0 1 0 1 0 2]
labels:       [1 2 0 0 2 2 2 1]
#############
predictions:  [1 1 0 0 0 1 2 1]
labels:       [0 0 1 2 2 1 2 2]
#############
predictions:  [1 1 1 0 1 1 2 0]
labels:       [0 0 0 2 0 1 0 2]
#############
predictions:  [0 1 0 0 1 1 2 1]
labels:       [2 0 0 1 2 2 1 2]
#############
predictions:  [1 0 0 0 1 0 0 1]
labels:       [1 2 2 2 2 1 0 1]
#############
predictions:  [2 0 0 0 0 0 0 0]
labels:       [1 2 0 2 2 1 1 1]
#############
predictions:  [2 0 1 1 0 0 1 0]
labels:       [0 0 0 0 2 2 1 1]
#############
predictions:  [2 0 0 1 0 0 1 1]
labels:       [2 2 1 1 0 0 1 0]
#############
predictions:  [1 1 1 1 1 2 0 0]
labels:       [0 0 2 1 0 0 0 0]
#############
predictions:  [1 1 2 0 1 2 0 1]
labels:       [0 2 1 0 2 0 0 1]
#############
predictions:  [0 0 2 1 0 2 0 1]
labels:       [1 2 0 2 2 1 0 0]
#############
predictions:  [0 0 2 0 2 1 1 2]
labels:       [2 2 1 2 2 2 0 0]
#############
predictions:  [0 1 0 0 0 0 2 1]
labels:       [1 1 0 1 1 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 0 2 0 0 2]
#############
predictions:  [2 2 2 0 1 1 1 0]
labels:       [1 0 2 1 1 2 0 0]
#############
predictions:  [0 0 1 0 0 0 2 0]
labels:       [0 1 2 1 1 0 0 0]
#############
predictions:  [0 2 0 1 0 2 0 0]
labels:       [0 0 2 1 1 0 2 2]
#############
predictions:  [0 0 1 2 0 2 0 1]
labels:       [2 2 0 0 0 2 2 2]
#############
predictions:  [1 0 0 0 2 0 0 1]
labels:       [2 0 1 1 1 0 0 1]
#############
predictions:  [0 1 0 0 0 0 0 2]
labels:       [1 1 1 0 0 0 2 2]
#############
predictions:  [0 2 0 1 0 2 0 0]
labels:       [1 1 1 1 2 2 1 0]
#############
predictions:  [1 2 0 0 0 0 0 0]
labels:       [2 0 2 1 0 1 1 1]
#############
Training Loss:  tensor(1.2082, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [0 2 0 0 0 0 2 0]
labels:       [1 0 2 1 2 2 1 1]
#############
predictions:  [2 0 0 0 0 0 1 0]
labels:       [1 0 0 0 0 2 1 0]
#############
predictions:  [0 0 0 0 2 1 1 1]
labels:       [0 2 2 0 1 2 1 1]
#############
predictions:  [2 1 0 1 0 0 2 0]
labels:       [1 0 2 1 0 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 0 0 0 1 0]
#############
predictions:  [0 2 1 0 0 0 1 1]
labels:       [0 2 2 2 2 1 1 0]
#############
predictions:  [0 0 0 1 1 0 0 1]
labels:       [0 1 0 1 2 2 2 2]
#############
predictions:  [0 0 0 1 1 1 1 2]
labels:       [2 2 1 2 0 1 1 1]
#############
predictions:  [0 1 2 0 0 1 0 0]
labels:       [0 2 1 0 0 1 0 0]
#############
predictions:  [1 1 1 1 0 0 0 0]
labels:       [2 1 2 1 0 2 2 1]
#############
predictions:  [0 1 2 0 0 1 1 0]
labels:       [2 0 2 1 1 1 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 1 1 0 0]
#############
predictions:  [0 0 0 0 0 1 2 2]
labels:       [2 2 1 1 0 2 1 2]
#############
predictions:  [0 1 0 0 1 1 0 1]
labels:       [0 1 0 2 1 0 0 1]
#############
predictions:  [0 2 2 0 0 0 0 2]
labels:       [0 0 2 1 2 2 0 1]
#############
predictions:  [2 0 0 2 2 0 2 0]
labels:       [2 1 0 2 2 0 1 0]
#############
predictions:  [0 2 2 0 2 1 1 2]
labels:       [1 1 0 0 2 1 0 0]
#############
predictions:  [1 1 2 2 0 0 1 2]
labels:       [2 0 2 0 1 1 1 1]
#############
predictions:  [0 1 1 0 0 1 1 0]
labels:       [0 2 1 0 0 2 2 0]
#############
predictions:  [2 1 0 0 0 0 1 1]
labels:       [0 2 0 2 0 0 1 1]
#############
predictions:  [1 2 0 1 2 0 0 0]
labels:       [1 0 1 1 0 2 2 2]
#############
predictions:  [0 0 0 0 2 2 1 2]
labels:       [2 2 2 1 1 1 1 0]
#############
predictions:  [1 2 0 1 0 0 2 0]
labels:       [2 2 1 1 1 0 2 0]
#############
predictions:  [2 0 0 0 0 2 1]
labels:       [0 1 1 2 2 0 2]
#############
======Average Training Loss: 1.11279======
======Average Training Accuracy: 33.77%======
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 2 1 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 2 2 1 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 2 0 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 0 0 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 1 2 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 1 2 0 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 1 2 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 2 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 2 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 1 1 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 2 2 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 1 2 2 1]
#############
predictions:  [0 0 0 1 0 0 0 0]
labels:       [0 0 1 1 0 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 2 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 2 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 1 2 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 0 1 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 2 2 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 1 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 1 1 2 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 1 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 0 2 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 1 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 0 1 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 1 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 1 0 0 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 0 0 0 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 1 1 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 1 2 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 0 1 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 0 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 2 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 0 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 1 1 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 2 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 0 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 1 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 2 1 1 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 2 2 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 2 1 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 2 2 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 0 1 0 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 0 0 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 1 2 0 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 2 0 1 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 0 0 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 0 0 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 0 1 1 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 0 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 2 1 1 1 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 2 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 1 0 2 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 1 2 2 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 0 0 2 1 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 0 2 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 2 0 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 0 1 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 2 0 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 1 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 2 2 1 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 2 0 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 1 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 1 1 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 1 0 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 2 1 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 2 0 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 2 0 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 2 2 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 0 1 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 2 0 1 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 0 0 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 1 2 0 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 1 0 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 0 1 0 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 1 2 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 2 0 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 0 1 1 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 1 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 1 2 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 1 1 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 1 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 0 1 1 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 0 2 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 0 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 1 2 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 0 1 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 1 0 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 2 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 2 2 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 1 2 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 1 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 2 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 0 0 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 1 2 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 0 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 1 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 0 2 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 1 1 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 0 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 2 2 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 2 2 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 1 0 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 1 0 2 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 0 2 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 0 1 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 1 0 0 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 0 1 2 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 2 2 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 0 1 2 0 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 1 1 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 0 0 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 0 1 2 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 1 2 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 1 1 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 0 0 0 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 1 2 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 1 0 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 2 2 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0]
labels:       [2 2 1 1 0 0 1]
#############
======Average Validation Loss: 1.09527======
======Average Validation Accuracy: 35.53%======
</code></pre>
"
64684506,Transformers get named entity prediction for words instead of tokens,"<p>This is very basic question, but I spend hours struggling to find the answer. I built NER using Hugginface transformers.</p>
<p>Say I have input sentence</p>
<pre class=""lang-py prettyprint-override""><code>input = &quot;Damien Hirst oil in canvas&quot;
</code></pre>
<p>I tokenize it to get</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')
tokenized = tokenizer.encode(input) #[101, 12587, 7632, 12096, 3514, 1999, 10683, 102]
</code></pre>
<p>Feed tokenized sentence to the model to get predicted tags for the tokens</p>
<pre class=""lang-py prettyprint-override""><code>['B-ARTIST' 'B-ARTIST' 'I-ARTIST' 'I-ARTIST' 'B-MEDIUM' 'I-MEDIUM'
 'I-MEDIUM' 'B-ARTIST']
</code></pre>
<p><code>prediction</code> comes as output from the model. It assigns tags to different tokens.</p>
<p>How can I recombine this data to obtain tags for words instead of tokens? So I would know that</p>
<pre><code>&quot;Damien Hirst&quot; = ARTIST
&quot;Oil in canvas&quot; = MEDIUM
</code></pre>
"
64800623,SpacyBert/SpacyCake cannot perform reduction function max on tensor,"<p>I've installed <em>SpacyBert</em> and <em>SpacyCake</em> to extract keyword phrases from a corpus of text. I've checked all the dependencies and have everything installed but I am getting the below error. Any ideas?</p>
<pre><code>File &quot;/usr/local/lib/python3.8/site-packages/spacy/language.py&quot;, line 449, in __call__ doc = proc(doc, **component_cfg.get(name, {})) File &quot;/usr/local/lib/python3.8/site-packages/spacycake/__init__.py&quot;, line 105, in __call__ second_part = torch.matmul( RuntimeError: cannot perform reduction function max on tensor with no elements because the operation does not have an identity
</code></pre>
<p>I have the correct language model downloaded so not sure what could be causing the issue. The models I have tested with:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_md&quot;) and
nlp = spacy.load(&quot;en&quot;)
</code></pre>
<p>Spacy is working fine because I can perform other NLP tasks but it's just when I try using:</p>
<pre><code>cake = bake(nlp, from_pretrained='bert-base-cased', top_k=3)
nlp.add_pipe(cake, last=True)

doc = nlp(&quot;This is a test but obviously you need to place a bigger document here to extract meaningful keyphrases&quot;)
print(doc._.extracted_phrases)
</code></pre>
"
64823332,Gradients returning None in huggingface module,"<p>I want to get the gradient of an embedding layer from a pytorch/huggingface model. Here's a minimal working example:</p>
<pre><code>from transformers import pipeline

nlp = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)

responses = [&quot;I'm having a great day!!&quot;]
hypothesis_template = 'This person feels {}'
candidate_labels = ['happy', 'sad']
nlp(responses, candidate_labels, hypothesis_template=hypothesis_template)
</code></pre>
<p>I can extract the logits just fine,</p>
<pre><code>inputs = nlp._parse_and_tokenize(responses, candidate_labels, hypothesis_template)
predictions = nlp.model(**inputs, return_dict=True, output_hidden_states=True)
predictions['logits']   
</code></pre>
<p>and the model returns a layer I'm interested in. I tried to retain the gradient and backprop with respect to a single logit I'm interested in:</p>
<pre class=""lang-py prettyprint-override""><code>layer = predictions['encoder_hidden_states'][0]
layer.retain_grad()
predictions['logits'][0][2].backward(retain_graph=True)
</code></pre>
<p>However, <code>layer.grad == None</code> no matter what I try. The other named parameters of the model have their gradients computed, so I'm not sure what I'm doing wrong. How do I get the grad of the encoder_hidden_states?</p>
"
64881478,Passing multiple sentences to BERT?,"<p>I have a dataset with paragraphs that I need to classify into two classes. These paragraphs are usually 3-5 sentences long. The overwhelming majority of them are less than 500 words long. I would like to make use of BERT to tackle this problem.</p>
<p>I am wondering how I should use BERT to generate vector representations of these paragraphs and especially, whether it is fine to just pass the whole paragraph into BERT?</p>
<p>There have been informative discussions of related problems <a href=""https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification/63413589#63413589"">here</a> and <a href=""https://stackoverflow.com/questions/63671085/how-to-use-bert-for-long-sentences"">here</a>. These discussions focus on how to use BERT for representing whole documents. In my case the paragraphs are not that long, and indeed could be passed to BERT without exceeding its maximum length of 512. However, BERT was trained on sentences. Sentences are relatively self-contained units of meaning. I wonder if feeding multiple sentences into BERT doesn't conflict fundamentally with what the model was designed to do (although this appears to be done regularly).</p>
"
65017564,Train n% last layers of BERT in Pytorch using HuggingFace Library (train Last 5 BERTLAYER out of 12 .),"<p>Bert has an Architecture something like <code>encoder -&gt; 12 BertLayer -&gt; Pooling</code>. I want to train the last 40% layers of Bert Model. I can freeze all the layers as:</p>
<pre><code># freeze parameters
bert = AutoModel.from_pretrained('bert-base-uncased')
for param in bert.parameters():
    param.requires_grad = False

</code></pre>
<p>But I want to Train last 40% layers. When I do <code>len(list(bert.parameters()))</code>, it gives me 199. So let us suppose 79 is the 40% of parameters. Can I do something like:</p>
<pre><code>for param in list(bert.parameters())[-79:]: # total  trainable 199 Params: 79 is 40%
    param.requires_grad = False
</code></pre>
<p>I think it will freeze first 60% layers.</p>
<p>Also, can someone tell me that which layers it will freeze according to architecture?</p>
"
65023526,RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1,"<p>I'm trying to build a model for document classification. I'm using <code>BERT</code> with <code>PyTorch</code>.</p>
<p>I got the bert model with below code.</p>
<pre><code>bert = AutoModel.from_pretrained('bert-base-uncased')
</code></pre>
<p>This is the code for training.</p>
<pre><code>for epoch in range(epochs):
 
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))

    #train model
    train_loss, _ = modhelper.train(proc.train_dataloader)

    #evaluate model
    valid_loss, _ = modhelper.evaluate()

    #save the best model
    if valid_loss &lt; best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(modhelper.model.state_dict(), 'saved_weights.pt')

    # append training and validation loss
    train_losses.append(train_loss)
    valid_losses.append(valid_loss)

    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')
</code></pre>
<p>this is my train method, accessible with the object <code>modhelper</code>.</p>
<pre><code>def train(self, train_dataloader):
    self.model.train()
    total_loss, total_accuracy = 0, 0
    
    # empty list to save model predictions
    total_preds=[]
    
        # iterate over batches
    for step, batch in enumerate(train_dataloader):
        
        # progress update after every 50 batches.
        if step % 50 == 0 and not step == 0:
            print('  Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))
        
        # push the batch to gpu
        #batch = [r.to(device) for r in batch]
        
        sent_id, mask, labels = batch
        
        # clear previously calculated gradients 
        self.model.zero_grad()        

        print(sent_id.size(), mask.size())
        # get model predictions for the current batch
        preds = self.model(sent_id, mask) #This line throws the error
        
        # compute the loss between actual and predicted values
        self.loss = self.cross_entropy(preds, labels)
        
        # add on to the total loss
        total_loss = total_loss + self.loss.item()
        
        # backward pass to calculate the gradients
        self.loss.backward()
        
        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        # update parameters
        self.optimizer.step()
        
        # model predictions are stored on GPU. So, push it to CPU
        #preds=preds.detach().cpu().numpy()
        
        # append the model predictions
        total_preds.append(preds)
      
    # compute the training loss of the epoch
    avg_loss = total_loss / len(train_dataloader)
    
    # predictions are in the form of (no. of batches, size of batch, no. of classes).
    # reshape the predictions in form of (number of samples, no. of classes)
    total_preds  = np.concatenate(total_preds, axis=0)
      
    #returns the loss and predictions
    return avg_loss, total_preds
</code></pre>
<p><code>preds = self.model(sent_id, mask)</code> this line throws the following error(including full traceback).</p>
<pre><code> Epoch 1 / 1
torch.Size([32, 4000]) torch.Size([32, 4000])
Traceback (most recent call last):

File &quot;&lt;ipython-input-39-17211d5a107c&gt;&quot;, line 8, in &lt;module&gt;
train_loss, _ = modhelper.train(proc.train_dataloader)

File &quot;E:\BertTorch\model.py&quot;, line 71, in train
preds = self.model(sent_id, mask)

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\model.py&quot;, line 181, in forward
#pass the inputs to the model

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 837, in forward
embedding_output = self.embeddings(

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 201, in forward
embeddings = inputs_embeds + position_embeddings + token_type_embeddings

RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>If you observe I've printed the torch size in the code.
<code>print(sent_id.size(), mask.size())</code></p>
<p>The output of that line of code is <code>torch.Size([32, 4000]) torch.Size([32, 4000])</code>.</p>
<p>as we can see that size is the same but it throws the error. Please put your thoughts. Really appreciate it.</p>
<p>please comment if you need further information. I'll be quick to add whatever is required.</p>
"
65072694,Make sure BERT model does not load pretrained weights?,"<p>I want to make sure my BertModel does not loads pre-trained weights. I am using auto class (hugging face) which loads model automatically.</p>
<p>My question is how do I load bert model without pretrained weights?</p>
"
65091635,"ValueError: logits and labels must have the same shape ((1, 21) vs (21, 1))","<p>I am trying to reproduce <a href=""https://stackoverflow.com/questions/61389018/big-loss-and-low-accuracy-on-training-data-in-both-bert-and-albert"">this</a> example using huggingface <code>TFBertModel</code> to do a classification task.</p>
<p>My model is almost the same of the example, but I'm performing multilabel classification. For this reason, I've performed the binarization of my labels using sklearn's <code>MultiLabelBinarizer</code>.</p>
<p>Then, I've adapted my model to have the predictions accordingly.</p>
<pre><code>def loadBertModel(max_length,n_classes):

  bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')

  input_ids = keras.Input(shape=(max_length,), dtype=np.int32)
  attention_mask = keras.Input(shape=(max_length,), dtype=np.int32)
  token_type_ids = keras.Input(shape=(max_length,), dtype=np.int32)

  _, output = bert_model([input_ids, attention_mask,token_type_ids])
  
  output = keras.layers.Dense(n_classes, activation=&quot;sigmoid&quot;, name=&quot;dense_out_dom&quot;)(output)

  model = keras.Model(
    inputs=[input_ids, attention_mask,token_type_ids],
    outputs=output,
    name='bert_classifier',
  )       

  model.compile(
    optimizer=Adam(lr=2e-5),  
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
  )

  model.summary()
  return model
</code></pre>
<p>Also, I'm using tensorflow's <code>Dataset</code> to produce my model's inputs:</p>
<pre><code>def map_example_to_dict(input_ids, attention_masks, token_type_ids, label): 
  return {
      &quot;input_ids&quot;: input_ids,
      &quot;token_type_ids&quot;: token_type_ids,
      &quot;attention_mask&quot;: attention_masks,
  }, label
  

def tokenize_sequences(tokenizer, max_length, corpus, labels):
  input_ids = []
  token_type_ids = []
  attention_masks = []

  for i in tqdm(range(len(corpus))):
    encoded = tokenizer.encode_plus(
        corpus[i], 
        max_length=max_length, 
        add_special_tokens=True,
        padding='max_length',
        truncation=True,
        return_token_type_ids=True,
        return_attention_mask=True,  # add attention mask to not focus on pad tokens)
        return_tensors=&quot;tf&quot;
    )
    input_ids.append(encoded[&quot;input_ids&quot;])
    attention_masks.append(encoded[&quot;attention_mask&quot;])
    token_type_ids.append(encoded[&quot;token_type_ids&quot;])

  input_ids = tf.convert_to_tensor(input_ids)
  attention_masks = tf.convert_to_tensor(attention_masks)
  token_type_ids = tf.convert_to_tensor(token_type_ids)
  
  labels = labels.toarray()

  return tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, token_type_ids, labels)).map(map_example_to_dict)
</code></pre>
<p>Finally, when I try to fit my model, I have an incoherence concerning the logits and the labels' shapes:</p>
<pre><code>ValueError: logits and labels must have the same shape ((1, 21) vs (21, 1))
</code></pre>
<p>I really don't know if the <code>Dataset</code> transformation is messing with my inputs' shapes or if I'm missing some other detail. Any ideas?</p>
<p>Full stack trace:</p>
<p>ValueError                                Traceback (most recent call last)</p>
<pre><code>&lt;ipython-input-42-19f4c0665eeb&gt; in &lt;module&gt;()
      4       epochs=N_EPOCHS,
      5       verbose=1,
----&gt; 6       batch_size=1,
      7       )

10 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--&gt; 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-&gt; 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = &quot;nonXla&quot;
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--&gt; 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-&gt; 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__
        losses = ag_call(y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1605 binary_crossentropy
        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4814 binary_crossentropy
        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits
        (logits.get_shape(), labels.get_shape()))

    ValueError: logits and labels must have the same shape ((1, 21) vs (21, 1))
</code></pre>
"
65132144,BertModel transformers outputs string instead of tensor,"<p>I'm following <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""noreferrer"">this</a> tutorial that codes a sentiment analysis classifier using BERT with the <a href=""https://huggingface.co/"" rel=""noreferrer"">huggingface</a> library and I'm having a very odd behavior. When trying the BERT model with a sample text I get a string instead of the hidden state. This is the code I'm using:</p>
<pre><code>import transformers
from transformers import BertModel, BertTokenizer

print(transformers.__version__)

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
PATH_OF_CACHE = &quot;/home/mwon/data-mwon/paperChega/src_classificador/data/hugingface&quot;

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,cache_dir = PATH_OF_CACHE)

sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'

encoding_sample = tokenizer.encode_plus(
  sample_txt,
  max_length=32,
  add_special_tokens=True, # Add '[CLS]' and '[SEP]'
  return_token_type_ids=False,
  padding=True,
  truncation = True,
  return_attention_mask=True,
  return_tensors='pt',  # Return PyTorch tensors
)

bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,cache_dir = PATH_OF_CACHE)


last_hidden_state, pooled_output = bert_model(
  encoding_sample['input_ids'],
  encoding_sample['attention_mask']
)

print([last_hidden_state,pooled_output])
</code></pre>
<p>that outputs:</p>
<pre><code>4.0.0
['last_hidden_state', 'pooler_output']
 
</code></pre>
"
65140400,ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds,"<p>Trying to convert a <code>question-generation</code> t5 model to <code>torchscript</code> <a href=""http://%5Bmodel%5D(https://huggingface.co/transformers/torchscript.html#torchscript)."" rel=""nofollow noreferrer"">model</a>, while doing that Running into this error</p>
<p><strong>ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds</strong></p>
<p>here's the code that I ran on colab.</p>
<pre><code>!pip install -U transformers==3.0.0
!python -m nltk.downloader punkt

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model = AutoModelForSeq2SeqLM.from_pretrained('valhalla/t5-base-qg-hl')

t_input =  'Python is a programming language. It is developed by &lt;hl&gt; Guido Van Rossum &lt;hl&gt;. &lt;/s&gt;'

tokenizer = AutoTokenizer.from_pretrained('valhalla/t5-base-qg-hl', return_tensors = 'pt')

def _tokenize(
    inputs,
    padding=True,
    truncation=True,
    add_special_tokens=True,
    max_length=64
):
    inputs = tokenizer.batch_encode_plus(
        inputs, 
        max_length=max_length,
        add_special_tokens=add_special_tokens,
        truncation=truncation,
        padding=&quot;max_length&quot; if padding else False,
        pad_to_max_length=padding,
        return_tensors=&quot;pt&quot;
    )
    return inputs

token = _tokenize(t_input, padding=True, truncation=True)


traced_model = torch.jit.trace(model, [token['input_ids'], token['attention_mask']] )
torch.jit.save(traced_model, &quot;traced_t5.pt&quot;)
</code></pre>
<p>got this error</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-1-f9b449524ef1&gt; in &lt;module&gt;()
     32 
     33 
---&gt; 34 traced_model = torch.jit.trace(model, [token['input_ids'], token['attention_mask']] )
     35 torch.jit.save(traced_model, &quot;traced_t5.pt&quot;)

7 frames
/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states)
    682         else:
    683             if self.is_decoder:
--&gt; 684                 raise ValueError(&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;)
    685             else:
    686                 raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)

ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
</code></pre>
<p>how to resolve this issue? or is there a better way for converting the t5 model to <code>torchscript</code>.</p>
<p>thank you.</p>
"
65221079,What do the logits and probabilities from RobertaForSequenceClassification represent?,"<p>Being new to the &quot;Natural Language Processing&quot; scene, I am experimentally learning and have implemented the following segment of code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
    
path = &quot;D:/LM/rb/&quot;
tokenizer = RobertaTokenizer.from_pretrained(path)
model = RobertaForSequenceClassification.from_pretrained(path)
    
inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
pred_logits = outputs.logits
print(pred_logits)
probs = pred_logits.softmax(dim=-1).detach().cpu().flatten().numpy().tolist()
print(probs)
</code></pre>
<p>I <em>understand</em> that applying the model returns a <em>&quot;<code>torch.FloatTensor</code> comprising various elements depending on the configuration (RobertaConfig) and inputs&quot;</em>, and that the logits are accessible using <code>.logits</code>. As demonstrated I have applied the <em>.softmax</em> function to the tensor to return normalised probabilities and have converted the result into a list. I am outputted with the following:</p>
<pre><code>[0.5022980570793152, 0.49770188331604004]
</code></pre>
<p>Do these probabilities represent some kind of overall &quot;masked&quot; probability?</p>
<p><strong>What do the first and second index represent in context of the input?</strong></p>
<hr />
<p>EDIT:</p>
<pre><code>model.num_labels
</code></pre>
<p>Output:</p>
<pre><code>2
</code></pre>
<p><a href=""https://stackoverflow.com/users/6664872/cronoik"">@cronoik</a> explains that the model &quot;tries to classify if a sequence belongs to one class or another&quot;</p>
<p>Am I to assume that because there are no trained output layers these classes don't mean anything yet?</p>
<p>For example, I <em>can</em> assume that the probability that the sentence, post analysis, belongs to class 1 is 0.5. However, what is class 1?</p>
<p>Additionally, model cards with pre-trained output layers such as the <a href=""https://huggingface.co/roberta-large-openai-detector"" rel=""nofollow noreferrer"">open-ai detector</a> help differentiate between what is <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;real&quot;</a> and <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;fake&quot;</a>, and so I can assume the class that a sentence belongs to. However, how can I confirm these &quot;labels&quot; without some type of &quot;mapping.txt&quot; file?</p>
"
65242786,Metrics mismatch between BertForSequenceClassification Class and my custom Bert Classification,"<p>I implemented my custom Bert Binary Classification Model class, by adding a classifier layer on top of Bert Model (attached below). However, the accuracy/metrics are significantly different when I train with the official BertForSequenceClassification model, which makes me wonder if I am missing somehting in my class.</p>
<p>Few Doubts I have:</p>
<p>While loading the official <code>BertForSequenceClassification</code> <code>from_pretrained</code> are the classifiers weight initialized as well from pretrained model or they are randomly initialized? Because in my custom class they are randomly initialized.</p>
<pre class=""lang-py prettyprint-override""><code>class MyCustomBertClassification(nn.Module):
    def __init__(self, encoder='bert-base-uncased',
                        num_labels,
                        hidden_dropout_prob):

    super(MyCustomBertClassification, self).__init__()
    self.config  = AutoConfig.from_pretrained(encoder)
    self.encoder = AutoModel.from_config(self.config)
    self.dropout = nn.Dropout(hidden_dropout_prob)
    self.classifier = nn.Linear(self.config.hidden_size, num_labels)

def forward(self, input_sent):
    outputs = self.encoder(input_ids=input_sent['input_ids'],
                          attention_mask=input_sent['attention_mask'],
                          token_type_ids=input_sent['token_type_ids'],
                          return_dict=True)
    
    pooled_output = self.dropout(outputs[1])
    # for both tasks
    logits = self.classifier(pooled_output)

    return logits
</code></pre>
"
65337804,calculate two losses in a model and backpropagate twice,"<p>I'm creating a model using BertModel to identify answer span (without using BertForQA).</p>
<p>I have an indepent linear layer for determining start and end token respectively. In <strong>init</strong>():</p>
<pre><code>self.start_linear = nn.Linear(h, output_dim)

self.end_linear = nn.Linear(h, output_dim)
</code></pre>
<p>In forward(), I output a predicted start layer and predicted end layer:</p>
<pre><code> def forward(self, input_ids, attention_mask):

 outputs = self.bert(input_ids, attention_mask) # input = bert tokenizer encoding

 lhs = outputs.last_hidden_state # (batch_size, sequence_length, hidden_size)

 out = lhs[:, -1, :] # (batch_size, hidden_dim)

 st = self.start_linear(out)

 end = self.end_linear(out) 



 predict_start = self.softmax(st)

 predict_end = self.softmax(end)

 return predict_start, predict_end
</code></pre>
<p>Then in train_epoch(), I tried to backpropagate the losses separately:</p>
<pre><code>def train_epoch(model, train_loader, optimizer):

 model.train()

 total = 0

 st_loss, st_correct, st_total_loss = 0, 0, 0

 end_loss, end_correct, end_total_loss = 0, 0, 0

 for batch in train_loader:

   optimizer.zero_grad()

   input_ids = batch['input_ids'].to(device)

   attention_mask = batch['attention_mask'].to(device)

   start_idx = batch['start'].to(device)

   end_idx = batch['end'].to(device)

   start, end = model(input_ids=input_ids, attention_mask=attention_mask)


   st_loss = model.compute_loss(start, start_idx)

   end_loss = model.compute_loss(end, end_idx)

   st_total_loss += st_loss.item()

   end_total_loss += end_loss.item()

 # perform backward propagation to compute the gradients

   st_loss.backward()

   end_loss.backward()

 # update the weights

   optimizer.step() 
</code></pre>
<p>But then I got on the line of <code>end_loss.backward()</code>:</p>
<pre><code>Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
</code></pre>
<p>Am I supposed to do the backward pass separately? Or should I do it in another way? Thank you!</p>
"
65353104,"I want to use ""grouped_entities"" in the huggingface pipeline for ner task, how to do that?","<p>I want to use &quot;grouped_entities&quot; in the huggingface pipeline for ner task. However having issues doing that.</p>
<p>I do look the following link on git but this did not help:
<a href=""https://github.com/huggingface/transformers/pull/4987"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/pull/4987</a></p>
"
65383059,Unable to import Hugging Face transformers,"<p>I have been using transformers fine up until today. However, when I imported the package today, I received this error message:</p>
<pre><code>In Transformers v4.0.0, the default path to cache downloaded models changed from '~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to '~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should only see this message once.

Error: Destination path '/home/user/.cache/huggingface/transformers/transformers' already exists
</code></pre>
<p>I have tried to install and uninstall the package but still unable to make it work.</p>
<p>Any suggestions to fix this would be really appreciated.</p>
"
65396650,BERT DataLoader: Difference between shuffle=True vs Sampler?,"<p>I trained a DistilBERT model with DistilBertForTokenClassification on ConLL data fro predicting NER. Training seem to have completed with no problems but I have 2 problems during evaluation phase.</p>
<ol>
<li><p>I'm getting negative loss value</p>
</li>
<li><p>During training, I used shuffle=True for DataLoader. But during evaluation, when I do shuffle=True for DataLoader, I get very poor metric results(f_1, accuracy, recall etc). But if I do shuffle = False or use a Sampler instead of shuffling I get pretty good metric results. I'm wondering if there is anything wrong with my code.</p>
</li>
</ol>
<p>Here is the evaluation code:</p>
<hr />
<pre><code>print('Prediction started on test data')
model.eval()

eval_loss = 0
predictions , true_labels = [], []

for batch in val_loader:
  b_input_ids = batch['input_ids'].to(device)
  b_input_mask = batch['attention_mask'].to(device)
  b_labels = batch['labels'].to(device)

  with torch.no_grad():
      outputs = model(b_input_ids, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.detach().cpu().numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

  eval_loss += outputs[0].mean().item()


print('Prediction completed')
eval_loss = eval_loss / len(val_loader)
print(&quot;Validation loss: {}&quot;.format(eval_loss))
</code></pre>
<p>out:</p>
<pre><code>Prediction started on test data
Prediction completed
Validation loss: -0.2584906197858579
</code></pre>
<p>I believe I'm calculating the loss wrong here. Is it possible to get negative loss values with BERT?</p>
<p>For DataLoader, if I use the code snippet below, I have no problems with the metric results.</p>
<pre><code>val_sampler = SequentialSampler(val_dataset)
val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=128)
</code></pre>
<p>Bu if I do this one I get very poor metric results</p>
<pre><code>val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)
</code></pre>
<p>Is it normal that I'm getting vastly different results with shuffle=True vs shuffle=False ?</p>
<p>code for the metric calculation:</p>
<pre><code>metric = load_metric(&quot;seqeval&quot;)
results = metric.compute(predictions=true_predictions, references=true_labels)
results
</code></pre>
<p>out:</p>
<pre><code>{'LOCATION': {'f1': 0.9588207767898924,
  'number': 2134,
  'precision': 0.9574766355140187,
  'recall': 0.9601686972820993},
 'MISC': {'f1': 0.8658965344048217,
  'number': 995,
  'precision': 0.8654618473895582,
  'recall': 0.8663316582914573},
 'ORGANIZATION': {'f1': 0.9066332916145182,
  'number': 1971,
  'precision': 0.8947628458498024,
  'recall': 0.9188229325215627},
 'PERSON': {'f1': 0.9632426988922457,
  'number': 2015,
  'precision': 0.9775166070516096,
  'recall': 0.9493796526054591},
 'overall_accuracy': 0.988255561629313,
 'overall_f1': 0.9324058459808882,
 'overall_precision': 0.9322748349023465,
 'overall_recall': 0.932536893886156}
</code></pre>
<p>The above metrics are printed when I use Sampler or shuffle=False. If I use shuffle=True, I get:</p>
<pre><code>{'LOCATION': {'f1': 0.03902284263959391,
  'number': 2134,
  'precision': 0.029496402877697843,
  'recall': 0.057638238050609185},
 'MISC': {'f1': 0.010318142734307824,
  'number': 995,
  'precision': 0.009015777610818933,
  'recall': 0.012060301507537688},
 'ORGANIZATION': {'f1': 0.027420984269014285,
  'number': 1971,
  'precision': 0.019160951996772892,
  'recall': 0.04819888381532217},
 'PERSON': {'f1': 0.02119907254057635,
  'number': 2015,
  'precision': 0.01590852597564007,
  'recall': 0.03176178660049628},
 'overall_accuracy': 0.5651741788003777,
 'overall_f1': 0.02722600361161272,
 'overall_precision': 0.020301063389034663,
 'overall_recall': 0.041321152494729445}
</code></pre>
<p>UPDATE: I modified loss code for evaluation. There seems to be no problem with this code. You can see the new code below:</p>
<pre><code>print('Prediction started on test data')
model.eval()

eval_loss = 0
predictions , true_labels = [], []

for batch in val_loader:

  b_labels = batch['labels'].to(device)

  batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}
  
  with torch.no_grad():
      outputs = model(**batch)

      loss, logits = outputs[0:2]
      logits = logits.detach().cpu().numpy()
      label_ids = b_labels.detach().cpu().numpy()
  
      predictions.append(logits)
      true_labels.append(label_ids)

      eval_loss += loss


print('Prediction completed')
eval_loss = eval_loss / len(val_loader)
print(&quot;Validation loss: {}&quot;.format(eval_loss))
</code></pre>
<p>Though I still haven't got an asnwer to the DataLoader question.
Also I jsut realised when I do <code>print(model.eval())</code> I still get dropouts from the model in evaluation mode.</p>
"
65408757,"Huggingface MarianMT translators lose content, depending on the model","<p><strong>Context</strong></p>
<p>I am using MarianMT von Huggingface via Python in order to translate text from a source to a target language.</p>
<p><strong>Expected behaviour</strong></p>
<p>I enter a sequence into the MarianMT model and get this sequence translated back. For this, I use a corresponding language model and a tokeniser. All the sentences I enter also come back. The sentences are treated as a sequence.</p>
<p><strong>Current behaviour</strong></p>
<p>Depending on the language model, the model does not translate everything, but only returns parts. In this example, the last sentence is missing:</p>
<p><strong>Original (German):</strong> Ein Nilpferd lief im Dschungel rum und musste aufs WC. Da traf es einen Kakadu und fragte nach dem Weg. Der sagte wenn du Kaka musst, dann pass mal ganz kurz auf. Ich sag dir wo du hingehen musst, ich kenn mich hier gut aus.</p>
<p><strong>Result (English):</strong>  A hippopotamus ran around in the jungle and had to go to the toilet. There was a cockatoo and asked for the way. He said if you have to Kaka, then watch out for a minute. <strong>I'll tell you where you have to go, I know my way around here.</strong></p>
<p><strong>Result (Dutch):</strong>  Een nijlpaard liep rond in de jungle en moest naar het toilet... en een kaketoe vroeg naar de weg... die zei dat als je Kaka moest, ik even moest oppassen.</p>
<p><strong>Current Code</strong></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


def translate_text(input, source, target):

    # Prepare output
    output = &quot;&quot;

    model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-&quot; + source + &quot;-&quot; + target)
    tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-&quot; + source + &quot;-&quot; + target)

    inputs = tokenizer.encode(input[:512], return_tensors=&quot;pt&quot;, padding='longest')
    outputs = model.generate(inputs, max_length=4000, num_beams=4, early_stopping=True)

    for t in [tokenizer.convert_ids_to_tokens(s) for s in outputs.tolist()[0]]:
        output = output + t.replace(&quot;â–&quot;, &quot; &quot;).replace(&quot;&lt;/s&gt;&quot;, &quot;&quot;)

    output.replace(&quot;&lt;pad&gt;&quot;, &quot;&quot;)

    return output


print(translate_text(&quot;Ein Nilpferd lief im Dschungel rum und musste aufs WC. Da traf es einen Kakadu und fragte nach dem Weg. Der sagte wenn du Kaka musst, dann pass mal ganz kurz auf. Ich sag dir wo du hingehen musst, ich kenn mich hier gut aus.&quot;, &quot;de&quot;, &quot;nl&quot;))
print(translate_text(&quot;Ein Nilpferd lief im Dschungel rum und musste aufs WC. Da traf es einen Kakadu und fragte nach dem Weg. Der sagte wenn du Kaka musst, dann pass mal ganz kurz auf. Ich sag dir wo du hingehen musst, ich kenn mich hier gut aus.&quot;, &quot;de&quot;, &quot;en&quot;))
</code></pre>
<p><strong>Help needed</strong></p>
<p>What do I miss? Why are some sequence parts missing?</p>
"
65419656,Parsing the Hugging Face Transformer Output,"<p>I am looking to use <code>bert-english-uncased-finetuned-pos</code> transformer, mentioned here</p>
<p><a href=""https://huggingface.co/vblagoje/bert-english-uncased-finetuned-pos?text=My+name+is+Clara+and+I+live+in+Berkeley%2C+California"" rel=""nofollow noreferrer"">https://huggingface.co/vblagoje/bert-english-uncased-finetuned-pos?text=My+name+is+Clara+and+I+live+in+Berkeley%2C+California</a>.</p>
<p>I am querying the transformer this way...</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained(&quot;vblagoje/bert-english-uncased-finetuned-pos&quot;)

model = AutoModelForTokenClassification.from_pretrained(&quot;vblagoje/bert-english-uncased-finetuned-pos&quot;)

text = &quot;My name is Clara and I live in Berkeley, California.&quot;
input_ids = tokenizer.encode(text + '&lt;/s&gt;', return_tensors='pt')
outputs = model(input_ids)
</code></pre>
<p>But the <code>outputs</code> is coming something like this</p>
<blockquote>
<p>(tensor([[[-1.8196e+00, -1.9783e+00, -1.7416e+00,  1.2082e+00,
-7.0337e-02,
-7.0322e-03,  3.4300e-01, -9.6914e-01, -1.3546e+00,  7.7266e-03,
3.7128e+00, -3.4061e-01,  4.8385e+00, -1.2548e+00, -5.1845e-01,
7.0140e-01,  1.0394e+00],<br />
[-1.2702e+00, -1.5518e+00, -1.1553e+00, -4.4077e-01, -9.8661e-01,
-3.2680e-01, -6.5338e-01, -3.9779e-01, -7.5383e-01, -1.2677e+00,
9.6353e+00,  1.9938e-01, -1.0282e+00, -7.5071e-01, -1.0307e+00,
-8.0589e-01,  4.2073e-01],<br />
[-9.6988e-01, -5.0090e-01, -1.3858e+00, -1.0554e+00, -1.4040e+00,
-7.5977e-01, -7.4156e-01,  8.0594e+00, -5.1854e-01, -1.9098e+00,
-1.6362e-02,  1.0594e+00, -8.4962e-01, -1.7415e+00, -1.0628e+00,
-1.7485e-01, -1.1490e+00],<br />
[-1.4368e+00, -1.6313e-01, -1.3202e+00,  8.7465e+00, -1.3782e+00,
-9.8889e-01, -1.1371e+00, -1.0917e+00, -9.8495e-01, -9.3237e-01,
-9.6111e-01, -4.1658e-01, -7.3133e-01, -9.6004e-01, -9.5337e-01,
3.1836e+00, -8.3462e-01],<br />
[-7.9476e-01, -7.9640e-01, -9.0027e-01, -6.9506e-01, -8.9706e-01,
-6.9383e-01, -3.1590e-01,  1.2390e+00, -1.0443e+00, -9.9977e-01,
-8.8189e-01,  8.7941e+00, -9.9445e-01, -1.2076e+00, -1.1424e+00,
-9.7801e-01,  5.6683e-01],<br />
[-8.2837e-01, -5.5060e-01, -2.1352e-01, -8.8721e-01,  9.5536e+00,
1.0478e+00, -5.6208e-01, -7.1037e-01, -7.0248e-01,  1.1298e-01</p>
<p>...</p>
<p>-7.3788e-01,  4.3640e-03,  1.6994e+00,  1.1528e-01, -1.0983e+00,
-8.9202e-01, -1.2869e+00,  4.9141e+00, -6.2096e-01,  4.8374e+00,
3.2384e-01,  4.6213e-01],<br />
[-1.3622e+00,  2.0772e+00, -1.6680e+00, -8.8679e-01, -8.6959e-01,
-1.7468e+00, -1.1424e+00,  1.6996e+00,  3.5800e-01, -4.3927e-01,
-3.6129e-01, -4.2220e-01, -1.7912e+00,  8.0154e-01,  7.4594e-01,
-1.0620e+00,  3.8152e+00],<br />
[-1.2889e+00, -2.9379e-01, -1.6543e+00, -4.3326e-01, -2.4919e-01,
-4.0112e-01, -4.4255e-01,  2.2697e-01, -4.6042e-01, -3.7862e-03,
-6.3061e-01, -1.3280e+00,  8.5533e+00, -4.6881e-01,  2.3882e+00,
2.4533e-01, -1.4095e-01],<br />
[-9.5640e-01, -5.7213e-01, -1.0245e+00, -5.3566e-01, -1.5287e-01,
-6.6977e-01, -5.3392e-01, -3.1967e-02, -7.3077e-01, -3.1048e-01,
-7.2973e-01, -3.1701e-01,  1.0196e+01, -5.2346e-01,  4.0820e-01,
-2.1350e-01,  1.0340e+00]]], grad_fn=),)</p>
</blockquote>
<p>But as per the documentation, I am expecting output to be in a JSON format...</p>
<blockquote>
<p>[   {
&quot;entity_group&quot;: &quot;PRON&quot;,
&quot;score&quot;: 0.9994694590568542,
&quot;word&quot;: &quot;my&quot;   },   {
&quot;entity_group&quot;: &quot;NOUN&quot;,
&quot;score&quot;: 0.997125506401062,
&quot;word&quot;: &quot;name&quot;   },   {
&quot;entity_group&quot;: &quot;AUX&quot;,
&quot;score&quot;: 0.9938186407089233,
&quot;word&quot;: &quot;is&quot;   },   {
&quot;entity_group&quot;: &quot;PROPN&quot;,
&quot;score&quot;: 0.9983252882957458,
&quot;word&quot;: &quot;clara&quot;   },   {
&quot;entity_group&quot;: &quot;CCONJ&quot;,
&quot;score&quot;: 0.9991229772567749,
&quot;word&quot;: &quot;and&quot;   },   {
&quot;entity_group&quot;: &quot;PRON&quot;,
&quot;score&quot;: 0.9994894862174988,
&quot;word&quot;: &quot;i&quot;   },   {
&quot;entity_group&quot;: &quot;VERB&quot;,
&quot;score&quot;: 0.9983153939247131,
&quot;word&quot;: &quot;live&quot;   },   {
&quot;entity_group&quot;: &quot;ADP&quot;,
&quot;score&quot;: 0.999370276927948,
&quot;word&quot;: &quot;in&quot;   },   {
&quot;entity_group&quot;: &quot;PROPN&quot;,
&quot;score&quot;: 0.9987357258796692,
&quot;word&quot;: &quot;berkeley&quot;   },   {
&quot;entity_group&quot;: &quot;PUNCT&quot;,
&quot;score&quot;: 0.9996636509895325,
&quot;word&quot;: &quot;,&quot;   },   {
&quot;entity_group&quot;: &quot;PROPN&quot;,
&quot;score&quot;: 0.9985638856887817,
&quot;word&quot;: &quot;california&quot;   },   {
&quot;entity_group&quot;: &quot;PUNCT&quot;,
&quot;score&quot;: 0.9996631145477295,
&quot;word&quot;: &quot;.&quot;   } ]</p>
</blockquote>
<p>What am I doing wrong? How can I parse the current output to the desired JSON output?</p>
"
65431837,Transformers v4.x: Convert slow tokenizer to fast tokenizer,"<p>I'm following the transformer's pretrained model <a href=""https://huggingface.co/joeddav/xlm-roberta-large-xnli?text=%0A&amp;candidate_labels=&amp;multi_class=true"" rel=""noreferrer"">xlm-roberta-large-xnli</a> example</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;,
                      model=&quot;joeddav/xlm-roberta-large-xnli&quot;)
</code></pre>
<p>and I get the following error</p>
<pre><code>ValueError: Couldn't instantiate the backend tokenizer from one of: (1) a `tokenizers` library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one.
</code></pre>
<p>I'm using Transformers version <code>'4.1.1'</code></p>
"
65464004,"Running a flask app that uses tensorflow on an Ubuntu 16.04 instance on GCP, model runs but predictions are different than on local host","<p>I trained a BERT model using huggingface for tensorflow on my localhost. Running predictions on my localhost works fine.</p>
<p>I then implemented a solution so I can call my model from a GCP VM instance (Ubuntu 16.04) via flask. The process seems to work as I can successfully make the calls to my app on the VM.</p>
<p>However, the prediction I receive from the VM differs from the one I receive on my localhost (which is the expected output), yet I use identical code. I use a model for Sequence Classification, and when trying to get the probabilities for both labels on my localhost I get: <code>array([0.67829543, 0.32170454], dtype=float32)</code> while the VM returns <code>array([1, 1], dtype=float32)</code>.
This snippet is what I use to predict the model just for reference:</p>
<pre><code>predict_input = tokenizer.encode(sentence,
                                  truncation=True,
                                  padding=True,
                                  return_tensors=&quot;tf&quot;
                                  )
tf_output = model.predict(predict_input)[0]
    
tf_prediction = tf.nn.softmax(tf_output, axis=0).numpy()
</code></pre>
<p>On my localhost I trained the model using tf with GPU support, the VM of course only has two vCPUs. When loading tensorflow on the VM I get the following warnings:</p>
<pre><code>2020-12-27 07:57:55.533847: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-12-27 07:57:55.533896: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-12-27 07:57:56.792914: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-12-27 07:57:56.792966: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-12-27 07:57:56.793002: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bertvm-1): /proc/driver/nvidia/version does not exist
2020-12-27 07:57:56.793316: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 A
VX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-27 07:57:56.801469: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000129999 Hz
2020-12-27 07:57:56.801693: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x64b8fe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-27 07:57:56.801805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
</code></pre>
<p>I'm not sure if that is the root of the error or if it's because I trained the model using tf for GPU and am predicting on an instance that runs tf for CPU but that doesn't seem to make too much sense to me.
The warnings only seem to pertain to CUDA 'issues' which I believe is related to GPU support.</p>
<p>Any idea or tips as to what could be the cause of the different predictions?
Thanks for your help in advance!</p>
<p>EDIT:
It seems that the model returns the same logits on both, the VM and the localhost. When I then apply <code>tf.nn.softmax(tf_output, axis=0).numpy()</code> I get different results.
tf_output being <code>[1.9530067 1.2070574]</code> on both instances while the above function returns <code>[0.67829543 0.32170454]</code> on localhost and <code>[[1. 1.]]</code> on the VM (both formatted here as a string) as mentioned above.</p>
"
65484081,Translating using pre-trained hugging face transformers not working,"<p>I have a situation where I am trying to using the pre-trained hugging-face models to translate a pandas column of text from Dutch to English. My input is simple:</p>
<pre><code>Dutch_text             
Hallo, het gaat goed
Hallo, ik ben niet in orde
Stackoverflow is nuttig
</code></pre>
<p>I am using the below code to translate the above column and I want to store my result into a new column ENG_Text. So the output will look like this:</p>
<pre><code>ENG_Text             
Hello, I am good
Hi, I'm not okay
Stackoverflow is helpful
</code></pre>
<p>The code that I am using is as follows:</p>
<pre><code>#https://huggingface.co/Helsinki-NLP for other pretrained models 
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
input_1 = df['Dutch_text']
input_ids = tokenizer(&quot;translate English to Dutch: &quot;+input_1, return_tensors=&quot;pt&quot;).input_ids # Batch size 1
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)
</code></pre>
<p>Any help would be appreciated!</p>
"
65499783,Apply transformer model to each row in a pandas column,"<p>I have a situation where I want to apply a translation model to each and every row in one of data frame columns.</p>
<p>The translation code that I am using :</p>
<pre><code>from transformers import FSMTForConditionalGeneration, FSMTTokenizer
mname = &quot;allenai/wmt19-de-en-6-6-big&quot;
tokenizer = FSMTTokenizer.from_pretrained(mname)
model = FSMTForConditionalGeneration.from_pretrained(mname)
#Loop here for all rows in the German_Text column

input_ids = tokenizer.encode(input, return_tensors=&quot;pt&quot;)
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)
</code></pre>
<p>I want to apply this model to the following column and create a new translated column post this:</p>
<pre><code>German_Text                     English_Text
Wie geht es dir heute
mir geht es gut
</code></pre>
<p>The column English text will consist of the translated text from the model above and hence I would like to apply that model to each row in the German_text column to create corresponding translations in the English_Text column</p>
"
65517232,"An error occurs when predict with the same data as when performing train (expects 3 input(s), but it received 75 input tensors.)","<p>After training the model, I tried to make predictions, but an error occurred and I don't know how to fix it.</p>
<p>The model was constructed using electra.</p>
<p><strong>here is my model</strong></p>
<pre><code>electra = TFElectraModel.from_pretrained(&quot;monologg/koelectra-base-v3-discriminator&quot;, from_pt=True)
input_ids = tf.keras.Input(shape=(MAX_LEN,), name='input_ids', dtype=tf.int32)
mask = tf.keras.Input(shape=(MAX_LEN,), name='attention_mask', dtype=tf.int32)
token = tf.keras.Input(shape=(MAX_LEN,), name='token_type_ids', dtype=tf.int32)
embeddings = electra(input_ids, attention_mask = mask, token_type_ids= token)[0]
X = tf.keras.layers.GlobalMaxPool1D()(embeddings)
X = tf.keras.layers.BatchNormalization()(X)
X = tf.keras.layers.Dense(128, activation='relu')(X)
X = tf.keras.layers.Dropout(0.1)(X)
y = tf.keras.layers.Dense(3, activation='softmax', name='outputs')(X)
model = tf.keras.Model(inputs=[input_ids, mask, token], outputs=y)
model.layers[2].trainable=False
model.summary()
</code></pre>
<p><strong>and here is summary</strong></p>
<pre><code>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 25)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 25)]         0                                            
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 25)]         0                                            
__________________________________________________________________________________________________
tf_electra_model_4 (TFElectraMo TFBaseModelOutput(la 112330752   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
                                                                 token_type_ids[0][0]             
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 768)          0           tf_electra_model_4[3][0]         
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 768)          3072        global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 128)          98432       batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dropout_390 (Dropout)           (None, 128)          0           dense_18[0][0]                   
__________________________________________________________________________________________________
outputs (Dense)                 (None, 3)            387         dropout_390[0][0]                
==================================================================================================
Total params: 112,432,643
Trainable params: 112,431,107
Non-trainable params: 1,536
__________________________________________________________________________________________________
</code></pre>
<p><strong>This is the code to make train data set.</strong></p>
<pre><code>input_ids = []
attention_masks = []
token_type_ids = []
train_data_labels = []

for train_sent, train_label in tqdm(zip(train_data[&quot;content&quot;], train_data[&quot;label&quot;]), total=len(train_data)):
    try:
        input_id, attention_mask, token_type_id = Electra_tokenizer(train_sent, MAX_LEN)
        input_ids.append(input_id)
        attention_masks.append(attention_mask)
        token_type_ids.append(token_type_id)
        train_data_labels.append(train_label)

    except Exception as e:
        print(e)
        print(train_sent)
        pass

train_input_ids = np.array(input_ids, dtype=int)
train_attention_masks = np.array(attention_masks, dtype=int)
train_type_ids = np.array(token_type_ids, dtype=int)
intent_train_inputs = (train_input_ids, train_attention_masks, train_type_ids)
intent_train_data_labels = np.asarray(train_data_labels, dtype=np.int32)
</code></pre>
<p><strong>this is train data set shape</strong></p>
<pre><code>tf.Tensor([ 3 75 25], shape=(3,), dtype=int32)
</code></pre>
<p><strong>With this train data, the model train works fine but execute the following code to predict, an error occurs.</strong></p>
<pre><code>sample_text = 'this is sample text'
input_id, attention_mask, token_type_id = Electra_tokenizer(sample_text, MAX_LEN)
sample_text = (input_id, attention_mask, token_type_id)
model(sample_text) #or model.predict(sample_text)
</code></pre>
<p><strong>here is error</strong></p>
<pre><code>Layer model_15 expects 3 input(s), but it received 75 input tensors. Inputs received: [&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;, &lt;tf.Tensor: ....
</code></pre>
<p>It's the same shape as when i train, but why do i get an error and ask for help on how to fix it.</p>
<p>hope you have a great year ahead. Happy New Year.</p>
"
65529156,Huggingface Transformer - GPT2 resume training from saved checkpoint,"<p>Resuming the <code>GPT2</code> finetuning, implemented from <code>run_clm.py</code></p>
<p>Does GPT2 <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a>  has a parameter to resume the training from the saved checkpoint, instead training again from the beginning? Suppose the python notebook crashes while training, the checkpoints will be saved, but when I train the model again still it starts the training from the beginning.</p>
<p>Source: <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">here</a></p>
<p>finetuning code:</p>
<pre><code>!python3 run_clm.py \
    --train_file source.txt \
    --do_train \
    --output_dir gpt-finetuned \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --model_name_or_path=gpt2 \
    --save_steps 100 \
    --num_train_epochs=1 \
    --block_size=200 \
    --tokenizer_name=gpt2
</code></pre>
<p>From the above code, <code>run_clm.py</code> is a script provided by <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">huggingface</a> to finetune gpt2 to train with the customized dataset</p>
"
65557918,XLNetTokenizer requires the SentencePiece library but it was not found in your environment,"<p>I am trying to implement the XLNET on Google Collaboratory. But I get the following issue.</p>
<pre><code>ImportError: 
XLNetTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment.
</code></pre>
<p>I have also tried the following steps:</p>
<pre><code>!pip install -U transformers
!pip install sentencepiece

from transformers import XLNetTokenizer
tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased-spiece.model')
</code></pre>
<p>Thank you for your help in advance.</p>
"
65582498,torch.nn.CrossEntropyLoss().ignore_index is crashing when importing transfomers library,"<p>I am using <code>layoutlm</code> <a href=""https://github.com/microsoft/unilm/tree/master/layoutlm"" rel=""nofollow noreferrer"">github</a> which require <code>python 3.6</code>, <code>transformer 2.9.0</code>. I created an <code>conda</code> env:</p>
<pre><code>name: env_test

    channels:
    - defaults
    - conda-forge
    dependencies:
    - python=3.6
    - pip=20.3.3
    - pytorch=1.4.0
    - cudatoolkit=10.1
    - pip:
      - transformers==2.9.0
</code></pre>
<p>I have the following test.py code to reproduce the issue:</p>
<pre><code>import sys

import torch
from torch.nn import CrossEntropyLoss

from transformers import (
    BertConfig,
    __version__
)

print (sys.version)
print(torch.__version__)
print(__version__)
CrossEntropyLoss().ignore_index

print(&quot;success!&quot;)
</code></pre>
<p>Importing <code>transformers</code> library results in segmentation fault (core dumped) a when calling <code>CrossEntropyLoss().ignore_index</code>:</p>
<pre><code>$python test.py 
3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) 
[GCC 7.3.0]
1.4.0
2.9.0
Segmentation fault (core dumped)
</code></pre>
<p>I tried to investigate a bit but I don't really see from where the problem is coming from:</p>
<pre><code>gdb python
GNU gdb (Ubuntu 8.1.1-0ubuntu1) 8.1.1
Copyright (C) 2018 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;
and &quot;show warranty&quot; for details.
This GDB was configured as &quot;x86_64-linux-gnu&quot;.
Type &quot;show configuration&quot; for configuration details.
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;.
Find the GDB manual and other documentation resources online at:
&lt;http://www.gnu.org/software/gdb/documentation/&gt;.
For help, type &quot;help&quot;.
Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...
Reading symbols from python...done.
(gdb) r test.py 
Starting program: /home/jupyter/.conda-env/env_test/bin/python test.py
warning: Error disabling address space randomization: Operation not permitted
[Thread debugging using libthread_db enabled]
Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;.
3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) 
[GCC 7.3.0]
1.4.0
2.9.0

Program received signal SIGSEGV, Segmentation fault.
0x00007f97000055fb in ?? ()
(gdb) where
#0  0x00007f97000055fb in ?? ()
#1  0x00007f97f4755729 in void pybind11::cpp_function::initialize&lt;void (*&amp;)(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;), void, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, pybind11::name, pybind11::scope, pybind11::sibling&gt;(void (*&amp;)(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;), void (*)(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::_FUN(pybind11::detail::function_call&amp;) ()
   from /home/jupyter/.conda-env/env_test/lib/python3.6/site-packages/torch/lib/libtorch_python.so
#2  0x00007f97f436bca6 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /home/jupyter/.conda-env/env_test/lib/python3.6/site-packages/torch/lib/libtorch_python.so
#3  0x000055fbadd73a14 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/methodobject.c:231
#4  0x000055fbaddfba5c in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4851
#5  0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#6  0x000055fbaddf5c1b in _PyFunction_FastCall (globals=&lt;optimized out&gt;, nargs=1, args=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4933
#7  fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4968
#8  0x000055fbaddfbb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872
#9  0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#10 0x000055fbaddf5166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#11 0x000055fbaddf5e51 in fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4992
#12 0x000055fbaddfbb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872
#13 0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#14 0x000055fbaddf5166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#15 0x000055fbaddf5e51 in fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4992
#16 0x000055fbaddfbb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872
#17 0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#18 0x000055fbaddf5166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#19 0x000055fbaddf632c in _PyFunction_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:5084
#20 0x000055fbadd73ddf in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2310
#21 0x000055fbadd78873 in _PyObject_Call_Prepend () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2373
#22 0x000055fbadd7381e in PyObject_Call () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2261
#23 0x000055fbaddcc88b in slot_tp_init () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:6420
#24 0x000055fbaddfbd97 in type_call () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:915
#25 0x000055fbadd73bfb in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2331
#26 0x000055fbaddfbbae in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4875
#27 0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#28 0x000055fbaddf6969 in _PyEval_EvalCodeWithName (qualname=0x0, name=&lt;optimized out&gt;, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwstep=2, kwcount=&lt;optimized out&gt;, kwargs=0x0, kwnames=0x0, argcount=0, args=0x0, 
    locals=0x7f98035bf1f8, globals=0x7f98035bf1f8, _co=0x7f980357aae0) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#29 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4187
#30 0x000055fbaddf770c in PyEval_EvalCode (co=co@entry=0x7f980357aae0, globals=globals@entry=0x7f98035bf1f8, locals=locals@entry=0x7f98035bf1f8) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:731
#31 0x000055fbade77574 in run_mod () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:1025
#32 0x000055fbade77971 in PyRun_FileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:978
#33 0x000055fbade77b73 in PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:419
#34 0x000055fbade77c7d in PyRun_AnyFileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:81
#35 0x000055fbade7b663 in run_file (p_cf=0x7fff210dc16c, filename=0x55fbaefa6dc0 L&quot;test.py&quot;, fp=0x55fbaefda800) at /tmp/build/80754af9/python_1599604603603/work/Modules/main.c:340
#36 Py_Main () at /tmp/build/80754af9/python_1599604603603/work/Modules/main.c:811
#37 0x000055fbadd4543e in main () at /tmp/build/80754af9/python_1599604603603/work/Programs/python.c:69
#38 0x00007f9803fd6bf7 in __libc_start_main (main=0x55fbadd45350 &lt;main&gt;, argc=2, argv=0x7fff210dc378, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, stack_end=0x7fff210dc368) at ../csu/libc-start.c:310
#39 0x000055fbade24d0b in _start () at ../sysdeps/x86_64/elf/start.S:103
(gdb
</code></pre>
<p>I am the following list of packages:</p>
<pre><code>_libgcc_mutex             0.1                        main    defaults
_pytorch_select           0.2                       gpu_0    defaults
blas                      1.0                         mkl    defaults
ca-certificates           2020.12.8            h06a4308_0    defaults
certifi                   2020.12.5        py36h06a4308_0    defaults
cffi                      1.14.4           py36h261ae71_0    defaults
chardet                   4.0.0                    pypi_0    pypi
click                     7.1.2                    pypi_0    pypi
cudatoolkit               10.1.243             h6bb024c_0    defaults
cudnn                     7.6.5                cuda10.1_0    defaults
dataclasses               0.8                      pypi_0    pypi
filelock                  3.0.12                   pypi_0    pypi
idna                      2.10                     pypi_0    pypi
intel-openmp              2020.2                      254    defaults
joblib                    1.0.0                    pypi_0    pypi
ld_impl_linux-64          2.33.1               h53a641e_7    defaults
libedit                   3.1.20191231         h14c3975_1    defaults
libffi                    3.3                  he6710b0_2    defaults
libgcc-ng                 9.1.0                hdf63c60_0    defaults
libstdcxx-ng              9.1.0                hdf63c60_0    defaults
mkl                       2020.2                      256    defaults
mkl-service               2.3.0            py36he8ac12f_0    defaults
mkl_fft                   1.2.0            py36h23d657b_0    defaults
mkl_random                1.1.1            py36h0573a6f_0    defaults
ncurses                   6.2                  he6710b0_1    defaults
ninja                     1.10.2           py36hff7bd54_0    defaults
numpy                     1.19.2           py36h54aff64_0    defaults
numpy-base                1.19.2           py36hfa32c7d_0    defaults
openssl                   1.1.1i               h27cfd23_0    defaults
pip                       20.3.3           py36h06a4308_0    defaults
pycparser                 2.20                       py_2    defaults
python                    3.6.12               hcff3b4d_2    defaults
pytorch                   1.4.0           cuda101py36h02f0884_0    defaults
readline                  8.0                  h7b6447c_0    defaults
regex                     2020.11.13               pypi_0    pypi
requests                  2.25.1                   pypi_0    pypi
sacremoses                0.0.43                   pypi_0    pypi
sentencepiece             0.1.94                   pypi_0    pypi
setuptools                51.0.0           py36h06a4308_2    defaults
six                       1.15.0           py36h06a4308_0    defaults
sqlite                    3.33.0               h62c20be_0    defaults
tk                        8.6.10               hbc83047_0    defaults
tokenizers                0.7.0                    pypi_0    pypi
tqdm                      4.55.1                   pypi_0    pypi
transformers              2.9.0                    pypi_0    pypi
urllib3                   1.26.2                   pypi_0    pypi
wheel                     0.36.2             pyhd3eb1b0_0    defaults
xz                        5.2.5                h7b6447c_0    defaults
zlib                      1.2.11               h7b6447c_3    defaults
</code></pre>
<p>What is responsible fo this core dump (I have a VM with 30 GB of memory) ? Seems to be related to <code>transformers</code>. Some dependency issues not catched by <code>conda</code> ? This piece of code seems to work with the latest version of <code>transformers 4.1.1</code> but this is not compatible with <code>layoutlm</code>. Any suggestions?</p>
"
65627663,BERT - Extracting CLS embedding from multiple outputs vs single,"<p>I'm using transformers TFBertModel to classify a bunch of input strings, however I'd like to access the CLS embedding in order to be able to rebalance my data.</p>
<p>When I pass a single element of my data to the predict method of my simplified bert model (in order to get the CLS data), I take the first array of the <code>last_hidden_state</code>, and voila. However, when I pass in more than one row of data, the shape of the output changes as expected, but it seems the actual CLS embedding (of the first row that I first passed in) changes too.</p>
<p>My dataset contains the input ids and the masks, and the model:</p>
<pre><code>from transformers import TFBertModel

model = TFBertModel.from_pretrained('bert-base-multilingual-cased', trainable=False, num_labels=len(le.classes_))

input_ids_layer = Input(shape=(256,), dtype=np.int32)
input_mask_layer = Input(shape=(256,), dtype=np.int32)

bert_layer = model([input_ids_layer, input_mask_layer])

model = Model(inputs=[input_ids_layer, input_mask_layer], outputs=bert_layer)
</code></pre>
<p>Then, to get the CLS embeddings I just call the predict method and dig into the result. So for the first row of data (data_x[0] being the input ids, and data_x[1] being the masks)</p>
<p><code>output1 = model.predict([data_x[0][0], data_x[1][0]])</code></p>
<pre><code>TFBaseModelOutputWithPooling([('last_hidden_state',
                               array([[[ 0.35013607, -0.5340336 ,  0.28577858, ..., -0.03405955,
                                        -0.0165604 , -0.36481357]],
                               
                                      [[ 0.34572566, -0.5361709 ,  0.281771  , ..., -0.03687727,
                                        -0.01690093, -0.35451806]],
                               
                                      [[ 0.34878412, -0.5399749 ,  0.28948805, ..., -0.03613809,
                                        -0.01503076, -0.35425758]],
                               
                                      ...,
</code></pre>
<p>My understanding is that the CLS representation of the sentence is the first array of the last_hidden_state i.e:</p>
<pre><code>lhs1 = output1[0]

lhs1.shape
&gt;&gt; (256, 1, 768)

cls1 = lhs1[0][0]

cls1
&gt;&gt;[0.35013607 ... -0.36481357]` (as above)
</code></pre>
<p>So far so good. My confusion arises when I now want to obtain the first 2 of the CLS embeddings from my dataset:</p>
<pre><code>output_both = model.predict([data_x[0][:2], data_x[1][:2]])
lhs_both = output_both[0] # last hidden states

lhs_both.shape
&gt;&gt; (2, 256, 768)

cls_both = lhs_both[0][0] # I thought this would give me two CLS arrays including the first one above

</code></pre>
<p>Inspecting <code>cls_both</code>:</p>
<pre><code>array([[[ 0.11075249, -0.02257648, -0.40831113, ...,  0.18384863,
          0.17032738, -0.05989586],
        [-0.22926208, -0.5627498 ,  0.2617012 , ...,  0.20701236,
          0.3141808 , -0.8650396 ],
        [-0.22352833, -0.49676323, -0.5286081 , ...,  0.23819353,
          0.3742358 , -0.69018203],
        ...,
        [ 0.5120927 , -0.09863365,  0.7378716 , ..., -0.19551781,
          0.45915398,  0.22804889],
        [-0.13397002,  0.1617202 ,  0.15663634, ..., -0.511597  ,
          0.3959382 ,  0.30565232],
        [-0.14100523,  0.22792323, -0.15898004, ..., -0.2690729 ,
          0.4730471 ,  0.18431285]],

       [[-0.20033133, -0.08412935, -0.0411438 , ...,  0.34706163,
          0.1919156 , -0.08740871],
        [-0.12536147, -0.44519228,  1.2984221 , ...,  0.07149828,
          0.7915938 ,  0.08048639],
        [ 0.4596323 , -0.3316555 ,  1.2545322 , ..., -0.02128018,
          0.5344383 ,  0.32054782],
        ...,
        [-0.54777217,  0.23129587,  0.5007771 , ...,  0.70299244,
          0.27277255, -0.2848366 ],
        [-0.49410668,  0.37352908,  0.8732239 , ...,  0.6065303 ,
          0.152081  , -0.9312557 ],
        [-0.33172935, -0.35368383,  0.5942321 , ...,  0.7171531 ,
          0.24436645,  0.08909844]]], dtype=float32)
</code></pre>
<p>I'm not sure how to interpret this - my expectation was to see the first rows CLS <code>cls1</code> contained within <code>cls_both</code>, but as you can see, the first row in the first sub array is different. Can anyone explain this?</p>
<p>Furthermore, if I run <strong>only</strong> the second row through, I get exactly the same CLS token as the first, despite them containing totally different input_ids/masks:</p>
<pre><code>output2 = model.predict([data_x[0][1], data_x[1][1]])
lhs2 = output2[0]
cls2 = lhs2[0][0]


cls2
&gt;&gt;
[ 0.35013607, -0.5340336 ,  0.28577858, ..., -0.03405955,
         -0.0165604 , -0.36481357]]

cls1 == cl2 
&gt;&gt; True
</code></pre>
<h3>Edit</h3>
<p><a href=""https://stackoverflow.com/questions/59330597/bert-sentence-embeddings-how-to-obtain-sentence-embeddings-vector"">BERT sentence embeddings: how to obtain sentence embeddings vector</a></p>
<p>Above post explains that <code>output[0][:,0,:]</code> is the correct way to obtain exactly the CLS tokens which makes things easiers.</p>
<p>When I run three rows through, I get consistent results, but any time I run a single row through, I get the result shown in <code>cls1</code> - why does this not differ each time?</p>
"
65674086,is it possible to use an external vectorizer in a standard SpaCy pipeline?,"<p>I have a script that relies almost entirely on SpaCy for a series of nlp tasks.
Since SpaCy only supports 3 english models by default (sm, md, lg), i would like to replace them with an external model such that i can vectorize my text and perform all the SpaCy methods i currently do in my pipeline.</p>
<p>Is it possible to replace the <code>nlp = spacy.load('en_core_web_lg')</code>
line with something else without affecting the rest of my pipeline? For example by defining 'nlp' with one of the language models supported in the transformers library?</p>
<p>For example, i use SpaCy's <code>(a).similarity(b)</code> method, and would like to retain the pipeline which includes this, except have the calculations based on the word vectors generated by a non-default language model.</p>
"
65676389,Huggingface TFBertForSequenceClassification always predicts the same label,"<p>TL;DR:
My model always predicts the same labels and I don't know why. Below is my entire code for fine-tuning in the hopes that someone can point out to me where I am going wrong.</p>
<p>I am using Huggingface's TFBertForSequenceClassification for sequence classification task to predict 4 labels of sentences in German text.</p>
<p>I use the bert-base-german-cased model since I don't use only lower case text (since German is more case sensitive than English).</p>
<p>I get my input from a csv file that I construct from an annotated corpus I received. Here's a sample of that:</p>
<pre><code>0       Hier kommen wir ins Spiel Die App Cognitive At...
1       Doch wenn Athlet Lebron James jede einzelne Mu...
2       Wie kann ein Gehirn auf Hochleistung getrimmt ...
3       Wie schafft es Warren Buffett knapp 1000 WÃ¶rte...
4       Entfalte dein mentales Potenzial und werde ein...
Name: sentence_clean, Length: 3094, dtype: object
</code></pre>
<p>And those are my labels, from the same csv file:</p>
<pre><code>0       e_1
1       e_4
2       e_4
3       e_4
4       e_4
</code></pre>
<p>The distinct labels are: e_1, e_2, e_3, and e_4</p>
<p>This is the code I am using to fine tune my model:</p>
<pre><code>import pandas as pd
import numpy as np
import os
    
# read in data
# sentences_df = pd.read_csv('path/file.csv')


X = sentences_df.sentence_clean
Y = sentences_df.classId

# =============================================================================
# One hot encode labels
# =============================================================================

# integer encode labels
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()
Y_integer_encoded = label_encoder.fit_transform(list(Y))


# one hot encode labels
from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder(sparse=False)
Y_integer_encoded_reshaped = Y_integer_encoded.reshape(len(Y_integer_encoded), 1)
Y_one_hot_encoded = onehot_encoder.fit_transform(Y_integer_encoded_reshaped)

# train test split
from sklearn.model_selection import train_test_split


X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, Y_one_hot_encoded, test_size=0.20, random_state=42)


# =============================================================================
# Perpare datasets for finetuning
# =============================================================================
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU') 
tf.config.experimental.set_memory_growth(physical_devices[0], True)

from transformers import BertTokenizer, TFBertForSequenceClassification


tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased') # initialize tokenizer


# tokenize trai and test sets
max_seq_length = 128

X_train_tokens = tokenizer(list(X_train_raw),
                            truncation=True,
                            padding=True)

X_test_tokens = tokenizer(list(X_test_raw),
                            truncation=True,
                            padding=True)


# create TF datasets as input for BERT model
bert_train_ds = tf.data.Dataset.from_tensor_slices((
    dict(X_train_tokens),
    y_train
))

bert_test_ds = tf.data.Dataset.from_tensor_slices((
    dict(X_test_tokens),
    y_test
))

# =============================================================================
# setup model and finetune
# =============================================================================

# define hyperparams
num_labels = 4
learninge_rate = 2e-5
epochs = 3
batch_size = 16

# create BERT model
bert_categorical_partial = TFBertForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=num_labels)

optimizer = tf.keras.optimizers.Adam(learning_rate=learninge_rate)
bert_categorical_partial.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

history = bert_categorical_partial.fit(bert_train_ds.shuffle(100).batch(batch_size),
          epochs=epochs,
          # batch_size=batch_size,
          validation_data=bert_test_ds.shuffle(100).batch(batch_size))
</code></pre>
<p>And here is the output from fine-tuning:</p>
<pre><code>Epoch 1/3
155/155 [==============================] - 31s 198ms/step - loss: 8.3038 - accuracy: 0.2990 - val_loss: 8.7751 - val_accuracy: 0.2811
Epoch 2/3
155/155 [==============================] - 30s 196ms/step - loss: 8.2451 - accuracy: 0.2913 - val_loss: 8.9314 - val_accuracy: 0.2779
Epoch 3/3
155/155 [==============================] - 30s 196ms/step - loss: 8.3101 - accuracy: 0.2913 - val_loss: 9.0355 - val_accuracy: 0.2746
</code></pre>
<p>Lastly, I try to predict the labels of the test set and validate the results with a confusion matrix:</p>
<pre><code>X_test_tokens_new = {'input_ids': np.asarray(X_test_tokens['input_ids']),
                     'token_type_ids': np.asarray(X_test_tokens['token_type_ids']),
                     'attention_mask': np.asarray(X_test_tokens['attention_mask']),
                     }

pred_raw = bert_categorical_partial.predict(X_test_tokens_new)
pred_proba = tf.nn.softmax(pred_raw).numpy()
pred = pred_proba[0].argmax(axis = 1)
y_true = y_test.argmax(axis = 1)

cm = confusion_matrix(y_true, pred)
</code></pre>
<p>Output of print(cm):</p>
<pre><code>array([[  0,   0,   0,  41],
       [  2,   0,   0, 253],
       [  2,   0,   0, 219],
       [  6,   0,   0,  96]], dtype=int64)
</code></pre>
<p>As you can see, my accuracy is really bad, and when I look at the cm, I can see that my model pretty much just predicts one single label.
I've tried everything and ran the model multiple times, but I always get the same results.
I do know that the data I am working with isn't great and I am only training on abour 2k sentences with labels. But I have a feeling the accuracy should still be higher and, more importantly, the model shouldn't just predict one single label 98% of the time, right?</p>
<p>I posted everything I am using to run the model in the hopes someone can point me to where I am going wrong.
Thank very much in advance for your help!</p>
"
65683013,IndexError: index out of range in self while try to fine tune Roberta model after adding special tokens,"<p>I am trying to fine tune a Roberta model after adding some special tokens to its tokenizer:</p>
<pre><code>    special_tokens_dict = {'additional_special_tokens': ['[Tok1]','[Tok2]']}

    tokenizer.add_special_tokens(special_tokens_dict)
</code></pre>
<p>I get this error when i try to train the model (on cpu):</p>
<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-75-d63f8d3c6c67&gt; in &lt;module&gt;()
     50         l = model(b_input_ids, 
     51                      attention_mask=b_input_mask,
---&gt; 52                     labels=b_labels)
     53         loss,logits = l
     54         total_train_loss += l[0].item()

8 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1850         # remove once script supports set_grad_enabled
   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1853 
   1854 

IndexError: index out of range in self
</code></pre>
<p>p.s. If I comment <code>add_special_tokens</code> the code works.</p>
"
65806586,"ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (10, 30))","<p>i'm fairly new to tensorflow and would appreciate answers a lot.
i'm trying to use a transformer model as an embedding layer and feed the data to a custom model.</p>
<pre><code>from transformers import TFAutoModel
from tensorflow.keras import layers
def build_model():
    transformer_model = TFAutoModel.from_pretrained(MODEL_NAME, config=config)
    
    input_ids_in = layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')
    input_masks_in = layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')

    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]

    X = layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)
    X = layers.GlobalMaxPool1D()(X)
    X = layers.Dense(64, activation='relu')(X)
    X = layers.Dropout(0.2)(X)
    X = layers.Dense(30, activation='softmax')(X)

    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

    for layer in model.layers[:3]:
        layer.trainable = False

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

    
model = build_model()
model.summary()
r = model.fit(
            train_ds,
            steps_per_epoch=train_steps,
            epochs=EPOCHS,
            verbose=3)
</code></pre>
<p>I have 30 classes and the labels are not one-hot encoded so im using sparse_categorical_crossentropy as my loss function but i keep getting the following error</p>
<pre><code>ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (10, 30)).
</code></pre>
<p>how can i solve this?
and why is the (10, 30) shape required? i know 30 is because of the last Dense layer with 30 units but why the 10? is it because of the MAX_LENGTH which is 10?</p>
<p>my model summary:</p>
<pre><code>Model: &quot;model_16&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 10)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 10)]         0                                            
__________________________________________________________________________________________________
tf_bert_model_21 (TFBertModel)  TFBaseModelOutputWit 162841344   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
__________________________________________________________________________________________________
bidirectional_17 (Bidirectional (None, 10, 100)      327600      tf_bert_model_21[0][0]           
__________________________________________________________________________________________________
global_max_pooling1d_15 (Global (None, 100)          0           bidirectional_17[0][0]           
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 64)           6464        global_max_pooling1d_15[0][0]    
__________________________________________________________________________________________________
dropout_867 (Dropout)           (None, 64)           0           dense_32[0][0]                   
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 30)           1950        dropout_867[0][0]                
==================================================================================================
Total params: 163,177,358
Trainable params: 336,014
Non-trainable params: 162,841,344
</code></pre>
"
65852264,Using XLNet for sentiment analysis - setting the correct reshape parameters,"<p>Following  <a href=""https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85"" rel=""nofollow noreferrer"">this link</a>, I am trying to use my own data to do sentiment analysis. But I get this error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

&lt;ipython-input-41-5f2f35b7976e&gt; in train_epoch(model, data_loader, optimizer, device, scheduler, n_examples)
      7 
      8     for d in data_loader:
----&gt; 9         input_ids = d[&quot;input_ids&quot;].reshape(4,64).to(device)
     10         attention_mask = d[&quot;attention_mask&quot;].to(device)
     11         targets = d[&quot;targets&quot;].to(device)

RuntimeError: shape '[4, 64]' is invalid for input of size 64
</code></pre>
<p>When I try to run this code</p>
<pre><code>history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,     
        optimizer, 
        device, 
        scheduler, 
        len(df_train)
    )

    print(f'Train loss {train_loss} Train accuracy {train_acc}')

    val_acc, val_loss = eval_model(
        model,
        val_data_loader, 
        device, 
        len(df_val)
    )

    print(f'Val loss {val_loss} Val accuracy {val_acc}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)
</code></pre>
<p>I know this error has something to do with the shape of my data but I am not sure how to find the correct <code>reshape</code> parameters in order to make this work.</p>
"
65854722,Huggingface AlBert tokenizer NoneType error with Colab,"<p>I simply tried the sample code from hugging face website: <a href=""https://huggingface.co/albert-base-v2"" rel=""noreferrer"">https://huggingface.co/albert-base-v2</a></p>
<pre><code>`from transformers import AlbertTokenizer, AlbertModel` 
`tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')`
`text = &quot;Replace me by any text you'd like.&quot;`
`encoded_input = tokenizer(text, return_tensors='pt')`
</code></pre>
<p>then I got the following error at the tokenizer step:
----&gt; 5 encoded_input = tokenizer(text, return_tensors='pt')</p>
<p>TypeError: 'NoneType' object is not callable</p>
<p>I tried the same code on my local machine, it worked no problem. The problem seems within Colab. However, I do need help to run this model on colab GPU.</p>
<p>My python version on colab is Python 3.6.9</p>
"
65872566,Using tensorflow and TFBertForNextSentencePrediction to further train bert on a specific corpus,"<p>I'm trying to train <code>TFBertForNextSentencePrediction</code> on my own corpus, not from scratch, but rather taking the existing bert model with only a next sentence prediction head and further train it on a specific cuprous of text (pairs of sentences). Then I want to use the model I trained to be able to extract sentence embeddings from the last hidden state for other texts.</p>
<p>Currently the problem I encounter is that after I train the <code>keras</code> model I am not able to extract the hidden states of the last layer before the next sentence prediction head.</p>
<p>Below is the code. Here I only train it on a few sentences just to make sure the code works.
Any help will be greatly appreciated.</p>
<p>Thanks,
Ayala</p>
<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf
from datetime import datetime
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.callbacks import ModelCheckpoint
from transformers import BertTokenizer, PreTrainedTokenizer, BertConfig, TFBertForNextSentencePrediction
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score


PRETRAINED_MODEL = 'bert-base-uncased'

# set paths and file names
time_stamp = str(datetime.now().year) + &quot;_&quot; + str(datetime.now().month) + &quot;_&quot; + str(datetime.now().day) + &quot;_&quot; + \
                     str(datetime.now().hour) + &quot;_&quot; + str(datetime.now().minute)
model_name = &quot;pretrained_nsp_model&quot;
model_dir_data = model_name + &quot;_&quot; + time_stamp
model_fn = model_dir_data + &quot;.h5&quot;
base_path = os.path.dirname(__file__)
input_path = os.path.join(base_path, &quot;input_data&quot;)
output_path = os.path.join(base_path, &quot;output_models&quot;)
model_path = os.path.join(output_path, model_dir_data)
if not os.path.exists(model_path):
    os.makedirs(model_path)

# set model checkpoint
checkpoint = ModelCheckpoint(os.path.join(model_path, model_fn), monitor=&quot;val_loss&quot;, verbose=1, save_best_only=True,
                             save_weights_only=True, mode=&quot;min&quot;)

# read data
max_length = 512

def get_tokenizer(pretrained_model_name):
    tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)
    return tokenizer

def tokenize_nsp_data(A, B, max_length):
    data_inputs = tokenizer(A, B, add_special_tokens=True, max_length=max_length, truncation=True,
                             pad_to_max_length=True, return_attention_mask=True,
                             return_tensors=&quot;tf&quot;)
    return data_inputs

def get_data_features(data_inputs, max_length):
    data_features = {}
    for key in data_inputs:
        data_features[key] = sequence.pad_sequences(data_inputs[key], maxlen=max_length, truncating=&quot;post&quot;,
                                                          padding=&quot;post&quot;, value=0)
    return data_features

def get_transformer_model(transformer_model_name):
    # get transformer model
    config = BertConfig(output_attentions=True)
    config.output_hidden_states = True
    config.return_dict = True
    transformer_model = TFBertForNextSentencePrediction.from_pretrained(transformer_model_name, config=config)
    return transformer_model

def get_keras_model(transformer_model):
    # get keras model
    input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')
    input_masks_ids = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')
    token_type_ids = tf.keras.layers.Input(shape=(max_length,), name='token_type_ids', dtype='int32')
    X = transformer_model({'input_ids': input_ids, 'attention_mask': input_masks_ids, 'token_type_ids': token_type_ids})[0]
    model = tf.keras.Model(inputs=[input_ids, input_masks_ids, token_type_ids], outputs=X)
    model.summary()
    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  optimizer=tf.optimizers.Adam(learning_rate=0.00005), metrics=['accuracy'])
    return model

def get_metrices(true_values, pred_values):
    cm = confusion_matrix(true_values, pred_values)
    acc_score = accuracy_score(true_values, pred_values)
    f1 = f1_score(true_values, pred_values, average=&quot;binary&quot;)
    precision = precision_score(true_values, pred_values, average=&quot;binary&quot;)
    recall = recall_score(true_values, pred_values, average=&quot;binary&quot;)
    metrices = {'confusion_matrix': cm,
                'acc_score': acc_score,
                'f1': f1,
                'precision': precision,
                'recall': recall
                }
    for k, v in metrices.items():
        print(k, ':\n', v)
    return metrices

# get tokenizer
tokenizer = get_tokenizer(PRETRAINED_MODEL)

# train 
prompt = [&quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;]
next_sentence = [&quot;How are you?&quot;, &quot;Pizza&quot;, &quot;How are you?&quot;, &quot;Pizza&quot;]
train_labels = [0, 1, 0, 1]
train_labels = to_categorical(train_labels)
train_inputs = tokenize_nsp_data(prompt, next_sentence, max_length)
train_data_features = get_data_features(train_inputs, max_length)

# val
prompt = [&quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;]
next_sentence = [&quot;How are you?&quot;, &quot;Pizza&quot;, &quot;How are you?&quot;, &quot;Pizza&quot;]
val_labels = [0, 1, 0, 1]
val_labels = to_categorical(val_labels)
val_inputs = tokenize_nsp_data(prompt, next_sentence, max_length)
val_data_features = get_data_features(val_inputs, max_length)

# get transformer model
transformer_model = get_transformer_model(PRETRAINED_MODEL)

# get keras model
model = get_keras_model(transformer_model)

callback_list = []
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, min_delta=0.005, verbose=1)
callback_list.append(early_stop)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, epsilon=0.001)
callback_list.append(reduce_lr)
callback_list.append(checkpoint)

history = model.fit([train_data_features['input_ids'], train_data_features['attention_mask'],
                     train_data_features['token_type_ids']], np.array(train_labels), batch_size=2, epochs=3,
                    validation_data=([val_data_features['input_ids'], val_data_features['attention_mask'],
                                      val_data_features['token_type_ids']], np.array(val_labels)), verbose=1,
                    callbacks=callback_list)

model.layers[3].save_pretrained(model_path)  # need to save this and make sure i can get the hidden states

##  predict
# load model
transformer_model = get_transformer_model(model_path)
model = get_keras_model(transformer_model)
model.summary()
model.load_weights(os.path.join(model_path, model_fn))


# test
prompt = [&quot;Hello&quot;, &quot;Hello&quot;]
next_sentence = [&quot;How are you?&quot;, &quot;Pizza&quot;]
test_labels = [0, 1]
test_df = pd.DataFrame({'A': prompt, 'B': next_sentence, 'label': test_labels})
test_labels = to_categorical(val_labels)
test_inputs = tokenize_nsp_data(prompt, next_sentence, max_length)
test_data_features = get_data_features(test_inputs, max_length)

# predict
pred_test = model.predict([test_data_features['input_ids'], test_data_features['attention_mask'], test_data_features['token_type_ids']])
preds = tf.keras.activations.softmax(tf.convert_to_tensor(pred_test)).numpy()

true_test = test_df['label'].to_list()
pred_test = [1 if p[1] &gt; 0.5 else 0 for p in preds]
test_df['pred_val'] = pred_test

metrices = get_metrices(true_test, pred_test)


</code></pre>
<p>I am also attaching a picture from the debugging mode in which I try (with no success) to view the hidden state. <strong>The problem is I am not able to see and save the transform model I trained and view the embeddings of the last hidden state.</strong> I tried converting the <code>KerasTensor</code> to <code>numpy array</code> but without success.</p>
<p><a href=""https://i.stack.imgur.com/inP4a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/inP4a.png"" alt=""enter image description here"" /></a></p>
"
65881820,HuggingFace Bert Sentiment analysis,"<p>I am getting the following error :</p>
<p><code>AssertionError: text input must of type str (single example), List[str] (batch or single pretokenized example) or List[List[str]] (batch of pretokenized examples).</code>, when I run <code>classifier(encoded)</code>. My text type is <code>str</code> so I am not sure what I am doing wrong. Any help is very appreciated.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, BertTokenizer, BertModel, BertForMaskedLM, AutoModelForSequenceClassification, pipeline

# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows
import logging
logging.basicConfig(level=logging.INFO)

# Load pre-trained model tokenizer (vocabulary)
# used the cased instead of uncased to account for cases like BAD.
tokenizer = BertTokenizer.from_pretrained('bert-base-cased') 


# alternative? what is the difference between these two tokenizers? 
#tokenizer = AutoTokenizer.from_pretrained(&quot;textattack/bert-base-uncased-SST-2&quot;)

model = AutoModelForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-SST-2&quot;)


# feed the model and the tokenizer into the pipeline
classifier = pipeline('sentiment-analysis', model=model, tokenizer= tokenizer)


#---------------sample raw input passage--------

text = &quot;Who was Jim Henson ? Jim Henson was a puppeteer. He is simply awful.&quot;
# tokenized_text = tokenizer.tokenize(text)

#----------Tokenization and Padding---------
# Encode the sentences to get tokenized and add padding stuff
encoded = tokenizer.encode_plus(
    text=text,  # the sentences to be encoded
    add_special_tokens=True,  # Add [CLS] and [SEP] !!!
    max_length = 64,  # maximum length of a sentence  (TODO Figure the longest passage length)
    pad_to_max_length=True,  # Add [PAD]s
    return_attention_mask = True,  # Generate the attention mask
    truncation=True,  #explicitly truncate examples to max length
    return_tensors = 'pt',  # ask the function to return PyTorch tensors
)

#-------------------------------------------
# view the IDs
for key, value in encoded.items():
    print(f&quot;{key}: {value.numpy().tolist()}&quot;)
    
#-------------------------------------------


classifier(encoded)

</code></pre>
"
65925640,Assigning weights during testing the bert model,"<p>I have a basic conceptual doubt. When i train a bert model on sentence say:</p>
<pre><code>Train: &quot;went to get loan from bank&quot; 
Test :&quot;received education loan from bank&quot;
</code></pre>
<p>How does the test sentence assigns the weights for each token because i however dont pass exact sentence for testing and there is a slight addition  of words like &quot;education&quot; which change the context slightly</p>
<p>Assuming such context is not trained in my model how the weights are assigned for each token in my bert before i fine tune further</p>
<p>If i confuse with my question, simply put i am trying to understand how the weights get assigned during testing if a slight variation in context occurs that was not trained on.</p>
"
65987683,Modifying the Learning Rate in the middle of the Model Training in Deep Learning,"<p>Below is the code to configure <a href=""https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"" rel=""nofollow noreferrer"">TrainingArguments</a> consumed from the <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">HuggingFace transformers</a> library to finetune the <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">GPT2</a> language model.</p>
<pre><code>training_args = TrainingArguments(
        output_dir=&quot;./gpt2-language-model&quot;, #The output directory
        num_train_epochs=100, # number of training epochs
        per_device_train_batch_size=8, # batch size for training #32, 10
        per_device_eval_batch_size=8,  # batch size for evaluation #64, 10
        save_steps=100, # after # steps model is saved
        warmup_steps=500,# number of warmup steps for learning rate scheduler
        prediction_loss_only=True,
        metric_for_best_model = &quot;eval_loss&quot;,
        load_best_model_at_end = True,
        evaluation_strategy=&quot;epoch&quot;,
        learning_rate=0.00004, # learning rate
    )

early_stop_callback = EarlyStoppingCallback(early_stopping_patience  = 3)
    
trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        callbacks = [early_stop_callback],
 )
</code></pre>
<p>The <strong>number of epochs</strong> as <strong>100</strong> and <strong>learning_rate</strong> as <strong>0.00004</strong> and also the <strong>early_stopping</strong> is configured with the patience value as <strong>3</strong>.</p>
<p>The model ran for <strong>5/100</strong> epochs and noticed that the difference in loss_value is negligible. The latest checkpoint is saved as <code>checkpoint-latest</code>.</p>
<p>Now Can I modify the <code>learning_rate</code> may be to <code>0.01</code> from <code>0.00004</code> and resume the training from the latest saved checkpoint - <code>checkpoint-latest</code>? Doing that will be efficient?</p>
<p>Or to train with the new <code>learning_rate</code> value should I start the <strong>training</strong> from the beginning?</p>
"
66015068,Is the pretrained model selected at random when not specified from transformers,"<p>I am trying to implement a QA system using models from huggingface. One thing I do not understand is, when I don't specify which pre-trained model I am using for question-answering, is the model chosen at random?</p>
<pre><code>from transformers import pipeline

# Allocate a pipeline for question-answering

question_answerer = pipeline('question-answering')
question_answerer({

     'question': 'What is the name of the repository ?',
     'context': 'Pipeline have been included in the huggingface/transformers repository'

})
</code></pre>
<p>Output:</p>
<p><code>{'score': 0.5135612454720828, 'start': 35, 'end': 59, 'answer': 'huggingface/transformers'}</code></p>
<p>I know how to specify a model by adding the name of the model (bert-base-uncased for example) as a model parameter, but which one is it using when you are not specifying anything? Does it use a combination of all models on huggingface? I could not find the answer.</p>
"
66064503,In HuggingFace tokenizers: how can I split a sequence simply on spaces?,"<p>I am using <code>DistilBertTokenizer</code> tokenizer from <a href=""https://huggingface.co"" rel=""nofollow noreferrer"">HuggingFace</a>.</p>
<p>I would like to tokenize my text by simple splitting it on space:</p>
<pre><code>[&quot;Don't&quot;, &quot;you&quot;, &quot;love&quot;, &quot;ðŸ¤—&quot;, &quot;Transformers?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;do.&quot;]
</code></pre>
<p>instead of the default behavior, which is like this:</p>
<pre><code>[&quot;Do&quot;, &quot;n't&quot;, &quot;you&quot;, &quot;love&quot;, &quot;ðŸ¤—&quot;, &quot;Transformers&quot;, &quot;?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;do&quot;, &quot;.&quot;]
</code></pre>
<p>I read their documentation about <a href=""https://huggingface.co/transformers/tokenizer_summary.html"" rel=""nofollow noreferrer"">Tokenization</a> in general as well as about <a href=""https://huggingface.co/transformers/model_doc/bert.html#transformers.BertTokenizer"" rel=""nofollow noreferrer"">BERT Tokenizer</a> specifically, but could not find an answer to this simple question :(</p>
<p>I assume that it should be a parameter when loading Tokenizer, but I could not find it among the parameters list ...</p>
<p>EDIT:
Minimal code example to reproduce:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('distilbert-base-cased')

tokens = tokenizer.tokenize(&quot;Don't you love ðŸ¤— Transformers? We sure do.&quot;)
print(&quot;Tokens: &quot;, tokens)
</code></pre>
"
66073395,BERT Convert 'SpanAnnotation' to answers using scores from hugging face models,"<p>I'm following along with the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering"" rel=""nofollow noreferrer"">documentation</a> for importing a pretrained model question and answer model from huggingface</p>
<pre><code>from transformers import BertTokenizer, BertForQuestionAnswering
import torch
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
question, text = &quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;
inputs = tokenizer(question, text, return_tensors='pt')
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])
outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits
</code></pre>
<p>this returns start and end scores, but how can I get a meaningful text answer from here?</p>
"
66096703,Running huggingface Bert tokenizer on GPU,"<p>I'm dealing with a huge text dataset for content classification. I've implemented the distilbert model and distilberttokenizer.from_pretrained() tokenizer..
This tokenizer is taking incredibly long to tokenizer my text data roughly 7 mins for just 14k records and that's because it runs on my CPU.</p>
<p>Is there any way to force the tokenizer to run on my GPU.</p>
"
66109084,how to convert HuggingFace's Seq2seq models to onnx format,"<p>I am trying to convert the Pegasus newsroom in HuggingFace's transformers model to the ONNX format. I followed <a href=""https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/04-onnx-export.ipynb#scrollTo=foYlXrSksR_v"" rel=""nofollow noreferrer"">this</a> guide published by Huggingface. After installing the prereqs, I ran this code:</p>
<pre><code>!rm -rf onnx/
from pathlib import Path
from transformers.convert_graph_to_onnx import convert

convert(framework=&quot;pt&quot;, model=&quot;google/pegasus-newsroom&quot;, output=Path(&quot;onnx/google/pegasus-newsroom.onnx&quot;), opset=11)

</code></pre>
<p>and got these errors:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-3b37ed1ceda5&gt; in &lt;module&gt;()
      3 from transformers.convert_graph_to_onnx import convert
      4 
----&gt; 5 convert(framework=&quot;pt&quot;, model=&quot;google/pegasus-newsroom&quot;, output=Path(&quot;onnx/google/pegasus-newsroom.onnx&quot;), opset=11)
      6 
      7 

6 frames
/usr/local/lib/python3.6/dist-packages/transformers/models/pegasus/modeling_pegasus.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, encoder_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)
    938             input_shape = inputs_embeds.size()[:-1]
    939         else:
--&gt; 940             raise ValueError(&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;)
    941 
    942         # past_key_values_length

ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

</code></pre>
<p>I have never seen this error before. Any ideas?</p>
"
66183163,Does converting a seq2seq NLP model to the ONNX format negatively affect its performance?,"<p>I was looking at potentially converting an ml NLP model to the ONNX format in order to take advantage of its speed increase (ONNX Runtime). However, I don't really understand what is fundamentally changed in the new models compared to the old models. Also, I don't know if there are any drawbacks. Any thoughts on this would be very appreciated.</p>
"
66207138,Errors appear when training an XLNET model,"<p>I am trying to train an XLNET model as the following. I want to set the hyperparameters by myself without using any pretrained models.</p>
<pre><code>from transformers import XLNetConfig, XLNetModel
from transformers import Trainer, TrainingArguments
# Initializing an XLNet configuration
configuration = XLNetConfig(use_mems_train = True)
model = XLNetModel(configuration)
train_dataset = 'sentences.txt'
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)
trainer = Trainer(
    model=model,                         # the instantiated Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
)
trainer.train()
</code></pre>
<p>However, the following errors appear:</p>
<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])
  0%|          | 0/9 [00:00&lt;?, ?it/s]Traceback (most recent call last):
  File &quot;untitled1/dfgd.py&quot;, line 23, in &lt;module&gt;
    trainer.train()
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\trainer.py&quot;, line 925, in train
    for step, inputs in enumerate(epoch_iterator):
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\dataloader.py&quot;, line 435, in __next__
    data = self._next_data()
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\dataloader.py&quot;, line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\data\data_collator.py&quot;, line 52, in default_data_collator
    features = [vars(f) for f in features]
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\data\data_collator.py&quot;, line 52, in &lt;listcomp&gt;
    features = [vars(f) for f in features]
TypeError: vars() argument must have __dict__ attribute
Exception ignored in: &lt;function tqdm.__del__ at 0x0000014A17ABE828&gt;
Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 1039, in __del__
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 1223, in close
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 555, in _decr_instances
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\_monitor.py&quot;, line 51, in exit
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 522, in set
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 365, in notify_all
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 348, in notify
TypeError: 'NoneType' object is not callable
</code></pre>
<p>How should I handle these errors? How can I train my XLNET model?</p>
"
66276186,HuggingFace - GPT2 Tokenizer configuration in config.json,"<p>The GPT2 finetuned model is uploaded in <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">huggingface-models</a> for the inferencing</p>
<p>Below error is observed during the inference,</p>
<p><strong>Can't load tokenizer using from_pretrained, please update its configuration: Can't load tokenizer for 'bala1802/model_1_test'. Make sure that: - 'bala1802/model_1_test' is a correct model identifier listed on 'https://huggingface.co/models' - or 'bala1802/model_1_test' is the correct path to a directory containing relevant tokenizer files</strong></p>
<p>Below is the configuration - config.json file for the Finetuned huggingface model,</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;gpt2&quot;,
  &quot;activation_function&quot;: &quot;gelu_new&quot;,
  &quot;architectures&quot;: [
    &quot;GPT2LMHeadModel&quot;
  ],
  &quot;attn_pdrop&quot;: 0.1,
  &quot;bos_token_id&quot;: 50256,
  &quot;embd_pdrop&quot;: 0.1,
  &quot;eos_token_id&quot;: 50256,
  &quot;gradient_checkpointing&quot;: false,
  &quot;initializer_range&quot;: 0.02,
  &quot;layer_norm_epsilon&quot;: 1e-05,
  &quot;model_type&quot;: &quot;gpt2&quot;,
  &quot;n_ctx&quot;: 1024,
  &quot;n_embd&quot;: 768,
  &quot;n_head&quot;: 12,
  &quot;n_inner&quot;: null,
  &quot;n_layer&quot;: 12,
  &quot;n_positions&quot;: 1024,
  &quot;resid_pdrop&quot;: 0.1,
  &quot;summary_activation&quot;: null,
  &quot;summary_first_dropout&quot;: 0.1,
  &quot;summary_proj_to_labels&quot;: true,
  &quot;summary_type&quot;: &quot;cls_index&quot;,
  &quot;summary_use_proj&quot;: true,
  &quot;task_specific_params&quot;: {
    &quot;text-generation&quot;: {
      &quot;do_sample&quot;: true,
      &quot;max_length&quot;: 50
    }
  },
  &quot;transformers_version&quot;: &quot;4.3.2&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50257
}
</code></pre>
<p>Should I configure the GPT2 Tokenizer just like the <code>&quot;model_type&quot;: &quot;gpt2&quot;</code> in the config.json file</p>
"
66287735,HuggingFace Training using GPU,"<p>Based on <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">HuggingFace script</a> to train a transformers model from scratch.
I run:</p>
<pre><code>python3 run_mlm.py \
--dataset_name wikipedia \
--tokenizer_name roberta-base \
--model_type roberta \
--dataset_config_name 20200501.en \
--do_train \
--do_eval \
--learning_rate 1e-5 \
--num_train_epochs 5 \
--save_steps 5000 \
--warmup_steps=10000 \ 
--seed 666 \
--gradient_accumulation_steps=4 \ 
--output_dir models/mlm_wikipedia_scratch/ \
--per_gpu_train_batch_size 8
</code></pre>
<p>I don't understand why I can't see my python3 process on GPU running <code>nvidia-smi</code>
Here a screen:
<a href=""https://i.stack.imgur.com/9JkYX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JkYX.png"" alt=""top | nvidia-smi | training_script "" /></a></p>
"
66315926,Split a sentence by words just as BERT Tokenizer would do?,"<p>I'm trying to localize all the [UNK] tokens of BERT tokenizer on my text. Once I have the position of the UNK token, I need to identify what word it belongs to. For that, I tried to get the position of the word using words_ids() or token_to_words() methods (the result is the same, I think) which give me the id word of this token.</p>
<p>The problem is, for a large text, there are many ways to split the text by words, and the ways I tried don't match with the position I get from token_to_words method. How I can split my text in the same way Bert tokenizer do?</p>
<p>I saw BERT use WordPiece for tokenize in sub-words, but nothing for complete words.</p>
<p>I'm at this point:</p>
<pre><code>  tokenized_text = tokenizer.tokenize(texto) # Tokens
  encoding_text = tokenizer(texto) # Esto es de tipo batchEncoding, como una instancia del tokenizer
  tpos = [i for i, element in enumerate(tokenized_text) if element == &quot;[UNK]&quot;]  # Posicion en la lista de tokens

  word_list = texto.split(&quot; &quot;)
  for x in tpos:
    wpos = encoding_text.token_to_word(x) # Posicion en la lista de palabras
    print(&quot;La palabra:  &quot;, word_list[wpos], &quot;    contiene un token desconocido: &quot;, tokenizer.tokenize(word_list[wpos]))
</code></pre>
<p>but it fails because the index &quot;wpos&quot; doesn't fit properly with my word_list.</p>
"
66372741,Applying pre trained facebook/bart-large-cnn for text summarization in python,"<p>I am in a situation where I am working with huggingface transformers and have got some insights into it. I am working with the facebook/bart-large-cnn model to perform text summarisation for my project and I am using the following code as of now to do some tests:</p>
<pre><code>text = &quot;&quot;&quot;
Justin Timberlake and Jessica Biel, welcome to parenthood. 
The celebrity couple announced the arrival of their son, Silas Randall Timberlake, in 
statements to People.&quot;&quot;&quot;

from transformers import pipeline
smr_bart = pipeline(task=&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)
smbart = smr_bart(text, max_length=150)
print(smbart[0]['summary_text'])
</code></pre>
<p>The small peice of code is actually giving me a very good summary of the text. But my ask is that how can I apply the same pre trained model on top of my dataframe column. My dataframe looks like this:</p>
<pre><code>ID        Lang          Text
1         EN            some long text here...
2         EN            some long text here...
3         EN            some long text here...
</code></pre>
<p>.... and so on for 50K rows</p>
<p>Now I want to apply the pre trained model to the col Text to generate a new column df['summary'] from it and the resultant dataframe should look like:</p>
<pre><code>ID        Lang         Text                              Summary
1         EN            some long text here...           Text summary goes here...
2         EN            some long text here...           Text summary goes here...
3         EN            some long text here...           Text summary goes here...
</code></pre>
<p>How can I achieve this? Any help would be much appreciated.</p>
"
66524542,AttributeError: 'str' object has no attribute 'shape' while encoding tensor using BertModel with PyTorch (Hugging Face),"<p><strong>AttributeError: 'str' object has no attribute 'shape' while encoding tensor using BertModel with PyTorch (Hugging Face). Below is the code</strong></p>
<pre><code>bert_model = BertModel.from_pretrained(r'downloads\bert-pretrained-model')
input_ids
</code></pre>
<p>Output is:</p>
<pre><code>tensor([[  101,   156, 13329,  ...,     0,     0,     0],
        [  101,   156, 13329,  ...,     0,     0,     0],
        [  101,  1302,  1251,  ...,     0,     0,     0],
        ...,
        [  101, 25456,  1200,  ...,     0,     0,     0],
        [  101,   143,  9664,  ...,     0,     0,     0],
        [  101,  2586,  7340,  ...,     0,     0,     0]])
</code></pre>
<p>Followed by code below</p>
<pre><code>last_hidden_state, pooled_output = bert_model(
  input_ids=encoding['input_ids'],
  attention_mask=encoding['attention_mask']
)
</code></pre>
<p>Followed by code below</p>
<pre><code>last_hidden_state.shape
</code></pre>
<p>Output is</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-70-9628339f425d&gt; in &lt;module&gt;
----&gt; 1 last_hidden_state.shape

AttributeError: 'str' object has no attribute 'shape'
</code></pre>
<p>Complete Code link is '<a href=""https://colab.research.google.com/drive/1FY4WtqCi2CQ9RjHj4slZwtdMhwaWv2-2?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1FY4WtqCi2CQ9RjHj4slZwtdMhwaWv2-2?usp=sharing</a>'</p>
"
66561880,Weights of pre-trained BERT model not initialized,"<p>I am using the <a href=""https://github.com/pair-code/lit"" rel=""nofollow noreferrer"">Language Interpretability Toolkit</a> (LIT) to load and analyze a BERT model that I pre-trained on an NER task.</p>
<p>However, when I'm starting the LIT script with the path to my pre-trained model passed to it, it fails to initialize the weights and tells me:</p>
<pre><code>    modeling_utils.py:648] loading weights file bert_remote/examples/token-classification/Data/Models/results_21_03_04_cleaned_annotations/04.03._8_16_5e-5_cleaned_annotations/04-03-2021 (15.22.23)/pytorch_model.bin
    modeling_utils.py:739] Weights of BertForTokenClassification not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
    modeling_utils.py:745] Weights from pretrained model not used in BertForTokenClassification: ['bert.embeddings.position_ids']

</code></pre>
<p>It then simply uses the <code>bert-base-german-cased</code> version of BERT, which of course doesn't have my custom labels and thus fails to predict anything. I think it might have to do with PyTorch, but I can't find the error.</p>
<p>If relevant, here is how I load my dataset into CoNLL 2003 format (modification of the dataloader scripts found <a href=""https://github.com/PAIR-code/lit/tree/main/lit_nlp/examples/datasets"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self):

        # Read ConLL Test Files

        self._examples = []

        data_path = &quot;lit_remote/lit_nlp/examples/datasets/NER_Data&quot;
        with open(os.path.join(data_path, &quot;test.txt&quot;), &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
            lines = f.readlines()

        for line in lines[:2000]:
            if line != &quot;\n&quot;:
                token, label = line.split(&quot; &quot;)
                self._examples.append({
                    'token': token,
                    'label': label,
                })
            else:
                self._examples.append({
                    'token': &quot;\n&quot;,
                    'label': &quot;O&quot;
                })

    def spec(self):
        return {
            'token': lit_types.Tokens(),
            'label': lit_types.SequenceTags(align=&quot;token&quot;),
        }
</code></pre>
<p>And this is how I initialize the model and start the LIT server (modification of the <code>simple_pytorch_demo.py</code> script found <a href=""https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/simple_pytorch_demo.py"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self, model_name_or_path):
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_name_or_path)
        model_config = transformers.AutoConfig.from_pretrained(
            model_name_or_path,
            num_labels=15,  # FIXME CHANGE
            output_hidden_states=True,
            output_attentions=True,
        )
        # This is a just a regular PyTorch model.
        self.model = _from_pretrained(
            transformers.AutoModelForTokenClassification,
            model_name_or_path,
            config=model_config)
        self.model.eval()

## Some omitted snippets here

    def input_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;token&quot;: lit_types.Tokens(),
            &quot;label&quot;: lit_types.SequenceTags(align=&quot;token&quot;)
        }

    def output_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;tokens&quot;: lit_types.Tokens(),
            &quot;probas&quot;: lit_types.MulticlassPreds(parent=&quot;label&quot;, vocab=self.LABELS),
            &quot;cls_emb&quot;: lit_types.Embeddings()
</code></pre>
"
66579324,Error running run_seq2seq.py Transformers training script,"<p>I am trying to train a seq2seq model. I ran the example code in Colab:</p>
<pre><code>!git clone https://github.com/huggingface/transformers
!git clone https://github.com/huggingface/datasets
!pip install transformers
!pip install datasets
</code></pre>
<pre><code>!python transformers/examples/seq2seq/run_seq2seq.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --task summarization \
    --dataset_name xsum \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate \
    --max_train_samples 500 \
    --max_val_samples 500
</code></pre>
<p>and got this error</p>
<pre><code>I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File &quot;transformers/examples/seq2seq/run_seq2seq.py&quot;, line 47, in &lt;module&gt;
    from transformers.file_utils import is_offline_mode
ImportError: cannot import name 'is_offline_mode' from 'transformers.file_utils' (/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py)
</code></pre>
<p>Any ideas?</p>
"
66596142,BertModel or BertForPreTraining,"<p>I want to use Bert only for embedding and use the Bert output as an input for a classification net that I will build from scratch.</p>
<p>I am not sure if I want to do finetuning for the model.</p>
<p>I think the relevant classes are BertModel or BertForPreTraining.</p>
<p><a href=""https://dejanbatanjac.github.io/bert-word-predicting/"" rel=""nofollow noreferrer"">BertForPreTraining</a>  head contains two &quot;actions&quot;:
self.predictions is MLM (Masked Language Modeling) head is what gives BERT the power to fix the grammar errors, and self.seq_relationship is NSP (Next Sentence Prediction); usually refereed as the classification head.</p>
<pre><code>class BertPreTrainingHeads(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)
</code></pre>
<p>I think the NSP isn't relevant for my task so I can &quot;override&quot; it.
what does the MLM do and is it relevant for my goal or should I use the BertModel?</p>
"
66625389,AttributeError: 'list' object has no attribute 'size' Hugging-Face transformers,"<p>I am trying to use Huggingface to transform stuff from English to Hindi.
This is the code snippet</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-hi&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-hi&quot;)
text = &quot;Hello my friends! How are you doing today?&quot;
tokenized_text = tokenizer.prepare_seq2seq_batch([text])

# Perform translation and decode the output
translation = model.generate(**tokenized_text)
translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]

# Print translated text
print(translated_text)
</code></pre>
<p>I am getting this error while trying to call the method generate on 'model'.</p>
<blockquote>
<p>AttributeError: 'list' object has no attribute 'size'.</p>
</blockquote>
<p>I am running on transformer version 4.3.3.</p>
"
66633813,Sequence to Sequence Loss,"<p>I'm trying to figure out how sequence to sequence loss is calculated. I am using the huggingface transformers library in this case, but this might actually be relevant to other DL libraries.</p>
<p>So to get the required data we can do:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import EncoderDecoderModel, BertTokenizer
import torch
import torch.nn.functional as F
torch.manual_seed(42)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
MAX_LEN = 128
tokenize = lambda x: tokenizer(x, max_length=MAX_LEN, truncation=True, padding=True, return_tensors=&quot;pt&quot;)

model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints
input_seq = [&quot;Hello, my dog is cute&quot;, &quot;my cat cute&quot;]
output_seq = [&quot;Yes it is&quot;, &quot;ok&quot;]
input_tokens = tokenize(input_seq)
output_tokens = tokenize(output_seq)

outputs = model(
    input_ids=input_tokens[&quot;input_ids&quot;], 
    attention_mask=input_tokens[&quot;attention_mask&quot;],
    decoder_input_ids=output_tokens[&quot;input_ids&quot;], 
    decoder_attention_mask=output_tokens[&quot;attention_mask&quot;],
    labels=output_tokens[&quot;input_ids&quot;], 
    return_dict=True)

idx = output_tokens[&quot;input_ids&quot;]
logits = F.log_softmax(outputs[&quot;logits&quot;], dim=-1)
mask = output_tokens[&quot;attention_mask&quot;]
</code></pre>
<h2>Edit 1</h2>
<p>Thanks to @cronoik I was able to replicate the loss calculated by huggingface as being:</p>
<pre class=""lang-py prettyprint-override""><code>output_logits = logits[:,:-1,:]
output_mask = mask[:,:-1]
label_tokens = output_tokens[&quot;input_ids&quot;][:, 1:].unsqueeze(-1)
select_logits = torch.gather(output_logits, -1, label_tokens).squeeze()
huggingface_loss = -select_logits.mean()
</code></pre>
<p>However, since the last two tokens of the second input is just padding, shouldn't we calculate the loss to be:</p>
<pre class=""lang-py prettyprint-override""><code>seq_loss = (select_logits * output_mask).sum(dim=-1, keepdims=True) / output_mask.sum(dim=-1, keepdims=True)
seq_loss = -seq_loss.mean()
</code></pre>
<p>^This takes into account the length of the sequence of each row of outputs, and the padding by masking it out. Think this is especially useful when we have batches of varying length outputs.</p>
"
66644432,Use huggingface transformers without IPyWidgets,"<p>I am trying to use the huggingface transformers library in a hosted Jupyter notebook platform called Deepnote. I want to download a model through the pipeline class but unfortunately deepnote does not support IPyWidgets. Is there a way to disable IPywidgets when using transformers? Specifically the below command.</p>
<pre><code>
classifier = pipeline(&quot;zero-shot-classification&quot;)
</code></pre>
<p>And the error I receive.</p>
<pre><code>ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
</code></pre>
<p>Note: Installing IPyWidgets is not an option</p>
"
66655023,Longformer get last_hidden_state,"<p>I am trying to follow this example in the huggingface documentation here <a href=""https://huggingface.co/transformers/model_doc/longformer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/longformer.html</a>:</p>
<pre><code>import torch
from transformers import LongformerModel, LongformerTokenizer
model = LongformerModel.from_pretrained('allenai/longformer-base-4096')
tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document
input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1
# Attention mask values -- 0: no attention, 1: local attention, 2: global attention
attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention
global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to global attention to be deactivated for all tokens
global_attention_mask[:, [1, 4, 21,]] = 1  # Set global attention to random tokens for the sake of this example
                                    # Usually, set global attention based on the task. For example,
                                    # classification: the &lt;s&gt; token
                                    # QA: question tokens
                                    # LM: potentially on the beginning of sentences and paragraphs
outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask, output_hidden_states= True)
sequence_output = outputs[0].last_hidden_state
pooled_output = outputs.pooler_output
</code></pre>
<p>I suppose that this would return a document embedding for the sample text.
However, I run into the following error:</p>
<pre><code>AttributeError: 'Tensor' object has no attribute 'last_hidden_state'
</code></pre>
<p>Why isnt it possible to call last_hidden_state?</p>
"
66656622,Python ImportError: cannot import name 'version' from 'packaging' (transformers),"<p>when I'm trying to simply <code>import transformers</code> I receive this error:</p>
<p>ImportError: cannot import name 'version' from 'packaging' (C:\Users\miria\packaging.py)</p>
<p>Can anyone help me solve this?</p>
<p><a href=""https://i.stack.imgur.com/RHMPL.png"" rel=""nofollow noreferrer"">traceback</a></p>
"
66693724,"while exporting T5 model to onnx using fastT5 getting ""RuntimeError:output with shape [5, 8, 1, 2] doesn't match the broadcast shape [5, 8, 2, 2]""","<p>i'm trying to convert T5 model to onnx using the <a href=""https://github.com/Ki6an/fastT5"" rel=""nofollow noreferrer"">fastT5</a> library, but
getting an error while running the following code</p>
<pre><code>from fastT5 import export_and_get_onnx_model
from transformers import AutoTokenizer

model_name = 't5-small'
model = export_and_get_onnx_model(model_name)

tokenizer = AutoTokenizer.from_pretrained(model_name)
t_input = &quot;translate English to French: The universe is a dark forest.&quot;
token = tokenizer(t_input, return_tensors='pt')

tokens = model.generate(input_ids=token['input_ids'],
               attention_mask=token['attention_mask'],
               num_beams=2)

output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)
print(output)
</code></pre>
<p>the error:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:244: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if causal_mask.shape[1] &lt; attention_mask.shape[1]:
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-16-80094b7c4f6f&gt; in &lt;module&gt;()
      7                     input_names=decoder_input_names,
      8                     output_names=decoder_output_names,
----&gt; 9                     dynamic_axes=dyn_axis_params,
     10                     )

24 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py in forward(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)
    497                 position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)
    498 
--&gt; 499         scores += position_bias
    500         attn_weights = F.softmax(scores.float(), dim=-1).type_as(
    501             scores

RuntimeError: output with shape [5, 8, 1, 2] doesn't match the broadcast shape [5, 8, 2, 2]
</code></pre>
<p>can someone please help me solve the issue?
<br/>
thank you.</p>
"
66747954,Tokenizing & encoding dataset uses too much RAM,"<p>Trying to tokenize and encode data to feed to a neural network.</p>
<p>I only have 25GB RAM and everytime I try to run the code below my google colab crashes. Any idea how to prevent his from happening? â€œYour session crashed after using all available RAMâ€</p>
<p>I thought tokenize/encoding chunks of 50000 sentences would work but unfortunately not.
The code works on a dataset with length 1.3 million. The current dataset has a length of  5 million.</p>
<pre><code>max_q_len = 128
max_a_len = 64    
trainq_list = train_q.tolist()    
batch_size = 50000
    
def batch_encode(text, max_seq_len):
      for i in range(0, len(trainq_list), batch_size):
        encoded_sent = tokenizer.batch_encode_plus(
            text,
            max_length = max_seq_len,
            pad_to_max_length=True,
            truncation=True,
            return_token_type_ids=False
        )
      return encoded_sent

    # tokenize and encode sequences in the training set
    tokensq_train = batch_encode(trainq_list, max_q_len)
</code></pre>
<p>The tokenizer comes from HuggingFace:</p>
<pre><code>tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')
</code></pre>
"
66767832,"BertTokenizer.from_pretrained errors out with ""Connection error""","<p>I am trying to download the tokenizer from Huggingface for BERT.</p>
<p>I am executing:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
</code></pre>
<p>Error:</p>
<pre><code>&lt;Path&gt;\tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1663                         resume_download=resume_download,
   1664                         local_files_only=local_files_only,
-&gt; 1665                         use_auth_token=use_auth_token,
   1666                     )
   1667 

&lt;Path&gt;\file_utils.py in cached_path(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)
   1140             user_agent=user_agent,
   1141             use_auth_token=use_auth_token,
-&gt; 1142             local_files_only=local_files_only,
   1143         )
   1144     elif os.path.exists(url_or_filename):

&lt;Path&gt;\file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)
   1347                 else:
   1348                     raise ValueError(
-&gt; 1349                         &quot;Connection error, and we cannot find the requested files in the cached path.&quot;
   1350                         &quot; Please try again or make sure your Internet connection is on.&quot;
   1351                     )

ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
</code></pre>
<p>Based on a similar discussion on <a href=""https://github.com/huggingface/transformers/issues/8690"" rel=""nofollow noreferrer"">github in huggingface's repo</a>, I gather that the file that the above call wants to download is: <a href=""https://huggingface.co/bert-base-uncased/resolve/main/config.json"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/resolve/main/config.json</a></p>
<p>While I can access that json file perfectly well on my browser, I can not download it via requests.
The error I get is:</p>
<pre><code>&gt;&gt; import requests as r
&gt;&gt; r.get('https://huggingface.co/bert-base-uncased/resolve/main/config.json')
...
requests.exceptions.SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))
</code></pre>
<p>While examining the certificate of the page - <a href=""https://huggingface.co/bert-base-uncased/resolve/main/config.json"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/resolve/main/config.json</a>, I see that it is signed by my IT department not the standard CA root I would expect to find.
Based on discussion <a href=""https://stackoverflow.com/questions/5846652/can-proxy-change-ssl-certificate"">here</a>, it looks like it is plausible for SSL proxies to do something like this.</p>
<p>My IT department's certificate is in the trusted authorities list. But requests does not seem to be considering that list for trusting certificates.</p>
<p>Taking a cue from <a href=""https://stackoverflow.com/questions/30405867/how-to-get-python-requests-to-trust-a-self-signed-ssl-certificate"">a stack-overflow discussion on how to let requests trust a self-signed certificate</a> I have also tried append cacert.pem (file pointed to by curl-config --ca) with the ROOT certificate that appears for the huggingface and adding the path of this pem to REQUESTS_CA_BUNDLE</p>
<pre><code>export REQUESTS_CA_BUNDLE=/mnt/&lt;path&gt;/wsl-anaconda/ssl/cacert.pem
</code></pre>
<p>But it did not help at all.</p>
<p>Would you know how I can let requests know that it is OK to trust my IT department's certificate ?</p>
<p>P.S: If it matters, I am working on windows and am facing this in WSL as well.</p>
"
66797173,Issue while using transformers package inside the docker image,"<p>I am using transformers pipeline to perform sentiment analysis on sample texts from 6 different languages. I tested the code in my local Jupyterhub and it worked fine. But when I wrap it in a flask application and create a docker image out of it, the execution is hanging at the pipeline inference line and its taking forever to return the sentiment scores.</p>
<ul>
<li>mac os catalina 10.15.7 (no GPU)</li>
<li>Python version : 3.8</li>
<li>Transformers package : 4.4.2</li>
<li>torch version : 1.6.0</li>
</ul>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
model_name = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
results = classifier([&quot;We are very happy to show you the Transformers library.&quot;, &quot;We hope you don't hate it.&quot;])
print([i['score'] for i in results])
</code></pre>
<p>The above code works fine in Jupyter notebook and it has provided me the expected result</p>
<pre><code>[0.7495927810668945,0.2365245819091797]
</code></pre>
<p>So now if I create a docker image with flask wrapper its getting stuck at the <code>results = classifier([input_data])</code> line and the execution is running forever.</p>
<p>My folder structure is as follows:</p>
<pre><code>- src
    |-- app
         |--main.py
    |-- Dockerfile
    |-- requirements.txt
</code></pre>
<p>I used the below <code>Dockerfile</code> to create the image</p>
<pre><code>FROM tiangolo/uwsgi-nginx-flask:python3.8
COPY ./requirements.txt /requirements.txt
COPY ./app /app
WORKDIR /app
RUN pip install -r /requirements.txt
RUN echo &quot;uwsgi_read_timeout 1200s;&quot; &gt; /etc/nginx/conf.d/custom_timeout.conf
</code></pre>
<p>And my <code>requirements.txt</code> file is as follows:</p>
<pre><code>pandas==1.1.5
transformers==4.4.2
torch==1.6.0
</code></pre>
<p>My <code>main.py</code> script look like this :</p>
<pre><code>from flask import Flask, json, request, jsonify
import traceback
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline


app = Flask(__name__)
app.config[&quot;JSON_SORT_KEYS&quot;] = False

model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
nlp = pipeline('sentiment-analysis', model=model_path, tokenizer=model_path)

@app.route(&quot;/&quot;)
def hello():
    return &quot;Model: Sentiment pipeline test&quot;


@app.route(&quot;/predict&quot;, methods=['POST'])
def predict():

    json_request = request.get_json(silent=True)
    input_list = [i['text'] for i in json_request[&quot;input_data&quot;]]
    
    results = nlp(input_list)         ##########  Getting stuck here
    for result in results:
        print(f&quot;label: {result['label']}, with score: {round(result['score'], 4)}&quot;)
    score_list = [round(i['score'], 4) for i in results]
    
    return jsonify(score_list)

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0', debug=False, port=80)
</code></pre>
<p>My input payload is of the form</p>
<pre><code>{&quot;input_data&quot; : [{&quot;text&quot; : &quot;We are very happy to show you the Transformers library.&quot;},
                 {&quot;text&quot; : &quot;We hope you don't hate it.&quot;}]}
</code></pre>
<p>I tried looking into the transformers github issues but couldn't find one. I execution works fine even when using the flask development server but it runs forever when I wrap it and create a docker image. I am not sure if I am missing any additional dependency to be included while creating the docker image.</p>
<p>Thanks.</p>
"
66821505,Extracting Features from BertForSequenceClassification,"<p>Hello together currently IÂ´m trying to develop a model for contradicition detection. Using and fine-tuning a BERT Model I already got quite statisfactionary result but I think with with some other features I could get a better accuracy. I oriented myself on this <a href=""https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db"" rel=""nofollow noreferrer"">Tutorial</a>. After fine-tuning, my model looks like this:</p>
<pre><code>==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30000, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)

==== First Transformer ====

bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)
bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)

==== Output Layer ====

bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
</code></pre>
<p>My next step would be to get the [CLS] token from this model, combine it with a few hand crafted features and feed them into a different model (MLP) for classfification. Any hints how to do this?</p>
"
66824985,Huggingface error: AttributeError: 'ByteLevelBPETokenizer' object has no attribute 'pad_token_id',"<p>I am trying to tokenize some numerical strings using a <code>WordLevel</code>/<code>BPE</code> tokenizer, create a data collator and eventually use it in a PyTorch DataLoader to train a new model from scratch.</p>
<p>However, I am getting an error</p>
<blockquote>
<p>AttributeError: 'ByteLevelBPETokenizer' object has no attribute 'pad_token_id'</p>
</blockquote>
<p>when running the following code</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import DataCollatorForLanguageModeling
from tokenizers import ByteLevelBPETokenizer
from tokenizers.pre_tokenizers import Whitespace
from torch.utils.data import DataLoader, TensorDataset

data = ['4814 4832 4761 4523 4999 4860 4699 5024 4788 &lt;unk&gt;']

# Tokenizer
tokenizer = ByteLevelBPETokenizer()
tokenizer.pre_tokenizer = Whitespace()
tokenizer.train_from_iterator(data, vocab_size=1000, min_frequency=1, 
    special_tokens=[
        &quot;&lt;s&gt;&quot;,
        &quot;&lt;/s&gt;&quot;,
        &quot;&lt;unk&gt;&quot;,
        &quot;&lt;mask&gt;&quot;,
    ])

# Data Collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

train_dataset = TensorDataset(torch.tensor(tokenizer(data, ......)))

# DataLoader
train_dataloader = DataLoader(
    train_dataset, 
    collate_fn=data_collator
)
</code></pre>
<p>Is this error due to not having configured the <code>pad_token_id</code> for the tokenizer? If so, how can we do this?</p>
<p>Thanks!</p>
<p><strong>Error trace:</strong></p>
<pre class=""lang-py prettyprint-override""><code>AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/opt/anaconda3/envs/x/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py&quot;, line 198, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/opt/anaconda3/envs/x/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;/opt/anaconda3/envs/x/lib/python3.8/site-packages/transformers/data/data_collator.py&quot;, line 351, in __call__
    if self.tokenizer.pad_token_id is not None:
AttributeError: 'ByteLevelBPETokenizer' object has no attribute 'pad_token_id'
</code></pre>
<p><strong>Conda packages</strong></p>
<pre><code>pytorch                   1.7.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch
pytorch-lightning         1.2.5              pyhd8ed1ab_0    conda-forge
tokenizers                0.10.1                   pypi_0    pypi
transformers              4.4.2                    pypi_0    pypi
</code></pre>
"
66901602,What is tokenizer.max len doing in this class definition?,"<p>I am following Rostylav's tutorial found <a href=""https://colab.research.google.com/drive/15wa925dj7jvdvrz8_z3vU7btqAFQLVlG#scrollTo=7KrNfVNueNhR"" rel=""nofollow noreferrer"">here</a> and am runnning into an error I dont quite understand:</p>
<pre><code>AttributeError                            
Traceback (most recent call last)
&lt;ipython-input-22-523c0d2a27d3&gt; in &lt;module&gt;()
----&gt; 1 main(trn_df, val_df)

&lt;ipython-input-20-1f17c050b9e5&gt; in main(df_trn, df_val)
     59     # Training
     60     if args.do_train:
---&gt; 61         train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)
     62 
     63         global_step, tr_loss = train(args, train_dataset, model, tokenizer)

&lt;ipython-input-18-3c4f1599e14e&gt; in load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate)
     40 
     41 def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
---&gt; 42     return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)
     43 
     44 def set_seed(args):

&lt;ipython-input-18-3c4f1599e14e&gt; in __init__(self, tokenizer, args, df, block_size)
      8     def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):
      9 
---&gt; 10         block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)
     11 
     12         directory = args.cache_dir

AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'
</code></pre>
<p>This is the class I believe is causing the error, however I am not able to understand what Tokenize.max_len is supposed to do so I can try to fix it:</p>
<pre><code>   class ConversationDataset(Dataset):

    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):

        block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)

        directory = args.cache_dir
        cached_features_file = os.path.join(
            directory, args.model_type + &quot;_cached_lm_&quot; + str(block_size)
        )

        if os.path.exists(cached_features_file) and not args.overwrite_cache:
            logger.info(&quot;Loading features from cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;rb&quot;) as handle:
                self.examples = pickle.load(handle)
        else:
            logger.info(&quot;Creating features from dataset file at %s&quot;, directory)

            self.examples = []
            for _, row in df.iterrows():
                conv = construct_conv(row, tokenizer)
                self.examples.append(conv)

            logger.info(&quot;Saving features into cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;wb&quot;) as handle:
                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, item):
        return torch.tensor(self.examples[item], dtype=torch.long)
 
# Cacheing and storing of data/checkpoints

def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)
</code></pre>
<p>Thank you for reading!</p>
"
66950157,Getting predict.proba from BERT classififer,"<p>I have a classifier on top of BERT, and I would like to see the predict probability for creating the ROC curve. How do I get the predict proba?. The predicted probas will be used to calculate the TPR FPR and threshold for ROC curve. <br>
here is the code</p>
<pre><code>class BertBinaryClassifier(nn.Module):
    def __init__(self, dropout=0.1):
        super(BertBinaryClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 1)
        self.sigmoid = nn.Sigmoid()
        
    
    def forward(self, tokens, masks=None):
        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        prediction = self.sigmoid(linear_output)
        return prediction
# Config setting
BATCH_SIZE = 4
EPOCHS = 5
# Making dataloaders
train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)
train_sampler =  torch.utils.data.RandomSampler(train_dataset)
train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)
test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)
test_sampler =  torch.utils.data.SequentialSampler(test_dataset)
test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)

bert_clf = BertBinaryClassifier()
bert_clf = bert_clf.cuda()
#wandb.watch(bert_clf)
optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)

# training 
for epoch_num in range(EPOCHS):
    bert_clf.train()
    train_loss = 0
    for step_num, batch_data in enumerate(train_dataloader):
        token_ids, masks, labels = tuple(t for t in batch_data)
        token_ids, masks, labels = token_ids.to(device), masks.to(device), labels.to(device)
        preds = bert_clf(token_ids, masks)
        loss_func = nn.BCELoss()
        batch_loss = loss_func(preds, labels)
        train_loss += batch_loss.item()
        bert_clf.zero_grad()
        batch_loss.backward()
        optimizer.step()
        #wandb.log({&quot;Training loss&quot;: train_loss})
        print('Epoch: ', epoch_num + 1)
        print(&quot;\r&quot; + &quot;{0}/{1} loss: {2} &quot;.format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))

# evaluating on test
bert_clf.eval()
bert_predicted = []
all_logits = []
probs=[]
with torch.no_grad():
    test_loss = 0
    for step_num, batch_data in enumerate(test_dataloader):
        token_ids, masks, labels = tuple(t for t in batch_data)
        token_ids, masks, labels = token_ids.to(device), masks.to(device), labels.to(device)
        logits = bert_clf(token_ids, masks)
        pr=logits.ravel()
        probs+=pr
        loss_func = nn.BCELoss()
        loss = loss_func(logits, labels)
        test_loss += loss.item()
        numpy_logits = logits.cpu().detach().numpy()
        #print(numpy_logits)
        #wandb.log({&quot;Testing loss&quot;: test_loss})
        bert_predicted += list(numpy_logits[:, 0] &gt; 0.5)
        all_logits += list(numpy_logits[:, 0])
</code></pre>
<p>I am able to get the prediction score to calculate the accuracy or f1 score. But not the probability for creating ROC curve.
Thanks</p>
"
67058709,BERT - Is that needed to add new tokens to be trained in a domain specific environment?,"<p>My question here is no how to add new tokens, or how to train using a domain-specific corpus, I'm already doing that.</p>
<p>The thing is, am I supposed to add the domain-specific tokens before the MLM training, or I just let Bert figure out the context? If I choose to not include the tokens, am I going to get a poor task-specific model like NER?</p>
<p>To give you more background of my situation, I'm training a Bert model on medical text using Portuguese language, so, deceased names, drug names, and other stuff are present in my corpus, but I'm not sure I have to add those tokens before the training.</p>
<p>I saw this one: <a href=""https://stackoverflow.com/questions/64816669/using-pretrained-bert-model-to-add-additional-words-that-are-not-recognized-by-t"">Using Pretrained BERT model to add additional words that are not recognized by the model</a></p>
<p>But the doubts remain, as other sources say otherwise.</p>
<p>Thanks in advance.</p>
"
67089849,AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len',"<p>I am just using the huggingface transformer library and get the following message when running run_lm_finetuning.py: AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'. Anyone else with this problem or an idea how to fix it? Thanks!</p>
<p>My full experiment run:
mkdir experiments</p>
<p>for epoch in 5
do
python run_lm_finetuning.py <br />
--model_name_or_path distilgpt2 <br />
--model_type gpt2 <br />
--train_data_file small_dataset_train_preprocessed.txt <br />
--output_dir experiments/epochs_$epoch <br />
--do_train <br />
--overwrite_output_dir <br />
--per_device_train_batch_size 4 <br />
--num_train_epochs $epoch
done</p>
"
67097467,About BertForMaskedLM,"<p>I have recently read about Bert and want to use BertForMaskedLM for fill_mask task. I know about Bert architecture. Also, as far as I know, BertForMaskedLM  is built from Bert with a language modeling head on top, but I have no idea about what <em>language modeling head</em> means here. Can anyone give me a brief explanation.</p>
"
67153058,Cannot load BERT from local disk,"<p>I am trying to use Huggingface transformer api to load a locally downloaded M-BERT model but it is throwing an exception.
I clone this repo: <a href=""https://huggingface.co/bert-base-multilingual-cased"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-multilingual-cased</a></p>
<pre><code>bert = TFBertModel.from_pretrained(&quot;input/bert-base-multilingual-cased&quot;)
</code></pre>
<p>The directory structure is:</p>
<p><a href=""https://i.stack.imgur.com/rDj4T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rDj4T.png"" alt=""Directory structure"" /></a></p>
<p>But I am getting this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1277, in from_pretrained
    missing_keys, unexpected_keys = load_tf_weights(model, resolved_archive_file, load_weight_prefix)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 467, in load_tf_weights
    with h5py.File(resolved_archive_file, &quot;r&quot;) as f:
  File &quot;/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py&quot;, line 408, in __init__
    swmr=swmr)
  File &quot;/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py&quot;, line 173, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File &quot;h5py/_objects.pyx&quot;, line 54, in h5py._objects.with_phil.wrapper
  File &quot;h5py/_objects.pyx&quot;, line 55, in h5py._objects.with_phil.wrapper
  File &quot;h5py/h5f.pyx&quot;, line 88, in h5py.h5f.open
OSError: Unable to open file (file signature not found)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;train.py&quot;, line 81, in &lt;module&gt;
    __main__()
  File &quot;train.py&quot;, line 59, in __main__
    model = create_model(num_classes)
  File &quot;/content/drive/My Drive/msc-project/code/model.py&quot;, line 26, in create_model
    bert = TFBertModel.from_pretrained(&quot;input/bert-base-multilingual-cased&quot;)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1280, in from_pretrained
    &quot;Unable to load weights from h5 file. &quot;
OSError: Unable to load weights from h5 file. If you tried to load a TF 2.0 model from a PyTorch checkpoint, please set from_pt=True. 
</code></pre>
<p>Where am I going wrong?
Need help!
Thanks in advance.</p>
"
67157185,What does 'output_dir' mean in transformers.TrainingArguments?,"<p>On the huggingface site documentation, it says 'The output directory where the model predictions and checkpoints will be written'. I don't quite understand what it means. Do I have to create any file for that?</p>
"
67158554,Fine-tuning model's classifier layer with new label,"<p>I would like to fine-tune already fine-tuned BertForSequenceClassification model with new dataset containing just 1 additional label which hasn't been seen by model before.</p>
<p>By that, I would like to add 1 new label to the set of labels that model is currently able of classifying properly.</p>
<p>Moreover, I don't want classifier weights to be randomly initialized, I'd like to keep them intact and just update them accordingly to the dataset examples while increasing the size of classifier layer by 1.</p>
<p>The dataset used for further fine-tuning could look like this:</p>
<pre><code>sentece,label
intent example 1,new_label
intent example 2,new_label
...
intent example 10,new_label
</code></pre>
<p>My model's current classifier layer looks like this:</p>
<pre><code>Linear(in_features=768, out_features=135, bias=True)
</code></pre>
<p>How could I achieve it?<br>
Is it even a good approach?</p>
"
67194634,Error loading weights from a Hugging Face model,"<p>I'm using transformers and I already have loaded a model and It works fine:</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer

task='sentiment'
MODEL = &quot;cardiffnlp/twitter-roberta-base-{task}&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL) 
model.save_pretrained(MODEL)
</code></pre>
<p>but If I try to load another task like &quot;emotion&quot; or &quot;hate&quot;, I get this error:</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer

task='emotion'
MODEL = &quot;cardiffnlp/twitter-roberta-base-{task}&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)  ## Here I get the error
model.save_pretrained(MODEL)
</code></pre>
<p>This error:</p>
<pre><code>OSError: Can't load weights for 'cardiffnlp/twitter-roberta-base-emotion'. Make sure that:

- 'cardiffnlp/twitter-roberta-base-emotion' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'cardiffnlp/twitter-roberta-base-emotion' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.
</code></pre>
<p>I have checked it and these models actually exists are Hugging Face models, as you can see <a href=""https://huggingface.co/models?filter=arxiv:2010.12421"" rel=""nofollow noreferrer"">here</a>, so I dont get why is not working.</p>
<p>Edit: I have noticed that the first time I run it, It works with all the tasks (hate, emotion, sentiment) but If I try to run it again, then I get the error.</p>
"
67257008,OSError: libmkl_intel_lp64.so.1: cannot open shared object file: No such file or directory,"<p>I am trying to run a model on TPU as given in <a href=""https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb"" rel=""nofollow noreferrer"">colab notebook</a>. The model was working fine, but today I could not run the model.</p>
<p>I used the following code to install pytorch-xla.</p>
<pre><code>VERSION = &quot;nightly&quot;  #@param [&quot;1.5&quot; , &quot;20200325&quot;, &quot;nightly&quot;]
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version $VERSION
</code></pre>
<p>I try to install required libraries as below:</p>
<pre><code>!pip install -U nlp
!pip install sentencepiece
!pip install numpy --upgrade
</code></pre>
<p>However, when I try the following</p>
<pre><code>import nlp
</code></pre>
<p>It gives the following error:</p>
<pre><code>OSError: libmkl_intel_lp64.so.1: cannot open shared object file: No such file or directory
</code></pre>
<p>I searched the error and I tried the followings, but still does not work. Any ideas how to fix it? Note: It was working a few days ago, however, today it is not.</p>
<pre><code>!pip install mkl
#!export PATH=&quot;$PATH:/opt/intel/bin&quot;
#!export LD_LIBRARY_PATH=&quot;$PATH:opt/intel/mkl/lib/intel64_lin/&quot;
!export LID_LIBRAEY_PATH=&quot;$LID_LIBRARY_PATH:/opt/intel/mkl/lib/intel64_lin/&quot;
</code></pre>
"
67282155,"What is the simplest way to continue training a pre-trained BERT model, on a specific domain?","<p>I want to use a pre-trained BERT model in order to use it on a text classification task (I'm using Huggingface library). However, the pre-trained model was trained on domains that are different than mine, and I have a large unannotated dataset that can be used for fine-tuning it. If I use only my tagged examples and fine-tune it &quot;on the go&quot; while training on the specific task (BertForSequenceClassification), the dataset is too small for adapting the language model for the specific domain. What it the best way to do so?
Thanks!</p>
"
67286034,Tokenizing a dataframe using Tensorflow and Transformers,"<p>I have a labeled dataset in a pandas dataframe.</p>
<pre><code>&gt;&gt;&gt; df.dtypes
title          object
headline       object
byline         object
dateline       object
text           object
copyright    category
country      category
industry     category
topic        category
file           object
dtype: object
</code></pre>
<p>I am building a model to predict <code>topic</code> based on <code>text</code>. While <code>text</code> is a large string, <code>topic</code> is a list of strings. For example:</p>
<pre><code>&gt;&gt;&gt; df['topic'].head(5)
0    ['ECONOMIC PERFORMANCE', 'ECONOMICS', 'EQUITY ...
1      ['CAPACITY/FACILITIES', 'CORPORATE/INDUSTRIAL']
2    ['PERFORMANCE', 'ACCOUNTS/EARNINGS', 'CORPORAT...
3    ['PERFORMANCE', 'ACCOUNTS/EARNINGS', 'CORPORAT...
4    ['STRATEGY/PLANS', 'NEW PRODUCTS/SERVICES', 'C...
</code></pre>
<p>Before I put this through a model, I have to tokenize this whole dataframe, yet when running it through transformer's <code>Autotokenizer</code> I get getting an error.</p>
<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import AutoTokenizer
import tensorflow_hub as hub
import tensorflow_text as text
from sklearn.model_selection import train_test_split

def preprocess_text(df):

    # Remove punctuations and numbers
    df['text'] = df['text'].str.replace('[^a-zA-Z]', ' ', regex=True)

    # Single character removal
    df['text'] = df['text'].str.replace(r&quot;\s+[a-zA-Z]\s+&quot;, ' ', regex=True)

    # Removing multiple spaces
    df['text'] = df['text'].str.replace(r'\s+', ' ', regex=True)

    # Remove NaNs
    df['text'] = df['text'].fillna('')
    df['topic'] = df['topic'].cat.add_categories('').fillna('')

    return df

# Load tokenizer and logger
tf.get_logger().setLevel('ERROR')
tokenizer = AutoTokenizer.from_pretrained('roberta-large')

# Load dataframe with just text and topic columns
# Only loading first 100 rows for testing purposes
df = pd.DataFrame()
for chunk in pd.read_csv(r'Reuters\test.csv', sep='|', chunksize=100,
                dtype={'topic': 'category', 'country': 'category', 'industry': 'category', 'copyright': 'category'}):
    df = chunk
    break
df = preprocess_text(df)

# Split dataset into train, test, val (70, 15, 15)
train, test = train_test_split(df, test_size=0.15)
train, val = train_test_split(train, test_size=0.15)

# Tokenize datasets
train = tokenizer(train, return_tensors='tf', truncation=True, padding=True, max_length=128)
val = tokenizer(val, return_tensors='tf', truncation=True, padding=True, max_length=128)
test = tokenizer(test, return_tensors='tf', truncation=True, padding=True, max_length=128)
</code></pre>
<p>I get this error:</p>
<pre><code>AssertionError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
</code></pre>
<p>on the line <code>train = tokenizer(train, return_tensors='tf', truncation=True, padding=True, max_length=128)</code>.</p>
<p>Does this mean I have to turn my df into a list?</p>
"
67291062,Control the logging frequency and contents when using wandb with HuggingFace,"<p>I am using the <code>wandb</code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions</p>
<ul>
<li>How does <code>wandb</code> decide when to log the loss? Is this decided by <code>logging_steps</code> in <code>TrainingArguments(...)</code>ï¼Ÿ</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, 
                                  learning_rate=lr,
                                  num_train_epochs=n_epoch,
                                  seed=seed,
                                  per_device_train_batch_size=2,
                                  per_device_eval_batch_size=2,
                                  logging_strategy=&quot;steps&quot;,
                                  logging_steps=5,
                                  report_to=&quot;wandb&quot;)
</code></pre>
<ul>
<li>How do I make sure <code>wandb</code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?</li>
</ul>
"
67299510,Understanding how gpt-2 tokenizes the strings,"<p>Using tutorials <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">here</a> , I wrote the following codes:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>So I realize that &quot;inputs&quot;, consists of tokenized items of my sentence.
But how can I get the values of tokenized items? (see for example [&quot;hello&quot;, &quot;,&quot;, &quot;my&quot;, &quot;dog&quot;, &quot;is&quot;, &quot;cute&quot;])</p>
<p>I am asking this because sometimes I think it separetes a word if that word is not in its dictionary (i.e., a word from another language). So I want to check that in my codes.</p>
"
67326333,Wav2Vec pytorch element 0 of tensors does not require grad and does not have a grad_fn,"<p>I am retraining a wav2vec model from hugging face for classification problem. I have 5 classes and the input is a list of tensors [1,400].
Here is how I am getting the model</p>
<pre><code>num_labels = 5
model_name = &quot;Zaid/wav2vec2-large-xlsr-53-arabic-egyptian&quot;
model_config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)  ##needed for the visualizations
tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name, config=model_config)
</code></pre>
<p>Here is the model updated settings</p>
<pre><code># Freeze the pre trained parameters
for param in model.parameters():
    param.requires_grad = False
criterion = nn.MSELoss().to(device)
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-6)

# Add three new layers at the end of the network
model.classifier = nn.Sequential(
    nn.Linear(768, 256),
    nn.Dropout(0.25),
    nn.ReLU(),
    nn.Linear(256, 64),
    nn.Dropout(0.25),
    nn.ReLU(),
    nn.Linear(64, 2),
    nn.Dropout(0.25),
    nn.Softmax(dim=1)
)
</code></pre>
<p>Then the training loop</p>
<pre><code>print_every = 300

total_loss = 0
all_losses = []
model.train()
for epoch in range(2):
    print(&quot;Epoch number: &quot;, epoch)
    for row in range(16918):
        Input = torch.tensor(trn_ivectors[row]).double()
        label = torch.tensor(trn_labels[row]).long().to(device)
        label = torch.unsqueeze(label,0).to(device)
        #print(&quot;Label&quot;, label.shape)
        Input = torch.unsqueeze(Input,1).to(device)
        #print(Input.shape)
        optimizer.zero_grad()
        
        #Input.requires_grad = True
        Input = F.softmax(Input[0], dim=-1)
        
        if label == 0:
            label = torch.tensor([1.0, 0.0]).float().to(device)
        elif label == 1:
            label = torch.tensor([0.0, 1.0]).float().to(device)

        # print(overall_output, label)

        loss = criterion(Input, label)
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

        if idx % print_every == 0 and idx &gt; 0:
            average_loss = total_loss / print_every
            print(&quot;{}/{}. Average loss: {}&quot;.format(idx, len(train_data), average_loss))
            all_losses.append(average_loss)
            total_loss = 0

torch.save(model.state_dict(), &quot;model_after_train.pt&quot;)
</code></pre>
<p>Unfortunately when I try to train the program it gives me the following error</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>Please I would appreciate if you could tell me how to fix this error. I have been searching a lot on a way fixing it but didn't fix it</p>
<p>Thanks</p>
"
67356666,how to add tokens in vocab.txt which decoded as [UNK] bert tokenizer,"<p>i was decoding the tokenized tokens from <strong>bert tokenizer</strong> and it was giving <strong>[UNK]</strong> for â‚¬ symbol. but i tried by add ##â‚¬ token in vocab.txt file. but it was not reflected in prediction result was same as previous it was giving [UNK] again. please let me know to solve this problem did i need to <strong>fine tune</strong> the model for again to reflect the changes in <strong>prediction</strong>. till now i was avoiding fine tuning again because it takes more than 10 hours.
Thanks in advance</p>
"
67412925,what is the difference between len(tokenizer) and tokenizer.vocab_size,"<p>I'm trying to add a few new words to the vocabulary of a pretrained HuggingFace Transformers model. I did the following to change the vocabulary of the tokenizer and also increase the embedding size of the model:</p>
<pre><code>tokenizer.add_tokens(['word1', 'word2', 'word3', 'word4'])
model.resize_token_embeddings(len(tokenizer))
print(len(tokenizer)) # outputs len_vocabulary + 4
</code></pre>
<p>But after training the model on my corpus and saving it, I found out that the saved tokenizer vocabulary size hasn't changed. After checking again I found out that the abovementioned code does not change the vocabulary size (tokenizer.vocab_size is still the same) and only the len(tokenizer) has changed.</p>
<p>So now my question is; what is the difference between tokenizer.vocab_size and len(tokenizer)?</p>
"
67496616,"RuntimeError: Input, output and indices must be on the current device. (fill_mask(""Random text <mask>."")","<p>I am getting &quot;RuntimeError: Input, output and indices must be on the current device.&quot;
when I run this line.
fill_mask(&quot;Auto Car .&quot;)</p>
<p>I am running it on Colab.
My Code:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer
from transformers import BertTokenizer, BertForMaskedLM


paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
print(paths)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

from transformers import BertModel, BertConfig

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config
print(configuration)

model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
    tokenizer=bert_tokenizer,
    file_path=&quot;./kant.txt&quot;,
    block_size=128,
)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./KantaiBERT&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

from transformers import pipeline

fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=model,
    tokenizer=bert_tokenizer
)

fill_mask(&quot;Auto Car &lt;mask&gt;.&quot;)
</code></pre>
<p>The last line is giving me the error mentioned above. Please let me know what I am doing wrong or what I have to do in order to remove this error.</p>
"
67511285,Using transformers class BertForQuestionAnswering for Extractive Question Answering,"<p>I'm using a BERT model for Extractive QA task with the <code>transformers</code> class library <code>BertForQuestionAnswering</code>.  Extractive Question Answering is the task of answering a question for a given context text and outputting the start and end indexes of where the answer matches in the context. <a href=""https://github.com/loretoparisi/hf-experiments/blob/master/src/bert/run.py"" rel=""nofollow noreferrer"">My code</a> is the following:</p>
<pre><code>model = BertForQuestionAnswering.from_pretrained('bert-base-uncased',
    cache_dir=os.getenv(&quot;cache_dir&quot;, &quot;../../models&quot;))
question = &quot;What is the capital of Italy?&quot;
text = &quot;The capital of Italy is Rome.&quot;
inputs = tokenizer.encode_plus(question, text, return_tensors='pt')
start, end = model(**inputs)
start_max = torch.argmax(F.softmax(start, dim = -1))
end_max = torch.argmax(F.softmax(end, dim = -1)) + 1 ## add one ##because of python list indexing
answer = tokenizer.decode(inputs[&quot;input_ids&quot;][0][start_max : end_max])
print(answer)
</code></pre>
<p>I get this error</p>
<pre><code>start_max = torch.argmax(F.softmax(start, dim = -1))
  File &quot;/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 1583, in softmax
    ret = input.softmax(dim)
AttributeError: 'str' object has no attribute 'softmax'
</code></pre>
<p>I have also tried this approach, slightly different</p>
<pre><code>encoding = tokenizer.encode_plus(text=question,text_pair=text, add_special=True)
inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens
start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))
start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)
answer = ' '.join(tokens[start_index:end_index+1])
</code></pre>
<p>but the error is likely the same:</p>
<pre><code>    start_index = torch.argmax(start_scores)
TypeError: argmax(): argument 'input' (position 1) must be Tensor, not str
</code></pre>
<p>I assume due to the unpack of the output as</p>
<pre><code>start, end = model(**inputs)
</code></pre>
<p>If so, how to correct unpack this model's outputs?</p>
"
67511800,PipelineException: No mask_token ([MASK]) found on the input,"<p>I am getting this error &quot;PipelineException: No mask_token ([MASK]) found on the input&quot;
when I run this line.
fill_mask(&quot;Auto Car .&quot;)</p>
<p>I am running it on Colab.
My Code:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer
from transformers import BertTokenizer, BertForMaskedLM


paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
print(paths)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

from transformers import BertModel, BertConfig

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config
print(configuration)

model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
    tokenizer=bert_tokenizer,
    file_path=&quot;./kant.txt&quot;,
    block_size=128,
)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./KantaiBERT&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

from transformers import pipeline

fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=model,
    tokenizer=bert_tokenizer,
    device=0,
)

fill_mask(&quot;Auto Car &lt;mask&gt;.&quot;).     # This line is giving me the error...
</code></pre>
<p>The last line is giving me the error mentioned above. Please let me know what I am doing wrong or what I have to do in order to remove this error.</p>
<p>Complete error: &quot;f&quot;No mask_token ({self.tokenizer.mask_token}) found on the input&quot;,&quot;</p>
"
67567587,Python: BERT Tokenizer cannot be loaded,"<p>I am working on the <code>bert-base-mutilingual-uncased</code> model but when I try to set the <code>TOKENIZER</code> in the <code>config</code> it throws an <code>OSError</code>.</p>
<h3>Model Config</h3>
<pre><code>class config: 
    DEVICE = &quot;cuda:0&quot;
    MAX_LEN = 256
    TRAIN_BATCH_SIZE = 8
    VALID_BATCH_SIZE = 4
    EPOCHS = 1

    BERT_PATH = {&quot;bert-base-multilingual-uncased&quot;: &quot;workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased&quot;}
    MODEL_PATH = &quot;workspace/data/jigsaw-multilingual/model.bin&quot;

    TOKENIZER = transformers.BertTokenizer.from_pretrained(
            BERT_PATH[&quot;bert-base-multilingual-uncased&quot;], 
            do_lower_case=True)
</code></pre>
<h3>Error</h3>
<pre><code>    ---------------------------------------------------------------------------
    OSError                                   Traceback (most recent call last)
    &lt;ipython-input-33-83880b6b788e&gt; in &lt;module&gt;
    ----&gt; 1 class config:
          2 #     def __init__(self):
          3 
          4         DEVICE = &quot;cuda:0&quot;
          5         MAX_LEN = 256
    
    &lt;ipython-input-33-83880b6b788e&gt; in config()
         11         TOKENIZER = transformers.BertTokenizer.from_pretrained(
         12             BERT_PATH[&quot;bert-base-multilingual-uncased&quot;],
    ---&gt; 13             do_lower_case=True)
    
    /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, *inputs, **kwargs)
       1138 
       1139         &quot;&quot;&quot;
    -&gt; 1140         return cls._from_pretrained(*inputs, **kwargs)
       1141 
       1142     @classmethod
    
    /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
       1244                     &quot;, &quot;.join(s3_models),
       1245                     pretrained_model_name_or_path,
    -&gt; 1246                     list(cls.vocab_files_names.values()),
       1247                 )
       1248             )
    
    OSError: Model name 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' was not  
 found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking,   
bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc,   
bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1,     
wietsedv/bert-base-dutch-cased). 

We assumed 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' was a path, a model   identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such  
 vocabulary files at this path or url.
</code></pre>
<p>As I can interpret the error, it says that the <code>vocab.txt</code> file was not found at the given location but actually its present.</p>
<p>Following are the files available in the <code>bert-base-multilingual-uncased</code> folder:</p>
<ul>
<li><code>config.json</code></li>
<li><code>pytorch_model.bin</code></li>
<li><code>vocab.txt</code></li>
</ul>
<p>I am new to working with <code>bert</code>, so I am not sure if there is a different way to define the tokenizer.</p>
"
67633551,Reading a pretrained huggingface transformer directly from S3,"<p>Loading a <a href=""https://huggingface.co/transformers/main_classes/model.html#pretrainedmodel"" rel=""nofollow noreferrer"">huggingface pretrained transformer model</a> seemingly requires you to have the model saved locally (as described <a href=""https://stackoverflow.com/a/64007213/1571593"">here</a>), such that you simply pass a local path to your model and config:</p>
<pre class=""lang-py prettyprint-override""><code>model = PreTrainedModel.from_pretrained('path/to/model', local_files_only=True)
</code></pre>
<p>Can this be achieved when the model is stored on S3?</p>
"
67635055,"Python: BERT Model Pooling Error - mean() received an invalid combination of arguments - got (str, int)","<p>I am writing the code to train a <code>bert</code> model on my dataset. By when I run the code it throws an error in the average pool layer. I am unable to understand what causes this error.</p>
<h2>Model</h2>
<pre><code>class BERTBaseUncased(nn.Module):
    def __init__(self, bert_path):
        super(BERTBaseUncased, self).__init__()
        self.bert_path = bert_path
        self.bert = transformers.BertModel.from_pretrained(self.bert_path)
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768 * 2, 1)

    def forward(
            self,
            ids,
            mask,
            token_type_ids
    ):
        o1, _ = self.bert(
            ids,
            attention_mask=mask,
            token_type_ids=token_type_ids)
        
        apool = torch.mean(o1, 1)
        mpool, _ = torch.max(o1, 1)
        cat = torch.cat((apool, mpool), 1)

        bo = self.bert_drop(cat)
        p2 = self.out(bo)
        return p2
</code></pre>
<h2>Error</h2>
<pre><code>Exception in device=TPU:0: mean() received an invalid combination of arguments - got (str, int), but expected one of:
 * (Tensor input, *, torch.dtype dtype)
 * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)
 * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)

Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py&quot;, line 228, in _start_fn
    fn(gindex, *args)
  File &quot;&lt;ipython-input-12-94e926c1f4df&gt;&quot;, line 4, in _mp_fn
    a = _run()
  File &quot;&lt;ipython-input-5-ef9fa564682f&gt;&quot;, line 146, in _run
    train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)
  File &quot;&lt;ipython-input-5-ef9fa564682f&gt;&quot;, line 22, in train_loop_fn
    token_type_ids=token_type_ids
  File &quot;/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 577, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;&lt;ipython-input-11-9196e0d23668&gt;&quot;, line 73, in forward
    apool = torch.mean(o1, 1)
TypeError: mean() received an invalid combination of arguments - got (str, int), but expected one of:
 * (Tensor input, *, torch.dtype dtype)
 * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)
 * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)
</code></pre>
<p>I am trying to run this on a Kaggle TPU. How to fix this?</p>
"
67639478,Is there a significant speed improvement when using transformers tokenizer over batch compared to per item?,"<p>is calling tokenizer on a batch significantly faster than on calling it on each item in a batch? e.g.</p>
<pre class=""lang-py prettyprint-override""><code>encodings = tokenizer(sentences)
# vs
encodings = [tokenizer(x) for x in sentences]
</code></pre>
"
67689219,Copy one layer's weights from one Huggingface BERT model to another,"<p>I have a pre-trained model which I load like so:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel
model = BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>
<p>I want to create a new model with the same architecture, and random initial weights, <em>except</em> for the embedding layer:</p>
<pre><code>==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30522, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)
</code></pre>
<p>It seems I can do this to create a new model with the same architecture, but then <em>all</em> the weights are random:</p>
<pre><code>configuration   = model.config
untrained_model = BertForSequenceClassification(configuration)
</code></pre>
<p>So how do I copy over <code>model</code>'s embedding layer weights to the new <code>untrained_model</code>?</p>
"
67699354,Are these normal speed of Bert Pretrained Model Inference in PyTorch,"<p>I am testing Bert base and Bert distilled model in Huggingface with 4 scenarios of speeds, batch_size = 1:</p>
<pre><code>1) bert-base-uncased: 154ms per request
2) bert-base-uncased with quantifization: 94ms per request
3) distilbert-base-uncased: 86ms per request
4) distilbert-base-uncased with quantifization: 69ms per request
</code></pre>
<p>I am using the IMDB text as experimental data and set the max_length=512, so it's quite long. The cpu on Ubuntu 18.04 info is below:</p>
<pre><code>cat /proc/cpuinfo  | grep 'name'| uniq
model name  : Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
</code></pre>
<p>The machine has 3 GPU available for use:</p>
<pre><code>Tesla V100-SXM2
</code></pre>
<p>It seems quite slow for realtime application. Are those speeds normal for bert base model?</p>
<p>The testing code is below:</p>
<pre><code>import pandas as pd
import torch.quantization

from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel

def get_embedding(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
    outputs = model(**inputs)
    output_tensors = outputs[0][0]
    output_numpy = output_tensors.detach().numpy()
    embedding = output_numpy.tolist()[0]

def process_text(model, tokenizer, text_lines):
    for index, line in enumerate(text_lines):
        embedding = get_embedding(model, tokenizer, line)
        if index % 100 == 0:
            print('Current index: {}'.format(index))

import time
from datetime import timedelta
if __name__ == &quot;__main__&quot;:

    df = pd.read_csv('../data/train.csv', sep='\t')
    df = df.head(1000)
    text_lines = df['review']
    text_line_count = len(text_lines)
    print('Text size: {}'.format(text_line_count))

    start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
    model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end = time.time()
    print('Total time spent with bert base: {}'.format(str(timedelta(seconds=end - start))))

    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end2 = time.time()
    print('Total time spent with bert base quantization: {}'.format(str(timedelta(seconds=end2 - end))))

    tokenizer = DistilBertTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end3 = time.time()
    print('Total time spent with distilbert: {}'.format(str(timedelta(seconds=end3 - end2))))

    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end4 = time.time()
    print('Total time spent with distilbert quantization: {}'.format(str(timedelta(seconds=end4 - end3))))
</code></pre>
<p>EDIT: based on suggestion I changed to the following:</p>
<pre><code>inputs = tokenizer(text_batch, padding=True, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
</code></pre>
<p>Where text_batch is a list of text as input.</p>
"
67740498,Huggingface Electra - Load model trained with google implementation error: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte,"<p>I have trained an electra model from scratch using <a href=""https://github.com/google-research/electra"" rel=""nofollow noreferrer"">google implementation code</a>.</p>
<pre class=""lang-sh prettyprint-override""><code>python run_pretraining.py --data-dir gc://bucket-electra/dataset/ --model-name greek_electra --hparams hparams.json
</code></pre>
<p>with this json hyperparams:</p>
<pre class=""lang-json prettyprint-override""><code>{
&quot;embedding_size&quot;: 768,
&quot;max_seq_length&quot;: 512,
&quot;train_batch_size&quot;: 128,
&quot;vocab_size&quot;: 100000,
&quot;model_size&quot;: &quot;base&quot;,
&quot;num_train_steps&quot;: 1500000
}
</code></pre>
<p>After having trained the model, I used the <a href=""https://github.com/huggingface/transformers/blob/d5d7d886128732091e92afff7fcb3e094c71a7ec/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py"" rel=""nofollow noreferrer"">convert_electra_original_tf_checkpoint_to_pytorch.py</a> script from transformers library to convert the checkpoint.</p>
<pre class=""lang-sh prettyprint-override""><code>python convert_electra_original_tf_checkpoint_to_pytorch.py --tf_checkpoint_path output/models/transformer/greek_electra --config_file resources/hparams.json --pytorch_dump_path output/models/transformer/discriminator  --discriminator_or_generator &quot;discriminator&quot;
</code></pre>
<p>Now I am trying to load the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import ElectraForPreTraining

model = ElectraForPreTraining.from_pretrained('discriminator')
</code></pre>
<p>but I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;~/.local/lib/python3.9/site-packages/transformers/configuration_utils.py&quot;, line 427, in get_config_dict
    config_dict = cls._dict_from_json_file(resolved_config_file)
  File &quot;~/.local/lib/python3.9/site-packages/transformers/configuration_utils.py&quot;, line 510, in _dict_from_json_file
    text = reader.read()
  File &quot;/usr/lib/python3.9/codecs.py&quot;, line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte
</code></pre>
<p>Any ideas what's causing this &amp; how to solve it?</p>
"
67771257,Why does Transformer's BERT (for sequence classification) output depend heavily on maximum sequence length padding?,"<p>I am using Transformer's RobBERT (the dutch version of RoBERTa) for sequence classification - trained for sentiment analysis on the Dutch Book Reviews dataset.</p>
<p>I wanted to test how well it works on a similar dataset (also on sentiment analysis), so I made annotations for a set of text fragments and checked its accuracy. When I checked what kind of sentence are misclassified, I noticed that the output for a unique sentence depends heavily on the length of padding I give when tokenizing. See code below.</p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch.nn.functional as F
import torch


model = RobertaForSequenceClassification.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, num_labels=2)
tokenizer = RobertaTokenizer.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, do_lower_case=True)

sent = 'De samenwerking gaat de laatste tijd beter'
max_seq_len = 64


test_token = tokenizer(sent,
                        max_length = max_seq_len,
                        padding = 'max_length',
                        truncation = True,
                        return_tensors = 'pt'
                        )

out = model(test_token['input_ids'],test_token['attention_mask'])

probs = F.softmax(out[0], dim=1).detach().numpy()
</code></pre>
<p>For the given sample text, which translates in English to &quot;The collaboration has been improving lately&quot;, there is a huge difference in output on classification depending on the max_seq_len. Namely, for <code>max_seq_len = 64</code> the output for <code>probs</code> is:</p>
<p>[[0.99149346 0.00850648]]</p>
<p>whilst for <code>max_seq_len = 9</code>, being the actual length including cls tokens:</p>
<p>[[0.00494814 0.9950519 ]]</p>
<p>Can anyone explain why this huge difference in classification is happening? I would think that the attention mask ensures that in the output there is no difference because of padding to the max sequence length.</p>
"
67785438,AttributeError: 'NoneType' object has no attribute 'tokenize',"<p>I am trying to use XLNET through transformers. however i keep getting the issue &quot;AttributeError: 'NoneType' object has no attribute 'tokenize'&quot;. I am unsure of how to proceed. if anyone could point me in the right direction it would be appreciated.</p>
<pre><code>tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)

print(' Original: ', X_train[1])

# Print the tweet split into tokens.
print('Tokenized: ', tokenizer.tokenize(X_train[1]))

# Print the tweet mapped to token ids.
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X_train[1])))




Original:  hey angel duh sexy really thanks haha
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-67-2b1b432b3e15&gt; in &lt;module&gt;()
      2 
      3 # Print the tweet split into tokens.
----&gt; 4 print('Tokenized: ', tokenizer.tokenize(X_train[2]))
      5 
      6 # Print the tweet mapped to token ids.

AttributeError: 'NoneType' object has no attribute 'tokenize'
</code></pre>
"
67872803,Huggingface SciBERT predict masked word not working,"<p>I am trying to use the pretrained SciBERT model (<a href=""https://huggingface.co/allenai/scibert_scivocab_uncased"" rel=""nofollow noreferrer"">https://huggingface.co/allenai/scibert_scivocab_uncased</a>) from Huggingface to predict masked words in scientific/biomedical text.  This produces errors, and not sure how to move forward from this point.</p>
<p>Here is the code so far -</p>
<pre><code>!pip install transformers

from transformers import pipeline, AutoTokenizer, AutoModel
  
tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)

model = AutoModel.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)

unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)
unmasker(&quot;the patient is a 55 year old [MASK] admitted with pneumonia&quot;)
</code></pre>
<p>This works with BERT alone, but is not the specialized pre-trained model -</p>
<pre><code>!pip install transformers

from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
unmasker(&quot;the patient is a 55 year old [MASK] admitted with pneumonia&quot;)
</code></pre>
<p>The errors with SciBERT are -</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)
    494         kwargs[&quot;feature_extractor&quot;] = feature_extractor
    495 
--&gt; 496     return task_class(model=model, framework=framework, task=task, **kwargs)

/usr/local/lib/python3.7/dist-packages/transformers/pipelines/fill_mask.py in __init__(self, model, tokenizer, modelcard, framework, args_parser, device, top_k, task)
     73         )
     74 
---&gt; 75         self.check_model_type(TF_MODEL_WITH_LM_HEAD_MAPPING if self.framework == &quot;tf&quot; else MODEL_FOR_MASKED_LM_MAPPING)
     76         self.top_k = top_k
     77 

/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py in check_model_type(self, supported_models)
    652                 self.task,
    653                 self.model.base_model_prefix,
--&gt; 654                 f&quot;The model '{self.model.__class__.__name__}' is not supported for {self.task}. Supported models are {supported_models}&quot;,
    655             )
    656 

PipelineException: The model 'BertModel' is not supported for fill-mask. Supported models are ['BigBirdForMaskedLM', 'Wav2Vec2ForMaskedLM', 'ConvBertForMaskedLM', 'LayoutLMForMaskedLM', 'DistilBertForMaskedLM', 'AlbertForMaskedLM', 'BartForConditionalGeneration', 'MBartForConditionalGeneration', 'CamembertForMaskedLM', 'XLMRobertaForMaskedLM', 'LongformerForMaskedLM', 'RobertaForMaskedLM', 'SqueezeBertForMaskedLM', 'BertForMaskedLM', 'MegatronBertForMaskedLM', 'MobileBertForMaskedLM', 'FlaubertWithLMHeadModel', 'XLMWithLMHeadModel', 'ElectraForMaskedLM', 'ReformerForMaskedLM', 'FunnelForMaskedLM', 'MPNetForMaskedLM', 'TapasForMaskedLM', 'DebertaForMaskedLM', 'DebertaV2ForMaskedLM', 'IBertForMaskedLM']
</code></pre>
"
67894649,ValueError with NERDA model import,"<p>I'm trying to import the NERDA library in order use it to engage in a Named-Entity Recognition task in Python. I initially tried importing the library in a jupyter notebook and got the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\NERDA\models.py&quot;, line 13, in &lt;module&gt;
    from .networks import NERDANetwork
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\NERDA\networks.py&quot;, line 4, in &lt;module&gt;
    from transformers import AutoConfig
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\__init__.py&quot;, line 43, in &lt;module&gt;
    from . import dependency_versions_check
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\dependency_versions_check.py&quot;, line 36, in &lt;module&gt;
    from .file_utils import is_tokenizers_available
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\file_utils.py&quot;, line 51, in &lt;module&gt;
    from huggingface_hub import HfApi, HfFolder, Repository
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\__init__.py&quot;, line 31, in &lt;module&gt;
    from .file_download import cached_download, hf_hub_url
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\file_download.py&quot;, line 37, in &lt;module&gt;
    if tuple(int(i) for i in _PY_VERSION.split(&quot;.&quot;)) &lt; (3, 8, 0):
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\file_download.py&quot;, line 37, in &lt;genexpr&gt;
    if tuple(int(i) for i in _PY_VERSION.split(&quot;.&quot;)) &lt; (3, 8, 0):
ValueError: invalid literal for int() with base 10: '6rc1'
</code></pre>
<p>I then tried globally installing using pip in gitbash and got the same error. The library appeared to install without error but when I try the following import, I get that same ValueError:</p>
<pre><code>from NERDA.models import NERDA
</code></pre>
<p>I've also tried some of the pre-cooked model imports and gotten the same ValueError.</p>
<pre><code>from NERDA.precooked import EN_ELECTRA_EN
from NERDA.precooked import EN_BERT_ML
</code></pre>
<p>I can't find anything on this error online and am hoping someone may be able to lend some insight? Thanks so much!</p>
"
67924216,Why would a Torchscript trace return different looking encoded_inputs compared to the original Transformer model?,"<h2>Background</h2>
<p>I'm working with a finetuned <a href=""https://huggingface.co/transformers/model_doc/mbart.html?highlight=mbart"" rel=""nofollow noreferrer"">Mbart50</a> model that I need sped up for inferencing because using the HuggingFace model as-is is fairly slow with my current hardware. I wanted to use <a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noreferrer"">TorchScript</a> because I couldn't get <a href=""https://huggingface.co/transformers/serialization.html?highlight=onnx"" rel=""nofollow noreferrer"">onnx</a> to export this particular model as it seems it will be supported at a later time (I would be glad to be wrong otherwise).</p>
<h2>Convert Transformer to a Pytorch trace:</h2>
<pre class=""lang-py prettyprint-override""><code>import torch
&quot;&quot;&quot; Model data  &quot;&quot;&quot;
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model = MBartForConditionalGeneration.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;, torchscript= True)

tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
tokenizer.src_lang = 'en_XX'

dummy = &quot;To celebrate World Oceans Day, we're swimming through a shoal of jack fish just off the coast of Baja, California, in Cabo Pulmo National Park. This Mexican marine park in the Sea of Cortez is home to the northernmost and oldest coral reef on the west coast of North America, estimated to be about 20,000 years old. Jacks are clearly plentiful here, but divers and snorkelers in Cabo Pulmo can also come across many other species of fish and marine mammals, including several varieties of sharks, whales, dolphins, tortoises, and manta rays.&quot;

model.config.forced_bos_token_id=250006
myTokenBatch = tokenizer(dummy, max_length=192, padding='max_length', truncation = True, return_tensors=&quot;pt&quot;)

torch.jit.save(torch.jit.trace(model, [myTokenBatch.input_ids,myTokenBatch.attention_mask]), &quot;././traced-model/mbart-many.pt&quot;)
</code></pre>
<h2>Inference Step:</h2>
<pre class=""lang-py prettyprint-override""><code>

import torch
 &quot;&quot;&quot; Model data  &quot;&quot;&quot;
from transformers import MBart50TokenizerFast

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
 
model = torch.jit.load('././traced-model/mbart-many.pt')
MAX_LENGTH =  192

tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
 
model.to(device)
model.eval()

tokenizer.src_lang = 'en_XX'

dummy = &quot;To celebrate World Oceans Day, we're swimming through a shoal of jack fish just off the coast of Baja, California, in Cabo Pulmo National Park. This Mexican marine park in the Sea of Cortez is home to the northernmost and oldest coral reef on the west coast of North America, estimated to be about 20,000 years old. Jacks are clearly plentiful here, but divers and snorkelers in Cabo Pulmo can also come across many other species of fish and marine mammals, including several varieties of sharks, whales, dolphins, tortoises, and manta rays.&quot;

myTokenBatch = tokenizer(dummy, max_length=192, padding='max_length', truncation = True, return_tensors=&quot;pt&quot;)

encode, pool , norm  = model(myTokenBatch.input_ids,myTokenBatch.attention_mask)


</code></pre>
<h2>Expected Encoding Output:</h2>
<p>These are tokens that can be decoded to words with MBart50TokenizerFast.</p>
<pre><code>
tensor([[250004,    717, 176016,   6661,  55609,      7,  10013,      4,    642,
             25,    107, 192298,   8305,     10,  15756,    289,    111, 121477,
          67155,   1660,   5773,     70, 184085,    111, 118191,      4,  39897,
              4,     23, 143740,  21694,    432,   9907,   5227,      5,   3293,
         181815, 122084,   9201,     23,     70,  27414,    111,  48892,    169,
             83,   5368,     47,     70, 144477,   9022,    840,     18,    136,
          10332,    525, 184518,    456,   4240,     98,     70,  65272, 184085,
            111,  23924,  21629,      4,  25902,   3674,     47,    186,   1672,
              6,  91578,   5369,  10332,      5,  21763,      7,    621, 123019,
          32328,    118,   7844,   3688,      4,   1284,  41767,    136, 120379,
           2590,   1314,     23, 143740,  21694,    432,    831,   2843,   1380,
          36880,   5941,   3789, 114149,    111,  67155,    136, 122084,  21968,
           8080,      4,  26719,  40368,    285,  68794,    111,  54524,   1224,
              4,    148,  50742,      7,      4,  13111,  19379,   1779,      4,
          43807, 125216,      7,      4,    136,    332,    102,  62656,      7,
              5,      2,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1]])
</code></pre>
<h2>Actual Output:</h2>
<p>I don't know what this is... <code>print(encode)</code></p>
<pre><code>
(tensor([[[[-9.3383e-02, -2.0395e-01,  4.8226e-03,  ...,  1.8068e+00,
            1.1528e-01,  7.0406e-02],
          [-4.4630e-02, -2.2453e-01,  9.5264e-02,  ...,  1.6921e+00,
            1.4607e-01,  4.8238e-02],
          [-7.8206e-01,  1.2699e-01,  1.6467e+00,  ..., -1.7057e+00,
            8.7768e-01,  8.2230e-01],
          ...,
 [-1.2145e-02, -2.1855e-03, -6.0966e-03,  ...,  2.9296e-02,
            2.2141e-03,  3.2074e-02],
          [-1.4671e-02, -2.8995e-03, -5.8610e-03,  ...,  2.8525e-02,
            2.4620e-03,  3.1593e-02],
          [-1.5877e-02, -3.5165e-03, -4.8743e-03,  ...,  2.8930e-02,
            2.9877e-03,  3.3892e-02]]]], grad_fn=&lt;CopyBackwards&gt;))

</code></pre>
"
67948945,Force BERT transformer to use CUDA,"<p>I want to force the Huggingface transformer (BERT) to make use of CUDA.
nvidia-smi showed that all my CPU cores were maxed out during the code execution, but my GPU was at 0% utilization. Unfortunately, I'm new to the Hugginface library as well as PyTorch and don't know where to place the CUDA attributes <code>device = cuda:0</code> or <code>.to(cuda:0)</code>.</p>
<p>The code below is basically a customized part from <a href=""https://huggingface.co/oliverguhr/german-sentiment-bert"" rel=""nofollow noreferrer"">german sentiment BERT working example</a></p>
<pre><code>class SentimentModel_t(pt.nn.Module):
      def __init__(self, model_name: str = &quot;oliverguhr/german-sentiment-bert&quot;):
           DEVICE = &quot;cuda:0&quot; if pt.cuda.is_available() else &quot;cpu&quot;
           print(DEVICE)
           super(SentimentModel_t,self).__init__()

           self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(DEVICE)
           self.tokenizer = BertTokenizerFast.from_pretrained(model_name)
    
        def predict_sentiment(self, texts: List[str])-&gt; List[str]:
            texts = [self.clean_text(text) for text in texts]
            # Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.
            input_ids = self.tokenizer.batch_encode_plus(texts,padding=True, add_special_tokens=True, truncation=True, max_length=self.tokenizer.max_len_single_sentence)
            input_ids = pt.tensor(input_ids[&quot;input_ids&quot;])
    
            with pt.no_grad():
                logits = self.model(input_ids)
    
            label_ids = pt.argmax(logits[0], axis=1)
    
            labels = [self.model.config.id2label[label_id] for label_id in label_ids.tolist()]
            return labels
</code></pre>
<p>EDIT: After applying the suggestions of @KonstantinosKokos (see edited code above) I got a</p>
<pre><code>RuntimeError: Input, output and indices must be on the current device
</code></pre>
<p>pointing to</p>
<pre><code>        with pt.no_grad():
           logits = self.model(input_ids)
</code></pre>
<p>The full error code can be obtained down below:</p>
<pre><code>&lt;ipython-input-15-b843edd87a1a&gt; in predict_sentiment(self, texts)
     23 
     24         with pt.no_grad():
---&gt; 25             logits = self.model(input_ids)
     26 
     27         label_ids = pt.argmax(logits[0], axis=1)

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1364         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1365 
-&gt; 1366         outputs = self.bert(
   1367             input_ids,
   1368             attention_mask=attention_mask,

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)
    859         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
    860 
--&gt; 861         embedding_output = self.embeddings(
    862             input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
    863         )

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds)
    196 
    197         if inputs_embeds is None:
--&gt; 198             inputs_embeds = self.word_embeddings(input_ids)
    199         token_type_embeddings = self.token_type_embeddings(token_type_ids)
    200 

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py in forward(self, input)
    122 
    123     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 124         return F.embedding(
    125             input, self.weight, self.padding_idx, self.max_norm,
    126             self.norm_type, self.scale_grad_by_freq, self.sparse)

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1850         # remove once script supports set_grad_enabled
   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1853 
   1854 
</code></pre>
"
67972661,Hugging Face: NameError: name 'sentences' is not defined,"<p>I am following this tutorial here: <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a> - though, I am coming across an error, and I think the tutorial is missing an import, but i do not know which.</p>
<p>These are my current imports:</p>
<pre><code># Transformers installation
! pip install transformers
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/transformers.git

! pip install datasets transformers

from transformers import pipeline
</code></pre>
<p>Current code:</p>
<pre><code>from datasets import load_dataset

raw_datasets = load_dataset(&quot;imdb&quot;)
</code></pre>
<pre><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
</code></pre>
<pre><code>inputs = tokenizer(sentences, padding=&quot;max_length&quot;, truncation=True)
</code></pre>
<p>The error:</p>
<pre><code>NameError                                 Traceback (most recent call last)

&lt;ipython-input-9-5a234f114e2e&gt; in &lt;module&gt;()
----&gt; 1 inputs = tokenizer(sentences, padding=&quot;max_length&quot;, truncation=True)

NameError: name 'sentences' is not defined
</code></pre>
"
67990545,I'm using bert pre-trained model for question and answering. It's returning correct result but with lot of spaces between the text,"<p><strong>I'm using bert pre-trained model for question and answering. It's returning correct result but with lot of spaces between the text</strong></p>
<p>The code is below :</p>
<pre><code>def get_answer_using_bert(question, reference_text):
  
  bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  input_ids = bert_tokenizer.encode(question, reference_text)
  input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)

  sep_location = input_ids.index(bert_tokenizer.sep_token_id)
  first_seg_len, second_seg_len = sep_location + 1, len(input_ids) - (sep_location + 1)
  seg_embedding = [0] * first_seg_len + [1] * second_seg_len

  model_scores = bert_model(torch.tensor([input_ids]), 
  token_type_ids=torch.tensor([seg_embedding]))
  ans_start_loc, ans_end_loc = torch.argmax(model_scores[0]), torch.argmax(model_scores[1])
  result = ' '.join(input_tokens[ans_start_loc:ans_end_loc + 1])

  result = result.replace('#', '')
  return result
</code></pre>
<p>Followed by code below :</p>
<pre><code>reference_text = 'Mukesh Dhirubhai Ambani was born on 19 April 1957 in the British Crown colony of Aden (present-day Yemen) to Dhirubhai Ambani and Kokilaben Ambani. He has a younger brother Anil Ambani and two sisters, Nina Bhadrashyam Kothari and Dipti Dattaraj Salgaonkar. Ambani lived only briefly in Yemen, because his father decided to move back to India in 1958 to start a trading business that focused on spices and textiles. The latter was originally named Vimal but later changed to Only Vimal His family lived in a modest two-bedroom apartment in Bhuleshwar, Mumbai until the 1970s. The family financial status slightly improved when they moved to India but Ambani still lived in a communal society, used public transportation, and never received an allowance. Dhirubhai later purchased a 14-floor apartment block called Sea Wind in Colaba, where, until recently, Ambani and his brother lived with their families on different floors.'
question = 'What is the name of mukesh ambani brother?'

get_answer_using_bert(question, reference_text)
</code></pre>
<p>And the output is :</p>
<pre><code>'an il am ban i'
</code></pre>
<p><strong>Can anyone help me how to fix this issue. It would be really helpful.</strong></p>
"
68006956,AttributeError: type object 'Wav2Vec2ForCTC' has no attribute 'from_pretrained',"<p>I am trying to fine tune Wav2Vec2 model for medical vocabulary. When I try to run the following code on my VS Code Jupyter notebook, I am getting an error, but when I run the same thing on Google Colab, it works fine.</p>
<pre><code>from transformers import Wav2Vec2ForCTC
 
model = Wav2Vec2ForCTC.from_pretrained(
    &quot;facebook/wav2vec2-base&quot;, 
    gradient_checkpointing=True, 
    ctc_loss_reduction=&quot;mean&quot;, 
    pad_token_id=processor.tokenizer.pad_token_id,
)
</code></pre>
<p>And here is the error that I getting on my VS Code</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-59-926a81051d7b&gt; in &lt;module&gt;
      1 from transformers import Wav2Vec2ForCTC
      2 
----&gt; 3 model = Wav2Vec2ForCTC.from_pretrained(
      4     &quot;facebook/wav2vec2-base&quot;,
      5     gradient_checkpointing=True,

AttributeError: type object 'Wav2Vec2ForCTC' has no attribute 'from_pretrained'
</code></pre>
"
68058647,Initialize HuggingFace Bert with random weights,"<p>How is it possible to initialize BERT with random weights? I want to compare the performance of multilingual vs monolingual vs randomly initialized BERT in a masked language modeling task. While in the former cases it is very straightforward:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM

tokenizer_multi = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model_multi = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')
model_multi.eval()

tokenizer_mono = BertTokenizer.from_pretrained('bert-base-cased')
model_mono = BertForMaskedLM.from_pretrained('bert-base-cased')
model_mono.eval()
</code></pre>
<p>I don't know how to load random weights.</p>
<p>Thanks in advance!</p>
"
68072148,HuggingFace SciBert AutoModelForMaskedLM cannot be imported,"<p>I am trying to use the pretrained SciBERT model (<a href=""https://huggingface.co/allenai/scibert_scivocab_uncased"" rel=""nofollow noreferrer"">https://huggingface.co/allenai/scibert_scivocab_uncased</a>) from Huggingface to evaluate masked words in scientific/biomedical text for bias using CrowS-Pairs (<a href=""https://github.com/nyu-mll/crows-pairs/"" rel=""nofollow noreferrer"">https://github.com/nyu-mll/crows-pairs/</a>).  The CrowS-Pairs code works great with the built in models like BERT.</p>
<p>I modified the code of metric.py with the goal of allowing an option of using the SciBERT model -</p>
<pre><code>import os
import csv
import json
import math
import torch
import argparse
import difflib
import logging
import numpy as np
import pandas as pd

from transformers import BertTokenizer, BertForMaskedLM
from transformers import AlbertTokenizer, AlbertForMaskedLM
from transformers import RobertaTokenizer, RobertaForMaskedLM
from transformers import AutoTokenizer, AutoModelForMaskedLM
</code></pre>
<p>and get the following error</p>
<pre><code>2021-06-21 17:11:38.626413: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File &quot;metric.py&quot;, line 15, in &lt;module&gt;
    from transformers import AutoTokenizer, AutoModelForMaskedLM
ImportError: cannot import name 'AutoModelForMaskedLM' from 'transformers' (/usr/local/lib/python3.7/dist-packages/transformers/__init__.py)
</code></pre>
<p>Later in the Python file, the AutoTokenizer and AutoModelForMaskedLM are defined as</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;) 
</code></pre>
<p>Libraries</p>
<pre><code>huggingface-hub-0.0.8
sacremoses-0.0.45
tokenizers-0.10.3
transformers-4.7.0 
</code></pre>
<p>The error occurs with and without GPU support.</p>
"
68113075,Problem with batch_encode_plus method of tokenizer,"<p>I am encountering a strange issue in the <code>batch_encode_plus</code> method of the tokenizers. I have recently switched from transformer version 3.3.0 to 4.5.1. (I am creating my databunch for NER).</p>
<p>I have 2 sentences whom I need to encode, and I have a case where the sentences are already tokenized, but since both the sentences differs in length so I need to <code>pad [PAD]</code> the shorter sentence in order to have my batch of uniform lengths.</p>
<p>Here is the code below of I did with 3.3.0 version of transformers</p>
<pre><code>from transformers import AutoTokenizer

pretrained_model_name = 'distilbert-base-cased'
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, add_prefix_space=True)

sentences = [&quot;He is an uninvited guest.&quot;, &quot;The host of the party didn't sent him the invite.&quot;]

# here we have the complete sentences
encodings = tokenizer.batch_encode_plus(sentences, max_length=20, padding=True)
batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0]))

# And the output
# [101, 1124, 1110, 1126, 8362, 1394, 5086, 1906, 3648, 119, 102, 0, 0, 0, 0]
# ['[CLS]', 'He', 'is', 'an', 'un', '##in', '##vi', '##ted', 'guest', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']

# here we have the already tokenized sentences
encodings = tokenizer.batch_encode_plus(batch_token_ids, max_length=20, padding=True, truncation=True, is_split_into_words=True, add_special_tokens=False, return_tensors=&quot;pt&quot;)

batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0])) 

# And the output 
tensor([ 101, 1124, 1110, 1126, 8362, 1394, 5086, 1906, 3648,  119,  102, 0, 0, 0, 0])
['[CLS]', 'He', 'is', 'an', 'un', '##in', '##vi', '##ted', 'guest', '.', '[SEP]', '[PAD]', [PAD]', '[PAD]', '[PAD]']
</code></pre>
<p>But if I try to mimic the same behavior in transformer version 4.5.1, I get different output</p>
<pre><code>from transformers import AutoTokenizer
    
pretrained_model_name = 'distilbert-base-cased'
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, add_prefix_space=True)

sentences = [&quot;He is an uninvited guest.&quot;, &quot;The host of the party didn't sent him the invite.&quot;]

# here we have the complete sentences
encodings = tokenizer.batch_encode_plus(sentences, max_length=20, padding=True)
batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0]))

# And the output
#[101, 1124, 1110, 1126, 8362, 1394, 5086, 1906, 3648, 119, 102, 0, 0, 0, 0]
#['[CLS]', 'He', 'is', 'an', 'un', '##in', '##vi', '##ted', 'guest', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']

# here we have the already tokenized sentences, Note we cannot pass the batch_token_ids 
# to the batch_encode_plus method in the newer version, so need to convert them to token first
tokens1 = tokenizer.tokenize(sentences[0], add_special_tokens=True)
tokens2 = tokenizer.tokenize(sentences[1], add_special_tokens=True)

encodings = tokenizer.batch_encode_plus([tokens1, tokens2], max_length=20, padding=True, truncation=True, is_split_into_words=True, add_special_tokens=False, return_tensors=&quot;pt&quot;)

batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0]))

# And the output (not the desired one)
tensor([  101,  1124,  1110,  1126,  8362,   108,   108,  1107,   108,   108,
          191,  1182,   108,   108, 21359,  1181,  3648,   119,   102])
['[CLS]', 'He', 'is', 'an', 'un', '#', '#', 'in', '#', '#', 'v', '##i', '#', '#', 'te', '##d', 'guest', '.', '[SEP]']
</code></pre>
<p>Not sure how to handle this, or what I am doing wrong here.</p>
"
68185061,Strange results with huggingface transformer[marianmt] translation of larger text,"<p>I need to translate large amounts of text from a database. Therefore, I've been dealing with transformers and models for a few days. I'm absolutely no data science expert and unfortunately I don't get any further.</p>
<p>The problem starts with longer text. The 2nd issue is the usual-maximum token size (512) of the sequencers. Just truncating is not really an option. <a href=""https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f"" rel=""nofollow noreferrer"">Here</a> I did  find a work-around, but it does not work properly and the result is a word salad on longer texts (&gt;300 sequences)</p>
<p>Here an Example <em>(please ignore the warnings, this is another issues - which does not hurt currently that much)</em>;</p>
<p>If i take the Example Sentence 2 (55 seq) or 5 times (163 sequences) - <strong>no issues.</strong></p>
<p>But it get messed up with e.g. 433 sequences (the 3rd green text block in the screenshot).</p>
<p><a href=""https://i.stack.imgur.com/IzYKf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IzYKf.png"" alt=""enter image description here"" /></a></p>
<p>With more than 510 sequences, I tried to split it up in chunks as in the upper described link. But the result here is as well pretty strange.</p>
<p>I am pretty sure - that I have more than just one mistake and underestimated this topic.
But I see no alternative (free/cheap) way for translating big amount of text.</p>
<p>Can you guys help me out? Which (thinking) errors do you see and how would you suggest to solve the issues? Thank you very much.</p>
<p><a href=""https://i.stack.imgur.com/S8jMW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S8jMW.png"" alt=""enter image description here"" /></a></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

if torch.cuda.is_available():  
  dev = &quot;cuda&quot;
else:  
  dev = &quot;cpu&quot; 
device = torch.device(dev)
 
mname = 'Helsinki-NLP/opus-mt-de-en'
tokenizer = AutoTokenizer.from_pretrained(mname)
model = AutoModelForSeq2SeqLM.from_pretrained(mname)
model.to(device)

chunksize = 512

text_short = &quot;Nach nur sieben Seiten appellierte man an die WÃ¤hlerinnen und WÃ¤hler, sich richtig zu entscheiden, nÃ¤mlich fÃ¼r Frieden, Freiheit, Sozialismus. &quot;
text_long = text_short
#this loop is just for debugging/testing and simulating long text
for x in range(30):
    text_long = text_long + text_short

tokens = tokenizer.encode_plus(text_long, return_tensors=&quot;pt&quot;, add_special_tokens=True, padding=False, truncation=False).to(device)
str_len = len(tokens['input_ids'][0])

if str_len &gt; 510:
    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)
    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))
    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))

    cnt = 1
    for tensor in input_id_chunks:
        print('\033[96m' + 'chunk ' + str(cnt) + ': ' + str(len(tensor)) + '\033[93m')
        cnt += 1
    
    # loop through each chunk
    # https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f
    for i in range(len(input_id_chunks)):
        # add CLS and SEP tokens to input IDs
        input_id_chunks[i] = torch.cat([
            torch.tensor([101]).to(device), input_id_chunks[i], torch.tensor([102]).to(device)
        ])
        # add attention tokens to attention mask
        mask_chunks[i] = torch.cat([
            torch.tensor([1]).to(device), mask_chunks[i], torch.tensor([1]).to(device)
        ])
        # get required padding length
        pad_len = chunksize - input_id_chunks[i].shape[0]
        # check if tensor length satisfies required chunk size
        if pad_len &gt; 0:
            # if padding length is more than 0, we must add padding
            input_id_chunks[i] = torch.cat([
                input_id_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
            mask_chunks[i] = torch.cat([
                mask_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
   
    input_ids = torch.stack(input_id_chunks)
    attention_mask = torch.stack(mask_chunks)
    input_dict = {'input_ids': input_ids.long(), 'attention_mask': attention_mask.int()}
    
    outputs = model.generate(**input_dict)
    #this doesnt work - following error comes to the console --&gt; &quot;host_softmax&quot; not implemented for 'Long'
    #probs = torch.nn.functional.softmax(outputs[0], dim=-1)
    # probs
    # probs = probs.mean(dim=0)
    # probs
  
else:
    tokens[&quot;input_ids&quot;] = tokens[&quot;input_ids&quot;][:, :512] #truncating normally not necessary
    tokens[&quot;attention_mask&quot;] = tokens[&quot;attention_mask&quot;][:, :512]
    outputs = model.generate(**tokens)

decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print('\033[94m' + str(str_len))
print('\033[92m' + decoded)
</code></pre>
<p>Remark; following libs are necessary:</p>
<blockquote>
<p>pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch_stable.html</a></p>
</blockquote>
<blockquote>
<p>pip install transformers</p>
</blockquote>
<blockquote>
<p>pip install sentencepiece</p>
</blockquote>
"
68196815,ModuleNotFoundError huggingface datasets in Jupyter notebook,"<p>I want to use the huggingface datasets library from within a Jupyter notebook.</p>
<p>This should be as simple as installing it (<code>pip install datasets</code>, in bash within a venv) and importing it (<code>import datasets</code>, in Python or notebook).</p>
<p>All works well when I test it in the standard Python interactive shell, however, when trying in a Jupyter notebook, it says:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-6-652e886d387f&gt; in &lt;module&gt;
----&gt; 1 import datasets

ModuleNotFoundError: No module named 'datasets'
</code></pre>
<p>At first, I thought it might be the case that the notebook kernel uses a different virtual environment, but I verified from within the notebook that the package is installed:</p>
<p><code>!pip install datasets</code></p>
<pre><code>Requirement already satisfied: datasets in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (1.8.0)
Requirement already satisfied: numpy&gt;=1.17 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (1.21.0)
Requirement already satisfied: xxhash in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (2.0.2)
Requirement already satisfied: pyarrow&lt;4.0.0,&gt;=1.0.0 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (3.0.0)
Requirement already satisfied: pandas in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (1.2.5)
Requirement already satisfied: fsspec in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (2021.6.1)
Requirement already satisfied: packaging in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (20.9)
Requirement already satisfied: dill in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (0.3.4)
Requirement already satisfied: requests&gt;=2.19.0 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (2.25.1)
Requirement already satisfied: tqdm&lt;4.50.0,&gt;=4.27 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (4.49.0)
Requirement already satisfied: multiprocess in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (0.70.12.2)
Requirement already satisfied: huggingface-hub&lt;0.1.0 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (0.0.13)
Requirement already satisfied: pytz&gt;=2017.3 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from pandas-&gt;datasets) (2021.1)
Requirement already satisfied: python-dateutil&gt;=2.7.3 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from pandas-&gt;datasets) (2.8.1)
Requirement already satisfied: pyparsing&gt;=2.0.2 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from packaging-&gt;datasets) (2.4.7)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (2021.5.30)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (4.0.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (1.26.6)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (2.10)
Requirement already satisfied: typing-extensions in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from huggingface-hub&lt;0.1.0-&gt;datasets) (3.10.0.0)
Requirement already satisfied: filelock in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from huggingface-hub&lt;0.1.0-&gt;datasets) (3.0.12)
Requirement already satisfied: six&gt;=1.5 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;datasets) (1.16.0)
</code></pre>
<p>and</p>
<p><code>!pip freeze</code></p>
<pre><code>certifi==2021.5.30
chardet==4.0.0
datasets==1.8.0
dill==0.3.4
filelock==3.0.12
fsspec==2021.6.1
huggingface-hub==0.0.13
idna==2.10
multiprocess==0.70.12.2
numpy==1.21.0
packaging==20.9
pandas==1.2.5
pyarrow==3.0.0
pyparsing==2.4.7
python-dateutil==2.8.1
pytz==2021.1
requests==2.25.1
six==1.16.0
tqdm==4.49.0
typing-extensions==3.10.0.0
urllib3==1.26.6
xxhash==2.0.2
</code></pre>
<p>Any ideas? Do I need to configure the notebook in a special way, or is there a problem with the datasets module? Thanks!</p>
<hr />
<p><strong>Edit:</strong> Following the answer below, this makes the error go away:</p>
<pre><code>datasets_dir=r&quot;/home/yoga/venvs/text_embeddings/lib/python3.8/site-packages/datasets&quot;

import sys
sys.path.append(datasets_dir)

import datasets
</code></pre>
<p>But is there a way that works without setting this path explicitely? (Or can somebody explain why this is necessary here?)</p>
"
68260614,PEGASUS From pytorch to tensorflow,"<p>I have fine-tuned PEGASUS model for abstractive  summarization using <a href=""https://gist.github.com/jiahao87/50cec29725824da7ff6dd9314b53c4b3"" rel=""nofollow noreferrer"">this script</a> which uses huggingface.
The output model is in pytorch.</p>
<p>Is there a way to transorm it into tensorflow model so I can use it in a javascript backend?</p>
"
68315780,Wrong tensor type when trying to do the HuggingFace tutorial (pytorch),"<p>I've recently been trying to get hands on experience with the transformer library from Hugging Face. Since I'm an absolute noob when it comes to using Pytorch (and Deep Learning in general), I started with the introduction that can be found <a href=""https://huggingface.co/course/chapter3?fw=pt"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Here is the code to install dependencies :</p>
<pre><code>#!pip install transformers
!pip install transformers[sentencepiece] # includes transformers dependencies
!pip install datasets # datasets from huggingface hub
!pip install tqdm
</code></pre>
<p>Here's the code they propose to use to fine-tune BERT the MNPR dataset (used in the GLUE benchmark). This dataset includes two sentences per &quot;sample&quot;, so in the tokenizer we have to use <code>sentence1</code> and <code>sentence2</code>.</p>
<pre><code>from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from torch.utils.data import DataLoader
from transformers import AutoModelForSequenceClassification
from transformers import AdamW
from transformers import get_scheduler
import torch
from tqdm.auto import tqdm

raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)
checkpoint = &quot;bert-base-uncased&quot;

# functions defining how the tokenizer works
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
  return tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True)

# tokenizer will use dynamic padding (https://huggingface.co/course/chapter3/2?fw=pt)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# remove unecessary columns from data and format in torch tensors
tokenized_datasets = tokenized_datasets.remove_columns(
  [&quot;sentence1&quot;, &quot;sentence2&quot;, &quot;idx&quot;]
)
tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;)
tokenized_datasets.set_format(&quot;torch&quot;)

train_dataloader = DataLoader(
  tokenized_datasets[&quot;train&quot;], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
  tokenized_datasets[&quot;validation&quot;], batch_size=8, collate_fn=data_collator
)

# loading model and training requirements
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
  &quot;linear&quot;,
  optimizer=optimizer,
  num_warmup_steps=0,
  num_training_steps=num_training_steps
)
print(num_training_steps)

device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
model.to(device)

progress_bar = tqdm(range(num_training_steps))

# training loop:
model.train()
for epoch in range(num_epochs):
  for batch in train_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()

    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()
    progress_bar.update(1)
    # assert 1==0
</code></pre>
<p>This works perfectly fine for me in Google Colab. I wanted to do the same thing with another dataset <code>sst2</code>. The code I use is very similar to the one above. The only few lines of code that change are the lines to import the data and the tokenizer (we have one sentence per feature instead of two). I have double-checked and the tokenizer works fine. Here is my code :</p>
<pre><code># imports
import torch
from datasets import load_dataset # datasets from huggingface
# tokenization
from transformers import AutoTokenizer, DataCollatorWithPadding
from torch.utils.data import DataLoader
# training
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
from tqdm.auto import tqdm

# Hyperparameters
batch_size = 8
learning_rate = 5e-5
num_epochs = 3
num_warmup_steps = 0

# load dataset and choosing checkpoint
raw_datasets = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)
checkpoint = &quot;bert-base-uncased&quot;
# load tokenizer
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# tokenization of dataset
def tokenize_function(example):
  return tokenizer(example[&quot;sentence&quot;], truncation=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns([&quot;sentence&quot;, &quot;idx&quot;])
tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;) 
tokenized_datasets.set_format(&quot;torch&quot;)

# setting DataLoader
train_dataloader = DataLoader(
  tokenized_datasets[&quot;train&quot;], shuffle=True, batch_size=batch_size, collate_fn=data_collator
)
eval_dataloader = DataLoader(
  tokenized_datasets[&quot;validation&quot;], batch_size=batch_size, collate_fn=data_collator
)

# import model
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)

# setup training loop
optimizer = AdamW(model.parameters(), lr=learning_rate)

num_training_steps = num_epochs * len(train_dataloader)
print(num_training_steps)
lr_scheduler = get_scheduler(
    &quot;linear&quot;,
    optimizer=optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)
# chose device (GPU or CPU)
device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
model.to(device)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
  for batch in train_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()} 
    for k,v in batch.items():
      print(f&quot;key={k},v.dtype={v.dtype}, type(v)={type(v)}&quot;)
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
        
    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()
    progress_bar.update(1)
</code></pre>
<p>And here's the error I get :</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-7893d7715ac2&gt; in &lt;module&gt;()
     69     outputs = model(**batch)
     70     loss = outputs.loss
---&gt; 71     loss.backward()
     72 
     73     optimizer.step()

1 frames
/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    147     Variable._execution_engine.run_backward(
    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,
--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
    150 
    151 

RuntimeError: Found dtype Long but expected Float
</code></pre>
<p>This seems like a very silly mistake, but like I said I'm an absolute pytorch noob and it's difficult for me to know where to start solving this issue. I have checked the type of the values in <code>batch.items()</code> and in both cases, they are all <code>torch.int64</code> (or <code>torch.long</code>). I tried to change the <code>attention_mask</code> and <code>input_ids</code> values to <code>torch.float32</code>, but I got the same error message.</p>
<p>Thanks in advance.</p>
<p>Python version and packages :</p>
<ul>
<li>python 3.7.20</li>
<li>Pytorch 1.9.0+cu102</li>
<li>transformers 4.8.2</li>
<li>GPU : Tesla T4 (also tried with tesla P4)</li>
</ul>
"
68322542,Problem connecting transformer output to CNN input in Keras,"<p>I need to build a transformer-based architecture in Tensorflow following the encoder-decoder approach where the encoder is a preexisting Huggingface Distilbert model and the decoder is a CNN.</p>
<p>Inputs: a text containing texts with several phrases in a row. Outputs: codes according to taxonomic criteria. My data file has 7387 pairs text-label in TSV format:</p>
<pre><code>text \t code
This is example text number one. It might contain some other phrases. \t C21
This is example text number two. It might contain some other phrases. \t J45.1
This is example text number three. It might contain some other phrases. \t A27
</code></pre>
<p>The remainder of the code is this:</p>
<pre><code>        text_file = &quot;data/datafile.tsv&quot;
        with open(text_file) as f:
                lines = f.read().split(&quot;\n&quot;)[:-1]
                text_and_code_pairs = []
                for line in lines:
                        text, code = line.split(&quot;\t&quot;)
                        text_and_code_pairs.append((text, code))


        random.shuffle(text_and_code_pairs)
        num_val_samples = int(0.10 * len(text_and_code_pairs))
        num_train_samples = len(text_and_code_pairs) - 3 * num_val_samples
        train_pairs = text_and_code_pairs[:num_train_samples]
        val_pairs = text_and_code_pairs[num_train_samples : num_train_samples + num_val_samples]
        test_pairs = text_and_code_pairs[num_train_samples + num_val_samples :]

        train_texts = [fst for (fst,snd) in train_pairs]
        train_labels = [snd for (fst,snd) in train_pairs]
        val_texts = [fst for (fst,snd) in val_pairs]
        val_labels = [snd for (fst,snd) in val_pairs]
        test_texts = [fst for (fst,snd) in test_pairs]
        test_labels = [snd for (fst,snd) in test_pairs]

        distilbert_encoder = TFDistilBertModel.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)
        tokenizer = DistilBertTokenizerFast.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)

        train_encodings = tokenizer(train_texts, truncation=True, padding=True)
        val_encodings = tokenizer(val_texts, truncation=True, padding=True)
        test_encodings = tokenizer(test_texts, truncation=True, padding=True)

        train_dataset = tf.data.Dataset.from_tensor_slices((
                dict(train_encodings),
                train_labels
        ))
        val_dataset = tf.data.Dataset.from_tensor_slices((
                dict(val_encodings),
                val_labels
        ))
        test_dataset = tf.data.Dataset.from_tensor_slices((
                dict(test_encodings),
                test_labels
        ))

        model = build_model(distilbert_encoder)
        model.fit(train_dataset.batch(64), validation_data=val_dataset, epochs=3, batch_size=64)
        model.predict(test_dataset, verbose=1)
</code></pre>
<p>Lastly, the <code>build_model</code> function:</p>
<pre><code>def build_model(transformer, max_len=512):
        model = tf.keras.models.Sequential()
        # Encoder
        inputs = layers.Input(shape=(max_len,), dtype=tf.int32)
        distilbert = transformer(inputs)
        # LAYER - something missing here?
        # Decoder
        conv1D = tf.keras.layers.Conv1D(filters=5, kernel_size=10)(distilbert)
        pooling = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1D)
        flat = tf.keras.layers.Flatten()(pooling)
        fc = tf.keras.layers.Dense(1255, activation='relu')(flat)
        softmax = tf.keras.layers.Dense(1255, activation='softmax')(fc)
        model = tf.keras.models.Model(inputs = inputs, outputs = softmax)
        model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5), loss=&quot;categorical_crossentropy&quot;, metrics=['accuracy'])
        print(model.summary())
        return model
</code></pre>
<p>I managed to narrow down the possible locations of my problem. After changing from sequential to functional Keras API, I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;keras_transformer.py&quot;, line 99, in &lt;module&gt;
    main()
  File &quot;keras_transformer.py&quot;, line 94, in main
    model = build_model(distilbert_encoder)
  File &quot;keras_transformer.py&quot;, line 23, in build_model
    conv1D = tf.keras.layers.Conv1D(filters=5, kernel_size=10)(distilbert)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 897, in __call__
    self._maybe_build(inputs)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 2416, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py&quot;, line 152, in build
    input_shape = tensor_shape.TensorShape(input_shape)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 771, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 771, in &lt;listcomp&gt;
    self._dims = [as_dimension(d) for d in dims_iter]
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 716, in as_dimension
    return Dimension(value)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 200, in __init__
    None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
TypeError: Dimension value must be integer or None or have an __index__ method, got 'last_hidden_state'
</code></pre>
<p>It seems that the error lies in the connection between the output of the transformer and the input of the convolutional layer. Am I supposed to include another layer between them so as to adapt the output of the transformer? If so, what would be the best option?I'm using tensorflow==2.2.0, transformers==4.5.1 and Python 3.6.9</p>
"
68343073,'Seq2SeqModelOutput' object has no attribute 'logits' BART transformers,"<p>I am trying to generate summary of long PDF. So, what I did, first I converted my pdf to text using <code>pdfminer.six</code> library. Next, I used 2 functions which were provided in a discuss <a href=""https://github.com/huggingface/transformers/issues/4224#issuecomment-694650789"" rel=""nofollow noreferrer"">here</a>.</p>
<p>The code:</p>
<pre><code>bart_tokenizer = BartTokenizer.from_pretrained(&quot;facebook/bart-large&quot;)
bart_model = BartModel.from_pretrained(&quot;facebook/bart-large&quot;, return_dict=True)

# generate chunks of text \ sentences &lt;= 1024 tokens
def nest_sentences(document):
  nested = []
  sent = []
  length = 0
  for sentence in nltk.sent_tokenize(document):
    length += len(sentence)
    if length &lt; 1024:
      sent.append(sentence)
    else:
      nested.append(sent)
      sent = [sentence]
      length = len(sentence)

  if sent:
    nested.append(sent)
  return nested

# generate summary on text with &lt;= 1024 tokens
def generate_summary(nested_sentences):
  device = 'cuda'
  summaries = []
  for nested in nested_sentences:
    input_tokenized = bart_tokenizer.encode(' '.join(nested), truncation=True, return_tensors='pt')
    input_tokenized = input_tokenized.to(device)
    summary_ids = bart_model.to(device).generate(
        input_tokenized,
        length_penalty=3.0,
        min_length=30,
        max_length=100,
    )
    output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]
    summaries.append(output)
  summaries = [sentence for sublist in summaries for sentence in sublist]
  return summaries
</code></pre>
<p>Then, to get the summary, I do:</p>
<pre><code>nested_sentences = nest_sentences(text)
</code></pre>
<p>Where, <code>text</code> is a text of string having length around 10K which I converted using pdf library.</p>
<pre><code>summary = generate_summary(nested_sentences)
</code></pre>
<p>Then, I get the following error:</p>
<pre><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-15-d5aa7709bb5f&gt; in &lt;module&gt;()
----&gt; 1 summary = generate_summary(nested_sentences)

3 frames

&lt;ipython-input-11-8554509269e0&gt; in generate_summary(nested_sentences)
     28         length_penalty=3.0,
     29         min_length=30,
---&gt; 30         max_length=100,
     31     )
     32     output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]

/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     26         def decorate_context(*args, **kwargs):
     27             with self.__class__():
---&gt; 28                 return func(*args, **kwargs)
     29         return cast(F, decorate_context)
     30 

/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)
   1061                 return_dict_in_generate=return_dict_in_generate,
   1062                 synced_gpus=synced_gpus,
-&gt; 1063                 **model_kwargs,
   1064             )
   1065 

/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)
   1799                 continue  # don't waste resources running the code we don't need
   1800 
-&gt; 1801             next_token_logits = outputs.logits[:, -1, :]
   1802 
   1803             # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`

AttributeError: 'Seq2SeqModelOutput' object has no attribute 'logits'


</code></pre>
<p>I cannot find anything related to this error, so I would really appreciate it if anyone could help or is there any better approach to generate summary for long texts?</p>
<p>Thank you in advance!</p>
"
68403128,Getting nans for gradient,"<p>I am trying to create a search relevance model where I take the dot product between query vector and resulting documents. I add a positional bias term on top to take into account the fact that position 1 is more likely to be clicked on. The final (unnormalised) log likelihood calculation is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>        query = self.query_model(query_input_ids, query_attention_mask)
        docs = self.doc_model(doc_input_ids, doc_attention_mask)
        positional_bias = self.position_model()
        
        if optimizer_idx is not None:
            if optimizer_idx == 0:
                docs = docs.detach()
                positional_bias = positional_bias.clone().detach()
            elif optimizer_idx == 1:
                query = query.detach()
                positional_bias = positional_bias.clone().detach()
            else:
                query = query.detach()
                docs = docs.detach()
                
        similarity = (docs @ query.unsqueeze(-1)).squeeze()

        click_log_lik = (similarity + positional_bias)\
                .reshape(doc_mask.shape)\
                .masked_fill_((1 - doc_mask).bool(), float(&quot;-inf&quot;))
</code></pre>
<p>The query and doc model is simply a distilbert model with a projection layer on top of CLS token. The models can be seen here: <a href=""https://pastebin.com/g21g9MG3"" rel=""nofollow noreferrer"">https://pastebin.com/g21g9MG3</a></p>
<p>When inspecting the first gradient descent step, it has <code>nan</code>s, but only for the query model and not the doc model. <strong>My hypothesis</strong> is that normalizing the return values for doc and query models (<code>return F.normalize(out, dim=-1)</code>) is somehow playing up with the gradients.</p>
<p>Does anyone know <s>1. If my hypothesis is true</s> and more importantly 2. <strong>How can I rectify nan gradients?</strong>.</p>
<h2 id=""additional-info-el5r"">Additional Info:</h2>
<ul>
<li>None of the losses are inf or nan.</li>
<li>query is BS x 768</li>
<li>docs is BS x DOC_RESULTS x 768</li>
<li>positional_bias is DOC_RESULTS</li>
<li>DOC_RESULTS is 10 in my case.</li>
<li>The <code>masked_fill</code> in the last line is because occasionally I have less than 10 data points for a query.</li>
</ul>
<h2 id=""update-1-t6xj"">Update 1</h2>
<p>The following changes made no difference to nans:</p>
<ul>
<li>Changing <code>masked_fill</code> from <code>-inf</code> to <code>1e5</code>.</li>
<li>Changing the projection from <code>F.normalize(out, dim=-1)</code> to <code>out / 100</code>.</li>
<li>Removed positional bias altogether with again no luck.</li>
</ul>
"
68421125,huggingface transformer with tensorflow saves two files as model weights,"<p>This is how I build the model for classification task:</p>
<pre><code>    def bert_for_classification(transformer_model_name, max_sequence_length, num_labels):
        config = ElectraConfig.from_pretrained(
            transformer_model_name,
            num_labels=num_labels,
            output_hidden_states=False,
            output_attentions=False
        )
        model = TFElectraForSequenceClassification.from_pretrained(transformer_model_name, config=config)
        # This is the input for the tokens themselves(words from the dataset after encoding):
        input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')

        # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.
        # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH,
        # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad
        # token:
        attention_mask = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32, name='attention_mask')

        # Use previous inputs as BERT inputs:
        output = model([input_ids, attention_mask])[0]
        output = tf.keras.layers.Dense(num_labels, activation='softmax')(output)
        model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)

        model.compile(loss=keras.losses.CategoricalCrossentropy(),
                      optimizer=keras.optimizers.Adam(3e-05, epsilon=1e-08),
                      metrics=['accuracy'])

        return model
</code></pre>
<p>After I trained this model I save it using <code>model.save_weights('model.hd5')</code>
But it turns out there are two files that are saved: <code>model.hd5.index</code> and <code>model.hd5.data-00000-of-00001</code></p>
<p>How should I load this model from the disk?</p>
"
68444252,Multiple training with huggingface transformers will give exactly the same result except for the first time,"<p>I have a function that will load a pre-trained model from huggingface and fine-tune it for sentiment analysis then calculates the F1 score and returns the result.
The problem is when I call this function multiple times with the exact same arguments, it will give the exact same metric score which is expected, except for the first time which is different, how is that possible?</p>
<p>This is my function which is written based on <a href=""https://huggingface.co/course/chapter3/3?fw=pt"" rel=""noreferrer"">this tutorial</a> in huggingface:</p>
<pre class=""lang-py prettyprint-override""><code>import uuid

import numpy as np

from datasets import (
    load_dataset,
    load_metric,
    DatasetDict,
    concatenate_datasets
)

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

CHECKPOINT = &quot;distilbert-base-uncased&quot;
SAVING_FOLDER = &quot;sst2&quot;
def custom_train(datasets, checkpoint=CHECKPOINT, saving_folder=SAVING_FOLDER):

    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    
    def tokenize_function(example):
        return tokenizer(example[&quot;sentence&quot;], truncation=True)

    tokenized_datasets = datasets.map(tokenize_function, batched=True)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    saving_folder = f&quot;{SAVING_FOLDER}_{str(uuid.uuid1())}&quot;
    training_args = TrainingArguments(saving_folder)

    trainer = Trainer(
        model,
        training_args,
        train_dataset=tokenized_datasets[&quot;train&quot;],
        eval_dataset=tokenized_datasets[&quot;validation&quot;],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    trainer.train()
    
    predictions = trainer.predict(tokenized_datasets[&quot;test&quot;])
    print(predictions.predictions.shape, predictions.label_ids.shape)
    preds = np.argmax(predictions.predictions, axis=-1)
    
    metric_fun = load_metric(&quot;f1&quot;)
    metric_result = metric_fun.compute(predictions=preds, references=predictions.label_ids)
    
    return metric_result
</code></pre>
<p>And then I will run this function several times with the same datasets, and append the result of the returned F1 score each time:</p>
<pre class=""lang-py prettyprint-override""><code>raw_datasets = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)

small_datasets = DatasetDict({
    &quot;train&quot;: raw_datasets[&quot;train&quot;].select(range(100)).flatten_indices(),
    &quot;validation&quot;: raw_datasets[&quot;validation&quot;].select(range(100)).flatten_indices(),
    &quot;test&quot;: raw_datasets[&quot;validation&quot;].select(range(100, 200)).flatten_indices(),
})

results = []
for i in range(4):
    result = custom_train(small_datasets)
    results.append(result)
</code></pre>
<p>And then when I check the results list:</p>
<pre><code>[{'f1': 0.7755102040816325}, {'f1': 0.5797101449275361}, {'f1': 0.5797101449275361}, {'f1': 0.5797101449275361}]
</code></pre>
<p>Something that may come to mind is that when I load a pre-trained model, the head will be initialized with random weights and that is why the results are different, if that is the case, why only the first one is different and the others are exactly the same?</p>
"
68461204,Continual pre-training vs. Fine-tuning a language model with MLM,"<p>I have some custom data I want to use to <em><strong>further pre-train</strong></em> the BERT model. Iâ€™ve tried the two following approaches so far:</p>
<ol>
<li>Starting with a pre-trained BERT checkpoint and continuing the pre-training with Masked Language Modeling (<code>MLM</code>) + Next Sentence Prediction (<code>NSP</code>) heads (e.g. using <em><strong>BertForPreTraining</strong></em> model)</li>
<li>Starting with a pre-trained BERT model with the <code>MLM</code> objective (e.g. using the <em><strong>BertForMaskedLM</strong></em> model assuming we donâ€™t need NSP for the pretraining part.)</li>
</ol>
<p>But Iâ€™m still confused that if using either <em>BertForPreTraining</em> or <em>BertForMaskedLM</em> actually does the continual pre-training on BERT or these are just two models for fine-tuning that use MLM+NSP and MLM for fine-tuning BERT, respectively. Is there even any difference between fine-tuning BERT with MLM+NSP or continually pre-train it using these two heads or this is something we need to test?</p>
<p>I've reviewed similar questions such as <a href=""https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"">this one</a> but still, I want to make sure that whether technically there's a difference between continual pre-training a model from an initial checkpoint and fine-tuning it using the same objective/head.</p>
"
68481189,Huggingface AutoTokenizer cannot be referenced when importing Transformers,"<p>I am trying to import AutoTokenizer and AutoModelWithLMHead, but I am getting the following error:</p>
<p>ImportError: cannot import name 'AutoTokenizer' from partially initialized module 'transformers' (most likely due to a circular import)</p>
<p>First, I install transformers: <code>pip install transformers</code> then implemented the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelWithLMHead

tokenizer = AutoTokenizer.from_pretrained(&quot;t5-base&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;t5-base&quot;)
</code></pre>
"
68489759,Huggingface NER with custom data,"<p>I have a csv data as below.</p>
<pre><code>**token**      **label**
0.45&quot;      length
1-12       size
2.6&quot;       length
8-9-78     size
6mm        length
</code></pre>
<p>Whenever I get the text as below</p>
<pre><code>6mm 8-9-78 silver head
</code></pre>
<p>I should be able to say <code>length = 6mm</code> and <code>size = 8-9-78</code>. I'm new to NLP world, I'm trying to solve this using Huggingface NER. I have gone through various articles. I'm not getting how to train with my own data. Which <code>model/tokeniser</code> should I make use of? Or should I build my own? Any help would be appreciated.</p>
"
68494108,Hyperparam search on huggingface with optuna fails with wandb error,"<p>I'm using this simple script, using the example blog post. However, it fails because of <code>wandb</code>. It was of no use to make <code>wandb</code> OFFLINE as well.</p>
<pre><code>from datasets import load_dataset, load_metric
from transformers import (AutoModelForSequenceClassification, AutoTokenizer,
                          Trainer, TrainingArguments)
import wandb


wandb.init()

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
dataset = load_dataset('glue', 'mrpc')
metric = load_metric('glue', 'mrpc')

def encode(examples):
    outputs = tokenizer(
        examples['sentence1'], examples['sentence2'], truncation=True)
    return outputs

encoded_dataset = dataset.map(encode, batched=True)

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        'distilbert-base-uncased', return_dict=True)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Evaluate during training and a bit more often
# than the default to be able to prune bad trials early.
# Disabling tqdm is a matter of preference.
training_args = TrainingArguments(
    &quot;test&quot;, eval_steps=500, disable_tqdm=True,
    evaluation_strategy='steps',)

trainer = Trainer(
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[&quot;validation&quot;],
    model_init=model_init,
    compute_metrics=compute_metrics,
)

def my_hp_space(trial):
    return {
        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-4, 1e-2, log=True),
        &quot;weight_decay&quot;: trial.suggest_float(&quot;weight_decay&quot;, 0.1, 0.3),
        &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 5, 10),
        &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 20, 40),
        &quot;per_device_train_batch_size&quot;: trial.suggest_categorical(&quot;per_device_train_batch_size&quot;, [32, 64]),
    }


trainer.hyperparameter_search(
    direction=&quot;maximize&quot;,
    backend=&quot;optuna&quot;,
    n_trials=10,
    hp_space=my_hp_space
)
</code></pre>
<p><code>Trail 0</code> finishes successfully, but next <code>Trail 1</code> crashes with following error:</p>
<pre><code>  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/integrations.py&quot;, line 138, in _objective
    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1376, in train
    self.log(metrics)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1688, in log
    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer_callback.py&quot;, line 371, in on_log
    return self.call_event(&quot;on_log&quot;, args, state, control, logs=logs)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer_callback.py&quot;, line 378, in call_event
    result = getattr(callback, event)(
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/integrations.py&quot;, line 754, in on_log
    self._wandb.log({**logs, &quot;train/global_step&quot;: state.global_step})
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/wandb/sdk/lib/preinit.py&quot;, line 38, in preinit_wrapper
    raise wandb.Error(&quot;You must call wandb.init() before {}()&quot;.format(name))
wandb.errors.Error: You must call wandb.init() before wandb.log()
</code></pre>
<p>Any help is highly appreciated.</p>
"
68536546,using pipelines with a local model,"<p>I am trying to use a simple <code>pipeline</code> offline. I am only allowed to download files directly from the web.</p>
<p>I went to <a href=""https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main</a> and downloaded all the files in a local folder <code>C:\\Users\\me\\mymodel</code></p>
<p>However, when I tried to load the model I get a strange error</p>
<pre><code>from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model= &quot;C:\\Users\\me\\mymodel&quot;,
                      tokenizer = &quot;C:\\Users\\me\\mymodel&quot;)

ValueError: unable to parse C:\Users\me\mymodel\modelcard.json as a URL or as a local path
</code></pre>
<p>What is the issue here?
Thanks!</p>
"
68557028,Setting `remove_unused_columns=False` causes error in HuggingFace Trainer class,"<p>I am training a model using HuggingFace Trainer class. The following code does a decent job:</p>
<pre><code>!pip install datasets
!pip install transformers

from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer

dataset = load_dataset('glue', 'mnli')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)

def preprocess_function(examples):
  return tokenizer(examples[&quot;premise&quot;], examples[&quot;hypothesis&quot;], truncation=True, padding=True)
encoded_dataset = dataset.map(preprocess_function, batched=True)

args = TrainingArguments(
    &quot;test-glue&quot;,
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    remove_unused_columns=True
  )

trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    tokenizer=tokenizer
)
trainer.train()
</code></pre>
<p>However, setting <code>remove_unused_columns=False</code> results in the following error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in convert_to_tensors(self, tensor_type, prepend_batch_axis)
    704                 if not is_tensor(value):
--&gt; 705                     tensor = as_tensor(value)
    706 

ValueError: too many dimensions 'str'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
8 frames
/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in convert_to_tensors(self, tensor_type, prepend_batch_axis)
    720                     )
    721                 raise ValueError(
--&gt; 722                     &quot;Unable to create tensor, you should probably activate truncation and/or padding &quot;
    723                     &quot;with 'padding=True' 'truncation=True' to have batched tensors with the same length.&quot;
    724                 )

ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.
</code></pre>
<p>Any suggestions are highly appreciated.</p>
"
68577198,pytorch summary fails with huggingface model,"<p>I want a summary of a <code>PyTorch</code> model downloaded from huggingface.</p>
<p>Am I doing something wrong here?</p>
<pre><code>from torchinfo import summary
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
summary(model, input_size=(16, 512))
</code></pre>
<p>Gives the error:</p>
<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    257             if isinstance(x, (list, tuple)):
--&gt; 258                 _ = model.to(device)(*x, **kwargs)
    259             elif isinstance(x, dict):

11 frames

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1530             output_hidden_states=output_hidden_states,
-&gt; 1531             return_dict=return_dict,
   1532         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    988             inputs_embeds=inputs_embeds,
--&gt; 989             past_key_values_length=past_key_values_length,
    990         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)
    214         if inputs_embeds is None:
--&gt; 215             inputs_embeds = self.word_embeddings(input_ids)
    216         token_type_embeddings = self.token_type_embeddings(token_type_ids)

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py in forward(self, input)
    159             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 160             self.norm_type, self.scale_grad_by_freq, self.sparse)
    161 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2042         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2043     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2044 

RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)


The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-8-4f70d4e6fa82&gt; in &lt;module&gt;()
      5 else:
      6     # Can't get this working
----&gt; 7     summary(model, input_size=(16, 512)) #, device='cpu')
      8     #print(model)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in summary(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)
    190     )
    191     summary_list = forward_pass(
--&gt; 192         model, x, batch_dim, cache_forward_pass, device, **kwargs
    193     )
    194     formatting = FormattingOptions(depth, verbose, col_names, col_width, row_settings)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    268             &quot;Failed to run torchinfo. See above stack traces for more details. &quot;
    269             f&quot;Executed layers up to: {executed_layers}&quot;
--&gt; 270         ) from e
    271     finally:
    272         if hooks is not None:

RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []
</code></pre>
"
68585678,"pytorch summary fails with huggingface model II: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu","<p>I want a summary of a PyTorch model downloaded from huggingface:</p>
<pre><code>from torchinfo import summary
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
summary(model, input_size=(16, 512), dtypes=['torch.IntTensor'])
</code></pre>
<p>(See <a href=""https://stackoverflow.com/a/68577755/1349673"">SO</a> for why the <code>dtypes</code> is needed.)</p>
<p>However, I am getting the error <code>Expected all tensors to be on the same device, ...</code> even though I have not provided any tensors. See the output below.</p>
<p>How can I fix this?</p>
<pre><code>

---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    257             if isinstance(x, (list, tuple)):
--&gt; 258                 _ = model.to(device)(*x, **kwargs)
    259             elif isinstance(x, dict):

11 frames

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1530             output_hidden_states=output_hidden_states,
-&gt; 1531             return_dict=return_dict,
   1532         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    988             inputs_embeds=inputs_embeds,
--&gt; 989             past_key_values_length=past_key_values_length,
    990         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)
    214         if inputs_embeds is None:
--&gt; 215             inputs_embeds = self.word_embeddings(input_ids)
    216         token_type_embeddings = self.token_type_embeddings(token_type_ids)

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py in forward(self, input)
    159             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 160             self.norm_type, self.scale_grad_by_freq, self.sparse)
    161 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2042         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2043     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2044 

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)


The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-13-d6f4e53beef7&gt; in &lt;module&gt;()
      3 else:
      4     # Can't get this working. See https://stackoverflow.com/questions/68577198/pytorch-summary-fails-with-huggingface-model
----&gt; 5     summary(model, input_size=(16, 512), dtypes=['torch.IntTensor'])
      6     print(model)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in summary(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)
    190     )
    191     summary_list = forward_pass(
--&gt; 192         model, x, batch_dim, cache_forward_pass, device, **kwargs
    193     )
    194     formatting = FormattingOptions(depth, verbose, col_names, col_width, row_settings)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    268             &quot;Failed to run torchinfo. See above stack traces for more details. &quot;
    269             f&quot;Executed layers up to: {executed_layers}&quot;
--&gt; 270         ) from e
    271     finally:
    272         if hooks is not None:

RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []

</code></pre>
<p>Output from <code>transformers-cli</code>:</p>
<pre><code>- `transformers` version: 4.9.1
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.11
- PyTorch version (GPU?): 1.9.0+cu102 (True)
- Tensorflow version (GPU?): 2.5.0 (True)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: &lt;fill in&gt;
- Using distributed or parallel set-up in script?: &lt;fill in&gt;
</code></pre>
"
68604289,AttributeError: module transformers has no attribute TFGPTNeoForCausalLM,"<p>I cloned this repository/documentation <a href=""https://huggingface.co/EleutherAI/gpt-neo-125M"" rel=""nofollow noreferrer"">https://huggingface.co/EleutherAI/gpt-neo-125M</a></p>
<p>I get the below error whether I run it on google collab or locally. I also installed transformers using this</p>
<pre><code>pip install git+https://github.com/huggingface/transformers
</code></pre>
<p>and made sure the configuration file is named as config.json</p>
<pre><code>      5 tokenizer = AutoTokenizer.from_pretrained(&quot;gpt-neo-125M/&quot;,from_tf=True)
----&gt; 6 model = AutoModelForCausalLM.from_pretrained(&quot;gpt-neo-125M&quot;,from_tf=True)
      7 
      8 

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in __getattr__(self, name)

AttributeError: module transformers has no attribute TFGPTNeoForCausalLM

</code></pre>
<p>Full code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM 

tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

model = AutoModelForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

</code></pre>
<p>transformers-cli env results:</p>
<ul>
<li><code>transformers</code> version: 4.10.0.dev0</li>
<li>Platform: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.29</li>
<li>Python version: 3.8.5</li>
<li>PyTorch version (GPU?): 1.9.0+cpu (False)</li>
<li>Tensorflow version (GPU?): 2.5.0 (False)</li>
<li>Flax version (CPU?/GPU?/TPU?): not installed (NA)</li>
<li>Jax version: not installed</li>
<li>JaxLib version: not installed</li>
<li>Using GPU in script?: </li>
<li>Using distributed or parallel set-up in script?: </li>
</ul>
<p>Both collab and locally have TensorFlow 2.5.0 version</p>
"
68624392,Running out of memory with pytorch,"<p>I am trying to train a model using huggingface's wav2vec for audio classification. I keep getting this error:</p>
<pre><code>The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: name, emotion, path.
***** Running training *****
  Num examples = 2708
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 2
  Total optimization steps = 42
 [ 2/42 : &lt; :, Epoch 0.02/1]
Step    Training Loss   Validation Loss

RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 61, in _worker
    output = module(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;&lt;ipython-input-81-dd9fe3ea0f13&gt;&quot;, line 77, in forward
    return_dict=return_dict,
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 1073, in forward
    return_dict=return_dict,
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 732, in forward
    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 574, in forward
    hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 510, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/functional.py&quot;, line 1555, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 10.49 GiB already allocated; 11.44 MiB free; 10.68 GiB reserved in total by PyTorch)
</code></pre>
<p>I'm on an AWS ubuntu deep learning AMI ec2.</p>
<p>I've been researching this a lot. I've already tried:</p>
<ul>
<li>reducing the batch size (I want 4, but I've gone down to 1 with no change in error)</li>
<li>adding:
<pre><code>import gc
gc.collect()
torch.cuda.empty_cache()
</code></pre>
</li>
<li>removing all wav files in my dataset that are longer than 6 seconds</li>
</ul>
<p>Is there anything else I can do? I'm on a p2.8xlarge dataset with 105 GiB mounted.</p>
<p>Running <code>torch.cuda.memory_summary(device=None, abbreviated=False) </code> gives me:</p>
<pre><code>|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 3            |        cudaMalloc retries: 4         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |    7550 MB |   10852 MB |  209624 MB |  202073 MB |\n|       from large pool |    7544 MB |   10781 MB |  209325 MB |  201780 MB |\n|       from small pool |       5 MB |      87 MB |     298 MB |     293 MB |\n|---------------------------------------------------------------------------|\n| Active memory         |    7550 MB |   10852 MB |  209624 MB |  202073 MB |\n|       from large pool |    7544 MB |   10781 MB |  209325 MB |  201780 MB |\n|       from small pool |       5 MB |      87 MB |     298 MB |     293 MB |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |   10936 MB |   10960 MB |   63236 MB |   52300 MB |\n|       from large pool |   10928 MB |   10954 MB |   63124 MB |   52196 MB |\n|       from small pool |       8 MB |      98 MB |     112 MB |     104 MB |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |  443755 KB |    1309 MB |  155426 MB |  154992 MB |\n|       from large pool |  443551 KB |    1306 MB |  155081 MB |  154648 MB |\n|       from small pool |     204 KB |      12 MB |     344 MB |     344 MB |\n|---------------------------------------------------------------------------|\n| Allocations           |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| Active allocs         |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |     495    |     495    |    2169    |    1674    |\n|       from large pool |     491    |     491    |    2113    |    1622    |\n|       from small pool |       4    |      49    |      56    |      52    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |     179    |     335    |   15998    |   15819    |\n|       from large pool |     165    |     272    |   12420    |   12255    |\n|       from small pool |      14    |      63    |    3578    |    3564    |\n|===========================================================================|\n'
</code></pre>
<p>After reducing data only to inputs that are less tahn 2 seconds in length, it trains a lot further but still errors with this:</p>
<pre><code>The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path, emotion, name.
***** Running training *****
  Num examples = 1411
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 2
  Total optimization steps = 22
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 [11/22 01:12 &lt; 01:28, 0.12 it/s, Epoch 0.44/1]
Step    Training Loss   Validation Loss Accuracy
10  2.428100    2.257138    0.300283
The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path, emotion, name.
***** Running Evaluation *****
  Num examples = 353
  Batch size = 32
Saving model checkpoint to trainingArgs/checkpoint-10
Configuration saved in trainingArgs/checkpoint-10/config.json
Model weights saved in trainingArgs/checkpoint-10/pytorch_model.bin
Configuration saved in trainingArgs/checkpoint-10/preprocessor_config.json
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
--&gt; 379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
    380                 return

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in _save(obj, zip_file, pickle_module, pickle_protocol)
    498         num_bytes = storage.size() * storage.element_size()
--&gt; 499         zip_file.write_record(name, storage.data_ptr(), num_bytes)
    500 

OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-25-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1334                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
   1335 
-&gt; 1336                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1337                 else:
   1338                     self.control = self.callback_handler.on_substep_end(args, self.state, self.control)

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1441 
   1442         if self.control.should_save:
-&gt; 1443             self._save_checkpoint(model, trial, metrics=metrics)
   1444             self.control = self.callback_handler.on_save(self.args, self.state, self.control)
   1445 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _save_checkpoint(self, model, trial, metrics)
   1531         elif self.args.should_save and not self.deepspeed:
   1532             # deepspeed.save_checkpoint above saves model/optim/sched
-&gt; 1533             torch.save(self.optimizer.state_dict(), os.path.join(output_dir, &quot;optimizer.pt&quot;))
   1534             with warnings.catch_warnings(record=True) as caught_warnings:
   1535                 torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, &quot;scheduler.pt&quot;))

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
    379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
--&gt; 380                 return
    381         _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
    382 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in __exit__(self, *args)
    257 
    258     def __exit__(self, *args) -&gt; None:
--&gt; 259         self.file_like.write_end_of_file()
    260         self.buffer.flush()
    261 

RuntimeError: [enforce fail at inline_container.cc:298] . unexpected pos 1849920000 vs 1849919888
</code></pre>
<p>When I run <code>!free</code> in the notebook, I get:</p>
<pre><code>The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.
              total        used        free      shared  buff/cache   available
Mem:      503392908     6223452   478499292      346492    18670164   492641984
Swap:             0           0           0
</code></pre>
<p>For training code, I am essentially running this colab notebook as an example:
<a href=""https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=6M8bNvLLJnG1"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=6M8bNvLLJnG1</a></p>
<p>All that I am changing is the incoming data/labels, which I have intentionally fit into the same directory structure used in the tutorial notebook. The tutorial notebook runs fine for some reason, even though my data has comparable size/num classes.</p>
"
68732271,RuntimeError: CUDA error: device-side assert triggered - BART model,"<p>I am trying to run BART language model for a text generation task.</p>
<p>My code was working fine when I used for another encoder-decoder model (T5), but with bart I am getting this error:</p>
<pre><code>File &quot;train_bart.py&quot;, line 89, in train
    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)                                                     cs-lab-host1&quot; 12:39 10-Aug-21
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1308, in forward
    return_dict=return_dict,
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1196, in forward
    return_dict=return_dict,
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 985, in forward
    attention_mask, input_shape, inputs_embeds, past_key_values_length
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 866, in _prepare_decoder_attent
ion_mask
    ).to(self.device)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>And this is where error happens:</p>
<pre><code>for _, data in tqdm(enumerate(loader, 0), total=len(loader), desc='Processing batches..'):
    y = data['target_ids'].to(device, dtype = torch.long)
    y_ids = y[:, :-1].contiguous()
    lm_labels = y[:, 1:].clone().detach()
    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
    ids = data['source_ids'].to(device, dtype = torch.long)
    mask = data['source_mask'].to(device, dtype = torch.long)

    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)
    loss = outputs[0]
</code></pre>
<p><code>loader</code> is the tokenized and processed data.</p>
"
68742863,Error while trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER,"<p>I'm trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER. I used the padding sequence length same as in the encode method (max_length = max([len(string) for string in list_of_strings])) along with attention_masks. And I got this error:</p>
<p><strong>ValueError: If training, make sure that config.axial_pos_shape factors: (128, 512) multiply to sequence length. Got prod((128, 512)) != sequence_length: 2248. You might want to consider padding your sequence length to 65536 or changing config.axial_pos_shape.</strong></p>
<ul>
<li>When I changed the sequence length to 65536, my colab session crashed by getting all the inputs of 65536 lengths.</li>
<li>According to the second option(changing config.axial_pos_shape), I cannot change it.</li>
</ul>
<p>I would like to know, Is there any chance to change config.axial_pos_shape while fine-tuning the model? Or I'm missing something in encoding the input strings for reformer-enwik8?</p>
<p>Thanks!</p>
<p><strong>Question Update: I have tried the following methods:</strong></p>
<ol>
<li>By giving paramteres at the time of model instantiation:</li>
</ol>
<blockquote>
<p>model = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9, max_position_embeddings=1024, axial_pos_shape=[16,64], axial_pos_embds_dim=[32,96],hidden_size=128)</p>
</blockquote>
<p>It gives me the following error:</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for ReformerModelWithLMHead:
size mismatch for reformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([258, 1024]) from checkpoint, the shape in current model is torch.Size([258, 128]).
size mismatch for reformer.embeddings.position_embeddings.weights.0: copying a param with shape torch.Size([128, 1, 256]) from checkpoint, the shape in current model is torch.Size([16, 1, 32]).</p>
</blockquote>
<p>This is quite a long error.</p>
<ol start=""2"">
<li>Then I tried this code to update the config:</li>
</ol>
<blockquote>
<p>model1 = transformers.ReformerModelWithLMHead.from_pretrained('google/reformer-enwik8', num_labels = 9)</p>
</blockquote>
<h4>Reshape Axial Position Embeddings layer to match desired max seq length</h4>
<pre><code>model1.reformer.embeddings.position_embeddings.weights[1] = torch.nn.Parameter(model1.reformer.embeddings.position_embeddings.weights[1][0][:128])
</code></pre>
<h4>Update the config file to match custom max seq length</h4>
<pre><code>model1.config.axial_pos_shape = 16,128
model1.config.max_position_embeddings = 16*128 #2048
model1.config.axial_pos_embds_dim= 32,96
model1.config.hidden_size = 128
output_model_path = &quot;model&quot;
model1.save_pretrained(output_model_path)
</code></pre>
<p>By this implementation, I am getting this error:</p>
<blockquote>
<p>RuntimeError: The expanded size of the tensor (512) must match the existing size (128) at non-singleton dimension 2.  Target sizes: [1, 128, 512, 768].  Tensor sizes: [128, 768]</p>
</blockquote>
<p>Because updated size/shape doesn't match with the original config parameters of pretrained model. The original parameters are: axial_pos_shape = 128,512 max_position_embeddings = 128*512 #65536 axial_pos_embds_dim= 256,768 hidden_size = 1024</p>
<p>Is it the right way I'm changing the config parameters or do I have to do something else?</p>
<p>Is there any example where ReformerModelWithLMHead('google/reformer-enwik8') model fine-tuned.</p>
<p>My main code implementation is as follow:</p>
<pre><code>class REFORMER(torch.nn.Module):
def __init__(self):
    super(REFORMER, self).__init__()
    self.l1 = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9)

def forward(self, input_ids, attention_masks, labels):
    output_1= self.l1(input_ids, attention_masks, labels = labels)
    return output_1


model = REFORMER()

def train(epoch):
    model.train()
    for _, data in enumerate(training_loader,0):
        ids = data['input_ids'][0]   # input_ids from encode method of the model https://huggingface.co/google/reformer-enwik8#:~:text=import%20torch%0A%0A%23%20Encoding-,def%20encode,-(list_of_strings%2C%20pad_token_id%3D0
        input_shape = ids.size()
        targets = data['tags']
        print(&quot;tags: &quot;, targets, targets.size())
        least_common_mult_chunk_length = 65536 
        padding_length = least_common_mult_chunk_length - input_shape[-1] % least_common_mult_chunk_length
        #pad input 
        input_ids, inputs_embeds, attention_mask, position_ids, input_shape = _pad_to_mult_of_chunk_length(self=model.l1,
                input_ids=ids,
                inputs_embeds=None,
                attention_mask=None,
                position_ids=None,
                input_shape=input_shape,
                padding_length=padding_length,
                padded_seq_length=None,
                device=None,
            )
        outputs = model(input_ids, attention_mask, labels=targets) # sending inputs to the forward method
        print(outputs)
        loss = outputs.loss
        logits = outputs.logits
        if _%500==0:
           print(f'Epoch: {epoch}, Loss:  {loss}')

for epoch in range(1):
    train(epoch)
</code></pre>
"
68757944,Is there a way to use a pre-trained transformers model without the configuration file?,"<p>I would like to fine-tune a pre-trained transformers model on Question Answering. The model was pre-trained on large engineering &amp; science related corpora.</p>
<p>I have been provided a &quot;checkpoint.pt&quot; file containing the weights of the model. They have also provided me with a &quot;bert_config.json&quot; file but I am not sure if this is the correct configuration file.</p>
<pre><code>from transformers import AutoModel, AutoTokenizer, AutoConfig

MODEL_PATH = &quot;./checkpoint.pt&quot;
config = AutoConfig.from_pretrained(&quot;./bert_config.json&quot;)
model = AutoModel.from_pretrained(MODEL_PATH, config=config)
</code></pre>
<p>The reason I believe that bert_config.json doesn't match &quot;./checkpoint.pt&quot; file is that,  when I load the model with the code above, I get the error that goes as below.</p>
<blockquote>
<p>Some weights of the model checkpoint at ./aerobert/phase2_ckpt_4302592.pt were not used when initializing BertModel: ['files', 'optimizer', 'model', 'master params']</p>
<ul>
<li>This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</li>
<li>This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at ./aerobert/phase2_ckpt_4302592.pt and are newly initialized: ['encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', ............</li>
</ul>
</blockquote>
<p>If I am correct in assuming that &quot;bert_config.json&quot; is not the correct one, is there a way to load this model correctly without the config.json file?</p>
<p>Is there a way to see the model architecture from the saved weights of checkpoint.pt file?</p>
"
68759885,Print input / output / grad / loss at every step/epoch when training Transformers HuggingFace model,"<p>I'm working on HuggingFace Transformers and using toy example from here:
<a href=""https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer</a></p>
<p>What I actually need: ability to print input, output, grad and loss at every step.
It is trivial using Pytorch training loop, but it is not obvious using HuggingFace <code>Trainer</code>.
At the current moment I have next idea: create a <code>CustomCallback</code> like this:</p>
<pre><code>class MyCallback(TrainerCallback):
    &quot;A callback that prints a grad at every step&quot;

    def on_step_begin(self, args, state, control, **kwargs):
        print(&quot;next step&quot;)
        print(kwargs['model'].classifier.out_proj.weight.grad.norm())

args = TrainingArguments(
    output_dir='test_dir',
    overwrite_output_dir=True,
    num_train_epochs=1,
    logging_steps=100,
    report_to=&quot;none&quot;,
    fp16=True,
    disable_tqdm=True,
)


trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    callbacks=[MyCallback],
)

trainer.train()
</code></pre>
<p>This way I can print grad and weights for any model layer.
But I still can't figure out how to print input/output (for example, I want to check them on <code>nan</code>) and loss?</p>
<p>P.S. I also read something about <code>forward_hook</code> but still can't find good code examples for it.</p>
"
68813979,"Bert Transformer ""Size Error"" while Machine Traslation","<p>I am getting desperate as I have no clue what is the problem over here. I want to translate a list of sentences from german to english. This is my code:</p>
<pre><code>
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;
)



results = model(batch)
</code></pre>
<p>And I am getting this error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/miniconda3/envs/textmallet/lib/python3.9/site-packages/transformers/tokenization_utils_base.py in __getattr__(self, item)
    247         try:
--&gt; 248             return self.data[item]
    249         except KeyError:

KeyError: 'size'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_26502/2652187977.py in &lt;module&gt;
     14 
     15 
---&gt; 16 results = model(batch)
     17 

~/miniconda3/envs/textmallet/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/textmallet/lib/python3.9/site-packages/transformers/models/marian/modeling_marian.py in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1274                 )
   1275 
-&gt; 1276         outputs = self.model(
   1277             input_ids,
   1278             attention_mask=attention_mask,
</code></pre>
<p>I have no clue what could be the precise issue over here. If someone can help me out I d be really thankful.</p>
"
68817989,Bert model output interpretation,"<p>I searched a lot for this but havent still got a clear idea so I hope you can help me out:</p>
<p>I am trying to translate german texts to english! I udes this code:</p>
<pre><code>
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]

results = model(batch)  
</code></pre>
<p>Which returned me a size error! I fixed this problem (thanks to the community: <a href=""https://github.com/huggingface/transformers/issues/5480"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5480</a>) with switching the last line of code to:</p>
<pre><code>results = model(input_ids = batch,decoder_input_ids=batch)
</code></pre>
<p>Now my output looks like a really long array. What is this output precisely? Are these some sort of word embeddings? And if yes: How shall I go on with converting these embeddings to the texts in the english language? Thanks alot!</p>
"
68829277,Transfer Learning using HuggingFace and Tensorflow with AutoModel does not work,"<p>I try to do transfer learning using a <code>HuggingFace</code> pretrained BERT model. I want to use tensorflow API with it. I do not understand why the last line produces an error</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

model_name = &quot;distilbert-base-uncased&quot;
text = &quot;this is a test&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)    
text_tensor = tokenizer.encode(text, return_tensors=&quot;tf&quot;)

model = AutoModel.from_pretrained(model_name).to(&quot;cuda&quot;)
output = model(text_tensor) # ERROR!!, but why?
</code></pre>
"
68835630,TypeError when trying to apply custom loss in a multilabel classification problem,"<p>I am trying to solve a multilabel text classification problem using BERT from huggingface transformers library. The model is defined as follows:</p>
<pre><code>def create_model(encoder, nb_classes=3, lr=1e-5):

    # inputs
    input_ids = tf.keras.Input(shape=(512,), ragged=False,
                               dtype=tf.int32, name='input_ids')
    input_attention_mask = tf.keras.Input(shape=(512,), ragged=False,
                                          dtype=tf.int32, name='attention_mask')
    # transformer
    output = encoder({'input_ids': input_ids, 
                      'attention_mask': input_attention_mask})[0]
    Y = tf.keras.layers.BatchNormalization()(output)
    Y = tf.keras.layers.Dense(nb_classes, activation='sigmoid')(Y)

    # compilation
    model = tf.keras.Model(inputs=[input_ids, input_attention_mask], 
                           outputs=[Y])
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    # losses
    # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)

    model.compile(optimizer=optimizer, 
                  loss=multilabel_loss, metrics=['acc'])
    model.summary()
    return model
</code></pre>
<p>As you can see, I tried to use tf.keras.losses, but it did not work (throwing <code>AttributeError: 'Tensor' object has no attribute 'nested_row_splits'</code>), so I defined a simple cross entropy by hand:</p>
<pre><code>def multilabel_loss(y_true, y_pred):
    y_pred = tf.convert_to_tensor(y_pred)
    y_true = tf.cast(y_true, y_pred.dtype)
    cross_entropy = -tf.reduce_sum((y_true*tf.math.log(y_pred + 1e-8) + (1 - y_true) * tf.math.log(1 - y_pred + 1e-8)),
                                   name='xentropy')
    return cross_entropy
</code></pre>
<p>The model is created with strategy.scope() as shown below, using 'distil-bert-uncased' as a checkpoint:</p>
<pre><code>with strategy.scope():
    encoder = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
    #encoder = TFRobertaForSequenceClassification.from_pretrained(checkpoint)
    model = create_model(encoder)
</code></pre>
<p>The labels are binary arrays:</p>
<pre><code>163350    [0, 0, 1]
118940    [0, 0, 1]
65243     [0, 0, 1]
30011     [0, 0, 1]
189713    [0, 1, 0]
</code></pre>
<p>They are combined with tokenized texts into a tf.dataset in a next function:</p>
<pre><code>def tf_text_data_prep(df):
    &quot;&quot;&quot;
    input: takes pandas dataframe
    output: returns tokenized tf.Dataset
    &quot;&quot;&quot;
    hugging_ds = Dataset.from_pandas(df)
    tokenized_ds = hugging_ds.map(
                      tokenize_function,
                      batched=True,
                      num_proc=strategy.num_replicas_in_sync,
                      remove_columns=[&quot;Text&quot;, '__index_level_0__'],
                      load_from_cache_file=True 
                      )
    
    # Convert to tensorflow
    tf_dataset = tokenized_ds.with_format(&quot;tensorflow&quot;)
    features = {x: tf_dataset[x].to_tensor() for x in tokenizer.model_input_names}
    tf_data = tf.data.Dataset.from_tensor_slices((features, tf_dataset[&quot;label&quot;]))
    return tf_data
</code></pre>
<p>The problem is when I launch the training, I get the error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-62-720b4634d50e&gt; in &lt;module&gt;()
----&gt; 1 get_ipython().run_cell_magic('time', '', 'steps_per_epoch = int(BUFFER_SIZE // BATCH_SIZE)\nprint(\n    f&quot;Model Params:\\nbatch_size: {BATCH_SIZE}\\nEpochs: {EPOCHS}\\n&quot;\n    f&quot;Step p. Epoch: {steps_per_epoch}\\n&quot;\n    f&quot;Initial Learning rate: {INITAL_LEARNING_RATE}&quot;\n)\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1,\n)')

12 frames
/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell)
   2115             magic_arg_s = self.var_expand(line, stack_depth)
   2116             with self.builtin_trap:
-&gt; 2117                 result = fn(magic_arg_s, cell)
   2118             return result
   2119 

&lt;decorator-gen-53&gt; in time(self, line, cell, local_ns)

/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py in &lt;lambda&gt;(f, *a, **k)
    186     # but it's overkill for just that one bit of state.
    187     def magic_deco(arg):
--&gt; 188         call = lambda f, *a, **k: f(*a, **k)
    189 
    190         if callable(arg):

/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns)
   1191         else:
   1192             st = clock2()
-&gt; 1193             exec(code, glob, local_ns)
   1194             end = clock2()
   1195             out = None

&lt;timed exec&gt; in &lt;module&gt;()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1176                 _r=1):
   1177               callbacks.on_train_batch_begin(step)
-&gt; 1178               tmp_logs = self.train_function(iterator)
   1179               if data_handler.should_sync:
   1180                 context.async_wait()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--&gt; 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--&gt; 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    762     self._concrete_stateful_fn = (
    763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 764             *args, **kwds))
    765 
    766     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3048       args, kwargs = None, None
   3049     with self._lock:
-&gt; 3050       graph_function, _ = self._maybe_define_function(args, kwargs)
   3051     return graph_function
   3052 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-&gt; 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3287             arg_names=arg_names,
   3288             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3289             capture_by_value=self._capture_by_value),
   3290         self._function_attributes,
   3291         function_spec=self.function_spec,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--&gt; 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    670         # the function a weak reference to itself to avoid a reference cycle.
    671         with OptionalXlaContext(compile_with_xla):
--&gt; 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

TypeError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:850 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:840 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:833 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:460 update_state
        metric_obj.update_state(y_t, y_p, sample_weight=mask)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:86 decorated
        update_op = update_state_fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:177 update_state_fn
        return ag_update_state(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:659 update_state  **
        [y_true, y_pred], sample_weight)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:546 ragged_assert_compatible_and_get_flat_values
        raise TypeError('One of the inputs does not have acceptable types.')

    TypeError: One of the inputs does not have acceptable types.
</code></pre>
<p>This same approach worked for ordinary binary classification, but not for multilabel.
I'd appreciate any help regarding the error or the approach in general.</p>
"
68850172,Token indices sequence length Issue,"<p>I am running a sentence transformer model and trying to truncate my tokens, but it doesn't appear to be working. My code is</p>
<pre><code>from transformers import AutoModel, AutoTokenizer
model_name = &quot;sentence-transformers/paraphrase-MiniLM-L6-v2&quot;
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
    
text_tokens = tokenizer(text, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
text_embedding = model(**text_tokens)[&quot;pooler_output&quot;]
</code></pre>
<p>I keep getting the following warning:</p>
<pre><code>Token indices sequence length is longer than the specified maximum sequence length 
for this model (909 &gt; 512). Running this sequence through the model will result in 
indexing errors
</code></pre>
<p>I am wondering why setting <code>truncation=True</code> is not truncating my text to the desired length?</p>
"
68875496,HUGGINGFACE TypeError: '>' not supported between instances of 'NoneType' and 'int',"<p>I am working on Fine-Tuning Pretrained Model on custom (using HuggingFace) dataset I will copy all code correctly from the one youtube video everything is ok but in this cell/code:</p>
<pre><code>with training_args.strategy.scope():
    model=TFDistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

    trainer = TFTrainer(model=model,     # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset)           # evaluation dataset


    trainer.train()
</code></pre>
<p>It will give me this error:</p>
<pre><code>TypeError: '&gt;' not supported between instances of 'NoneType' and 'int'
</code></pre>
"
68907519,Bert with Padding and Masked Token Predicton,"<p>I am Playing around with Bert Pretrained Models (bert-large-uncased-whole-word-masking)
I used Huggingface to try it I first Used this Piece of Code</p>
<pre><code>m = TFBertLMHeadModel.from_pretrained(&quot;bert-large-cased-whole-word-masking&quot;)
logits = m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;)[&quot;input_ids&quot;]).logits
</code></pre>
<p>I then used Argmax to get max probabilities after applying softmax,
Things works fine Until now.</p>
<p>When I used padding with max_length = 100 The model started making false prediction and not working well and all predicted tokens were the same i.e 119-Token ID</p>
<p>Code I used for Argmax</p>
<pre><code>tf.argmax(tf.keras.activations.softmax(m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;,max_length=,padding=&quot;max_length&quot;)[&quot;input_ids&quot;]).logits)[0],axis=-1)
</code></pre>
<p>Output Before using padding</p>
<pre><code>&lt;tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 9800, 19082,  1362,   146,  1176,  1122,   119])&gt;
</code></pre>
<p>Output After using padding with max_length of 100</p>
<pre><code>&lt;tf.Tensor: shape=(100,), dtype=int64, numpy=
array([119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119])&gt;
</code></pre>
<p>I wonder if this problem prevail even training a new model as It is mandatory to set Input shape for training new model I Padded and tokenized the data but, now I want to know if this problem continues with it too.</p>
"
68918962,HuggingFace-Transformers --- NER single sentence/sample prediction,"<p>I am trying to predict with the NER model, as in the tutorial from huggingface (it contains only the training+evaluation part).</p>
<p>I am following this exact tutorial here : <a href=""https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb</a></p>
<p>The training works flawlessly, but the problems that I have begin when I try to predict on a simple sample.</p>
<pre><code>model_checkpoint = &quot;distilbert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
loaded_model = AutoModel.from_pretrained('./my_model_own_custom_training.pth',
                                         from_tf=False)



input_sentence = &quot;John Nash is a great mathematician, he lives in France&quot;
tokenized_input_sentence = tokenizer([input_sentence],
                                     truncation=True, 
                                     is_split_into_words=False,
                                     return_tensors='pt')
predictions = loaded_model(tokenized_input_sentence[&quot;input_ids&quot;])[0]
</code></pre>
<p>Predictions is of shape <code>(1,13,768)</code></p>
<p>How can I arrive at the final result of the form <code>[JOHN &lt;-&gt; â€˜B-PERâ€™, â€¦ France &lt;-&gt; â€œB-LOCâ€]</code>, where <code>B-PER</code> and <code>B-LOC</code> are two ground truth labels, representing the tag for a person and location respectively?</p>
<p>The result of the prediction is:</p>
<pre><code>torch.Size([1, 13, 768])
</code></pre>
<p>If I write:</p>
<pre><code>print(predictions.argmax(axis=2))
tensor([613, 705, 244, 620, 206, 206, 206, 620, 620, 620, 477, 693, 308])
</code></pre>
<p>I get the tensor above.</p>
<p>However I would have expected to get the tensor representing the ground truth <code>[0â€¦8]</code> labels from the ground truth annotations.</p>
<p>Summary when loading the model :</p>
<blockquote>
<pre><code>loading configuration file ./my_model_own_custom_training.pth/config.json
Model config DistilBertConfig {
â€œname_or_path&quot;: â€œdistilbert-base-uncasedâ€,
â€œactivationâ€: â€œgeluâ€,
â€œarchitecturesâ€: [
â€œDistilBertForTokenClassificationâ€
],
â€œattention_dropoutâ€: 0.1,
â€œdimâ€: 768,
â€œdropoutâ€: 0.1,
â€œhidden_dimâ€: 3072,
â€œid2labelâ€: {
â€œ0â€: â€œLABEL_0â€,
â€œ1â€: â€œLABEL_1â€,
â€œ2â€: â€œLABEL_2â€,
â€œ3â€: â€œLABEL_3â€,
â€œ4â€: â€œLABEL_4â€,
â€œ5â€: â€œLABEL_5â€,
â€œ6â€: â€œLABEL_6â€,
â€œ7â€: â€œLABEL_7â€,
â€œ8â€: â€œLABEL_8â€
},
â€œinitializer_rangeâ€: 0.02,
â€œlabel2idâ€: {
â€œLABEL_0â€: 0,
â€œLABEL_1â€: 1,
â€œLABEL_2â€: 2,
â€œLABEL_3â€: 3,
â€œLABEL_4â€: 4,
â€œLABEL_5â€: 5,
â€œLABEL_6â€: 6,
â€œLABEL_7â€: 7,
â€œLABEL_8â€: 8
},
â€œmax_position_embeddingsâ€: 512,
â€œmodel_typeâ€: â€œdistilbertâ€,
â€œn_headsâ€: 12,
â€œn_layersâ€: 6,
â€œpad_token_idâ€: 0,
â€œqa_dropoutâ€: 0.1,
â€œseq_classif_dropoutâ€: 0.2,
â€œsinusoidal_pos_embdsâ€: false,
&quot;tie_weightsâ€: true,
â€œtransformers_versionâ€: â€œ4.8.1â€,
â€œvocab_sizeâ€: 30522
}
</code></pre>
</blockquote>
"
68928299,multiclass sequence classifiaction with fastai and huggingface,"<p>I am looking to implement DistilBERT via fastai and huggingface for a mutliclass sequence classification problem. I found a useful tutorial that gave a good example on how to do this with binary classification. The code is below:</p>
<pre><code># !pip install torch==1.9.0
# !pip install torchtext==0.10
# !pip install transformers==4.7
# !pip install fastai==2.4

from fastai.text.all import *
from sklearn.model_selection import train_test_split
import pandas as pd
import glob
from transformers import AutoTokenizer, AutoModelForSequenceClassification


hf_tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
hf_model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

&quot;&quot;&quot;
train_df and val_df looks like this:

      label text
4240    5   whoa interesting.
13      7   you could you could we just
4639    4   you set the goal,
28      1   because ive already agreed to that
66      8   oh hey freshman thats you gona need
&quot;&quot;&quot;

print(list(train_df.label.value_counts().index))
&quot;&quot;&quot;
[4, 1, 5, 6, 7, 0, 2, 3, 8]
&quot;&quot;&quot;

class HF_Dataset(torch.utils.data.Dataset):
    def __init__(self, df, hf_tokenizer):
        self.df = df
        self.hf_tokenizer = hf_tokenizer
        
        self.label_map = {
            0:0,
            1:0,
            2:0,
            3:0,
            4:1,
            5:1,
            6:1,
            7:1,
            8:1
        }
        
    def __len__(self):
        return len(self.df)

    def decode(self, token_ids):
        return ' '.join([hf_tokenizer.decode(x) for x in tokenizer_outputs['input_ids']])
    
    def decode_to_original(self, token_ids):
        return self.hf_tokenizer.decode(token_ids.squeeze())

    def __getitem__(self, index):
        label, text = self.df.iloc[index]
        label = self.label_map[label]
        label = torch.tensor(label)

        tokenizer_output = self.hf_tokenizer(text, return_tensors=&quot;pt&quot;, padding='max_length', truncation=True, max_length=512)
        
        tokenizer_output['input_ids'].squeeze_()
        tokenizer_output['attention_mask'].squeeze_()
        
        return tokenizer_output, label
        

train_dataset = HF_Dataset(train_df, hf_tokenizer)
valid_dataset = HF_Dataset(valid_df, hf_tokenizer)

train_dl = DataLoader(train_dataset, bs=16, shuffle=True)
valid_dl = DataLoader(valid_dataset, bs=16)
dls = DataLoaders(train_dl, valid_dl)
hf_model(**batched_data)


class HF_Model(nn.Module):
  
    def __init__(self, hf_model):
        super().__init__()
        
        self.hf_model = hf_model
        
    def forward(self, tokenizer_outputs):
        
        model_output = self.hf_model(**tokenizer_outputs)
        
        return model_output.logits
        
model = HF_Model(hf_model)
# Manually popping the model onto the gpu since the data is in a dictionary format
# (doesn't automatically place model + data on gpu otherwise)
learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=[accuracy])
learn.fit_one_cycle(3, 1e-4)
</code></pre>
<p>This works fine. However, I mapped my multiclass labels to 2 labels to allow this to work. I actually have 9 classes. I tried adjusting the label mapping scheme in <code>HF_Dataset()</code> class to match my actual labels like below:</p>
<pre><code>class HF_Dataset(torch.utils.data.Dataset):
    def __init__(self, df, hf_tokenizer):
        self.df = df
        self.hf_tokenizer = hf_tokenizer
        
        self.label_map = {
            0:0,
            1:1,
            2:2,
            3:3,
            4:4,
            5:5,
            6:6,
            7:7,
            8:8
        }
        
    def __len__(self):
        return len(self.df)

    def decode(self, token_ids):
        return ' '.join([hf_tokenizer.decode(x) for x in tokenizer_outputs['input_ids']])
    
    def decode_to_original(self, token_ids):
        return self.hf_tokenizer.decode(token_ids.squeeze())

    def __getitem__(self, index):
        label, text = self.df.iloc[index]
        label = self.label_map[label]
        label = torch.tensor(label)

        tokenizer_output = self.hf_tokenizer(text, return_tensors=&quot;pt&quot;, padding='max_length', truncation=True, max_length=512)
        
        tokenizer_output['input_ids'].squeeze_()
        tokenizer_output['attention_mask'].squeeze_()
        
        return tokenizer_output, label
</code></pre>
<p>Every line works until <code>learn.fit_one_cycle</code>.</p>
<p>Here is the full stack trace from this line:</p>
<pre><code>
 0.00% [0/3 00:00&lt;00:00]
epoch   train_loss  valid_loss  accuracy    time

 0.00% [0/519 00:00&lt;00:00]
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-21-0ec2ff9e12e1&gt; in &lt;module&gt;
----&gt; 1 learn.fit_one_cycle(3, 1e-4)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)
    111     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),
    112               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}
--&gt; 113     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)
    114 
    115 # Cell

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)
    219             self.opt.set_hypers(lr=self.lr if lr is None else lr)
    220             self.n_epoch = n_epoch
--&gt; 221             self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
    222 
    223     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_fit(self)
    210         for epoch in range(self.n_epoch):
    211             self.epoch=epoch
--&gt; 212             self._with_events(self._do_epoch, 'epoch', CancelEpochException)
    213 
    214     def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_epoch(self)
    204 
    205     def _do_epoch(self):
--&gt; 206         self._do_epoch_train()
    207         self._do_epoch_validate()
    208 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_epoch_train(self)
    196     def _do_epoch_train(self):
    197         self.dl = self.dls.train
--&gt; 198         self._with_events(self.all_batches, 'train', CancelTrainException)
    199 
    200     def _do_epoch_validate(self, ds_idx=1, dl=None):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in all_batches(self)
    167     def all_batches(self):
    168         self.n_iter = len(self.dl)
--&gt; 169         for o in enumerate(self.dl): self.one_batch(*o)
    170 
    171     def _do_one_batch(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in one_batch(self, i, b)
    192         b = self._set_device(b)
    193         self._split(b)
--&gt; 194         self._with_events(self._do_one_batch, 'batch', CancelBatchException)
    195 
    196     def _do_epoch_train(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_one_batch(self)
    173         self('after_pred')
    174         if len(self.yb):
--&gt; 175             self.loss_grad = self.loss_func(self.pred, *self.yb)
    176             self.loss = self.loss_grad.clone()
    177         self('after_loss')

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py in forward(self, input, target)
   1119     def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
   1120         return F.cross_entropy(input, target, weight=self.weight,
-&gt; 1121                                ignore_index=self.ignore_index, reduction=self.reduction)
   1122 
   1123 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)
   2822     if size_average is not None or reduce is not None:
   2823         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2824     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2825 
   2826 

IndexError: Target 6 is out of bounds.
</code></pre>
<p>This seems like it should be a simple fix. Do I need to adjust something in the model architecture to allow it to accept 9 labels? Or do I need to one hot encode my labels? If so, is there a solution prebuilt to do this in the pipeline?</p>
"
68961546,Get the index of subwords produced by BertTokenizer (in transformers library),"<p><code>BertTokenizer</code> can tokenize a sentence to a list of tokens, where some long words e.g. &quot;embeddings&quot; is splitted to several subwords i.e. 'em', '##bed', '##ding', and '##s'.</p>
<p><strong>Is there a way to locate the subwords?</strong> For example,</p>
<pre><code>t = BertTokenizer.from_pretrained('bert-base-uncased')

tokens = t('word embeddings', add_special_tokens=False)
location = locate_subwords(tokens)
</code></pre>
<p>I want the <code>location</code> be like <code>[0, 1, 1, 1, 1]</code> corresponding to <code>['word', 'em', '##bed', '##ding', '##s']</code>, where 0 means normal word, 1 means subword.</p>
"
68988378,Jupyter notebook progress bar issue in TensorFlow method,"<p>When using <code>TFAutoModel.from_pretrained()</code> the following error is returned</p>
<pre class=""lang-py prettyprint-override""><code>~/opt/anaconda3/envs/contracts/lib/python3.8/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols)
    110         # Prepare IPython progress bar
    111         if IProgress is None:  # #187 #451 #558 #872
--&gt; 112             raise ImportError(
    113                 &quot;IProgress not found. Please update jupyter and ipywidgets.&quot;
    114                 &quot; See https://ipywidgets.readthedocs.io/en/stable&quot;

ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
</code></pre>
<p>I've followed these links, manually installed tqdm, IProgress, and ipywidgets. For whatever reason, this TensorFlow object method is unable to execute. Any recommendations?</p>
"
69046964,"Can BERT output be fixed in shape, irrespective of string size?","<p>I am confused about using huggingface BERT models and about how to make them yield a prediction at a fixed shape, regardless of input size (i.e., input string length).</p>
<p>I tried to call the tokenizer with the parameters <code>padding=True, truncation=True, max_length = 15</code>, but the prediction output dimensions for <code>inputs = [&quot;a&quot;, &quot;a&quot;*20, &quot;a&quot;*100, &quot;abcede&quot;*20000]</code> are not fixed. What am I missing here?</p>
<pre><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)

inputs = [&quot;a&quot;, &quot;a&quot;*20, &quot;a&quot;*100, &quot;abcede&quot;*20000]
for input in inputs:
  inputs = tokenizer(input, padding=True, truncation=True, max_length = 15, return_tensors=&quot;pt&quot;)
  outputs = model(**inputs)
  print(outputs.last_hidden_state.shape, input, len(input))
</code></pre>
<p>output:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
torch.Size([1, 3, 768]) a 1
torch.Size([1, 12, 768]) aaaaaaaaaaaaaaaaaaaa 20
torch.Size([1, 15, 768]) aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa 100
torch.Size([1, 3, 768]) abcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcededeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeab....deabbcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcede 120000
</code></pre>
"
69087044,Early stopping in Bert Trainer instances,"<p>I am fine tuning a BERT model for a multiclass classification task. My problem is that I don't know how to add &quot;early stopping&quot; to those Trainer instances. Any ideas?</p>
"
69159507,Load a model as DPRQuestionEncoder in HuggingFace,"<p>I would like to load the BERT's weights (or whatever transformer) into a <a href=""https://huggingface.co/transformers/model_doc/dpr.html#transformers.DPRQuestionEncoder"" rel=""nofollow noreferrer"">DPRQuestionEncoder</a> architecture, such that I can use the HuggingFace <em>save_pretrained</em> method and plug the saved model into the <a href=""https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag"" rel=""nofollow noreferrer"">RAG architecture to do end-to-end fine-tuning</a>.</p>
<pre><code>from transformers import DPRQuestionEncoder
model = DPRQuestionEncoder.from_pretrained('bert-base-uncased')
</code></pre>
<p>But I got the following error</p>
<pre><code>You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.

NotImplementedErrorTraceback (most recent call last)
&lt;ipython-input-27-1f1b990b906b&gt; in &lt;module&gt;
----&gt; 1 model = DPRQuestionEncoder.from_pretrained(model_name)
      2 # https://github.com/huggingface/transformers/blob/41cd52a768a222a13da0c6aaae877a92fc6c783c/src/transformers/models/dpr/modeling_dpr.py#L520

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1211                     )
   1212 
-&gt; 1213             model, missing_keys, unexpected_keys, error_msgs = cls._load_state_dict_into_model(
   1214                 model, state_dict, pretrained_model_name_or_path, _fast_init=_fast_init
   1215             )

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, _fast_init)
   1286             )
   1287             for module in unintialized_modules:
-&gt; 1288                 model._init_weights(module)
   1289 
   1290         # copy state_dict so _load_from_state_dict can modify it

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _init_weights(self, module)
    515         Initialize the weights. This method should be overridden by derived class.
    516         &quot;&quot;&quot;
--&gt; 517         raise NotImplementedError(f&quot;Make sure `_init_weigths` is implemented for {self.__class__}&quot;)
    518 
    519     def tie_weights(self):

NotImplementedError: Make sure `_init_weigths` is implemented for &lt;class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'&gt;
</code></pre>
<p>I am using the last version of Transformers.</p>
"
69196995,Using Hugging-face transformer with arguments in pipeline,"<p>I am working on using a transformer. Pipeline to get BERT embeddings to my input. using this without a pipeline i am able to get constant outputs but not with pipeline since I was not able to pass arguments to it.</p>
<p>How can I pass transformer-related arguments for my Pipeline?</p>
<pre class=""lang-py prettyprint-override""><code># These are BERT and tokenizer definitions
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

inputs = ['hello world']

# Normally I would do something like this to initialize the tokenizer and get the result with constant output
tokens = tokenizer(inputs,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model(**tokens)[0].detach().numpy().shape


# using the pipeline 
pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# or other option
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

nlp=pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# to call the pipeline
nlp(&quot;hello world&quot;)
</code></pre>
<p>I have tried several ways like the options listed above but was not able to get results with constant output size. one can achieve constant output size by setting the tokenizer arguments but have no idea how to give arguments for the pipeline.</p>
<p>any idea?</p>
"
69239925,TypeError in torch.argmax() when want to find the tokens with the highest `start` score,"<p>I want to run this code for question answering using hugging face transformers.</p>
<pre><code>import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer

#Model
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = '''Why was the student group called &quot;the Methodists?&quot;'''

paragraph = ''' The movement which would become The United Methodist Church began in the mid-18th century within the Church of England.
            A small group of students, including John Wesley, Charles Wesley and George Whitefield, met on the Oxford University campus.
            They focused on Bible study, methodical study of scripture and living a holy life.
            Other students mocked them, saying they were the &quot;Holy Club&quot; and &quot;the Methodists&quot;, being methodical and exceptionally detailed in their Bible study, opinions and disciplined lifestyle.
            Eventually, the so-called Methodists started individual societies or classes for members of the Church of England who wanted to live a more religious life. '''
            
encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)

inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens

start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(start_scores)
</code></pre>
<p>but I get this error at the last line:</p>
<pre><code>Exception has occurred: TypeError
argmax(): argument 'input' (position 1) must be Tensor, not str
  File &quot;D:\bert\QuestionAnswering.py&quot;, line 33, in &lt;module&gt;
    start_index = torch.argmax(start_scores)
</code></pre>
<p>I don't know what's wrong. can anyone help me?</p>
"
69266293,Getting embeddings from wav2vec2 models in HuggingFace,"<p>I am trying to get the embeddings from pre-trained wav2vec2 models (e.g., from jonatasgrosman/wav2vec2-large-xlsr-53-german) using my own dataset.</p>
<p>My aim is to use these features for a downstream task (not specifically speech recognition). Namely, since the dataset is relatively small, I would train an SVM with these embeddings for the final classification.</p>
<p>So far I have tried this:</p>
<pre><code>model_name = &quot;facebook/wav2vec2-large-xlsr-53-german&quot;
feature_extractor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2Model.from_pretrained(model_name)

input_values = feature_extractor(train_dataset[:10][&quot;speech&quot;], return_tensors=&quot;pt&quot;, padding=True, 
                                 feature_size=1, sampling_rate=16000 ).input_values 
</code></pre>
<p>Then, I am not sure whether the embeddings here correspond to the sequence of last_hidden_states:</p>
<pre><code>hidden_states = model(input_values).last_hidden_state
</code></pre>
<p>or to the sequence of features of the last conv layer of the model:</p>
<pre><code>features_last_cnn_layer = model(input_values).extract_features
</code></pre>
<p>Also, is this the correct way to extract features from a pre-trained model?</p>
<p>How one can get embeddings from a specific layer?</p>
<p>PD: Posting here as the HuggingFace's forum seems to be less active.</p>
"
69343395,Hugging Face H5 load model error : No model found in config file,"<p>I'm trying to load model from Hugging Face and I downloaded h5 model from here: <a href=""https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main</a></p>
<pre><code>from flask import Flask, jsonify, request  # import objects from the Flask model
from keras.models import load_model
from transformers import AutoTokenizer, AutoModelForSequenceClassification,TextClassificationPipeline

model = load_model('./tf_model.h5') # trying to load model here
</code></pre>
<p>And the error shows up:</p>
<pre><code>File &quot;C:\D\Learning\Flask\flask-pp-rest\main.py&quot;, line 11, in &lt;module&gt;
    model = load_model('./tf_model.h5') File &quot;C:\Users\ndrez\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\saving\save.py&quot;,
line 200, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, File
&quot;C:\Users\ndrez\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\saving\hdf5_format.py&quot;,
line 176, in load_model_from_hdf5
    raise ValueError('No model found in config file.') ValueError: **No model found in config file.**
</code></pre>
<p>Does anyone know how to solve this? If you know please help me out. I will monitor this question and try to implement your solution's answer.</p>
"
69364068,"Loading tf.keras model, ValueError: The two structures don't have the same nested structure","<p>I created a <code>tf.keras model</code> that has <strong>BERT</strong> and I want to train and save it for further use.
Loading this model is a big issue cause I keep getting error: <code>ValueError: The two structures don't have the same nested structure.</code></p>
<p>I simplified the model a lot, to see where is the problem exactly. The code is pretty simple:</p>
<pre><code>bert = TFBertModel.from_pretrained(&quot;bert-base-german-cased&quot;)

model_name = &quot;Model&quot;
txt12_input_ids = tf.keras.layers.Input(shape=(max_length,),  name='txt12_input_ids', dtype='int32')
txt12_mask      = tf.keras.layers.Input(shape=(max_length,),  name='txt12_mask', dtype='int32')
txt12_outputs = bert(txt12_input_ids, txt12_mask).pooler_output

model_K = tf.keras.Model(inputs=(txt12_input_ids,  txt12_mask), outputs=txt12_outputs, name=model_name)
model_K.compile(optimizer=Adam(1e-5), loss=&quot;binary_crossentropy&quot;, metrics=&quot;accuracy&quot;)


model_K.save(dir_path+'Prob')
model_2 = tf.keras.models.load_model(dir_path+'Prob')
</code></pre>
<p><em>Some notes before you start replying:</em></p>
<ol>
<li><p>I did specified <code>dtype</code>.</p>
</li>
<li><p>No, I don't want to save just weights.</p>
</li>
<li><p>I tried to use <code>tf.keras.models.save_model(model_K, dir_path+'Prob')</code> instead and it gives the same error.</p>
</li>
</ol>
<p>And the last thing, I work with <code>tf version: 2.6.0</code>. Does anyone knows how to solve it?</p>
<p>Full error message:</p>
<pre><code>ValueError: The two structures don't have the same nested structure.

First structure: type=tuple str=(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, None, False), {})

Second structure: type=tuple str=((TensorSpec(shape=(None, 120), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 120), dtype=tf.int32, name='attention_mask'), None, None, None, None, None, None, None, False), {})

More specifically: Substructure &quot;type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}&quot; is a sequence, while substructure &quot;type=TensorSpec str=TensorSpec(shape=(None, 120), dtype=tf.int32, name='input_ids')&quot; is not
Entire first structure:
(({'input_ids': .}, ., ., ., ., ., ., ., ., .), {})
Entire second structure:
((., ., ., ., ., ., ., ., ., .), {})
</code></pre>
"
69433514,Test Intel Extension for Pytorch(IPEX) in multiple-choice from huggingface / transformers,"<p>I am trying out one huggingface sample with SWAG dataset
<a href=""https://github.com/huggingface/transformers/tree/master/examples/pytorch/multiple-choice"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/pytorch/multiple-choice</a></p>
<p>I would like to use Intel Extension for Pytorch in my code to increase the performance.</p>
<p>Here I am using the one without training (run_swag_no_trainer)</p>
<p>In the run_swag_no_trainer.py , I made some changes to use ipex .
#Code before changing is given below:</p>
<pre><code>device = accelerator.device
model.to(device)
</code></pre>
<p>#After adding ipex:</p>
<pre><code>import intel_pytorch_extension as ipex
    device = ipex.DEVICE
    model.to(device)
</code></pre>
<p>While running the below command, its taking too much time.</p>
<pre><code>export DATASET_NAME=swag

accelerate launch run_swag_no_trainer.py \
  --model_name_or_path bert-base-cased \
  --dataset_name $DATASET_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$DATASET_NAME/
</code></pre>
<p>Is there any other method to test the same on intel ipex?</p>
"
69459301,NER using spaCy & Transformers - different result when running inside and outside of a loop,"<p>I am using NER (spacy &amp; Transformer) for finding and anonymizing personal information. I noticed that the output I get when giving an input line directly is different than when the input line is read from a file (see screenshot below). Does anyone have suggestions on how to fix this?</p>
<p><a href=""https://i.stack.imgur.com/DZbaQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZbaQ.png"" alt=""enter image description here"" /></a></p>
<p>Here is my code:</p>
<pre><code>import pandas as pd
import csv
import spacy
from spacy import displacy
from transformers import pipeline
import re

!python -m spacy download en_core_web_trf
nlp = spacy.load('en_core_web_trf')

sent = nlp('Yesterday I went out with Andrew, johanna and Jonathan Sparow.')
displacy.render(sent, style = 'ent')

with open('Synth_dataset_raw.txt', 'r') as fd:
    reader = csv.reader(fd)
    for row in reader:
        sent = nlp(str(row))
        displacy.render(sent, style = 'ent')
</code></pre>
"
69473082,ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler',"<p>I am attempting to issue this statement in a jupyter Notebook.</p>
<pre><code>from transformers import BertForQuestionAnswering
</code></pre>
<p>I get the error:</p>
<blockquote>
<p>ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (C:\Users\sbing.conda\envs\Tensorflow\lib\site-packages\torch\optim\lr_scheduler.py)</p>
</blockquote>
<p>Here is the complete stack:</p>
<blockquote>
<p>ImportError                               Traceback (most recent call last)
 in 
----&gt; 1 from transformers import BertForQuestionAnswering</p>
<p>~.conda\envs\Tensorflow\lib\site-packages\transformers_<em>init</em>_.py in 
624
625     # Trainer
--&gt; 626     from .trainer import Trainer
627     from .trainer_pt_utils import torch_distributed_zero_first
628 else:</p>
<p>~.conda\envs\Tensorflow\lib\site-packages\transformers\trainer.py in 
67     TrainerState,
68 )
---&gt; 69 from .trainer_pt_utils import (
70     DistributedTensorGatherer,
71     SequentialDistributedSampler,</p>
<p>~.conda\envs\Tensorflow\lib\site-packages\transformers\trainer_pt_utils.py in 
38     SAVE_STATE_WARNING = &quot;&quot;
39 else:
---&gt; 40     from torch.optim.lr_scheduler import SAVE_STATE_WARNING
41
42 logger = logging.get_logger(<strong>name</strong>)</p>
<p>ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (C:\Users\sbing.conda\envs\Tensorflow\lib\site-packages\torch\optim\lr_scheduler.py)</p>
</blockquote>
"
69480199,pad_token_id not working in hugging face transformers,"<p>I want to download the GPT-2 model and tokeniser. For open-end generation, HuggingFace sets the padding token ID to be equal to the end-of-sentence token ID, so I configured it manually using :</p>
<pre><code>
import tensorflow as tf
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
model = TFGPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;, pad_token_id=tokenizer.eos_token_id)

</code></pre>
<p>However, it gives me the following error:</p>
<blockquote>
<p>TypeError: ('Keyword argument not understood:', 'pad_token_id')</p>
</blockquote>
<p>I haven't been able to find a solution for this nor do I understand why I am getting this error. Insights will be appreciated.</p>
"
69517460,BERT get sentence embedding,"<p>I am replicating code from <a href=""https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX"" rel=""noreferrer"">this page</a>. I have downloaded the BERT model to my local system and getting sentence embedding.</p>
<p>I have around 500,000 sentences for which I need sentence embedding and it is taking a lot of time.</p>
<ol>
<li>Is there a way to expedite the process?</li>
<li>Would sending batches of sentences rather than one sentence at a time help?</li>
</ol>
<p>.</p>
<pre><code>#!pip install transformers
import torch
import transformers
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True, # Whether the model returns all hidden-states.
                                  )

# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.
model.eval()

corpa=[&quot;i am a boy&quot;,&quot;i live in a city&quot;]



storage=[]#list to store all embeddings

for text in corpa:
    # Add the special tokens.
    marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot;

    # Split the sentence into tokens.
    tokenized_text = tokenizer.tokenize(marked_text)

    # Map the token strings to their vocabulary indeces.
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

    segments_ids = [1] * len(tokenized_text)

    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # Run the text through BERT, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # Calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)

    storage.append((text,sentence_embedding))
</code></pre>
<p>######update 1</p>
<p>I modified my code based upon the answer provided. It is not doing full batch processing</p>
<pre><code>#!pip install transformers
import torch
import transformers
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True, # Whether the model returns all hidden-states.
                                  )

# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.
model.eval()

batch_sentences = [&quot;Hello I'm a single sentence&quot;,
                    &quot;And another sentence&quot;,
                    &quot;And the very very last one&quot;]
encoded_inputs = tokenizer(batch_sentences)


storage=[]#list to store all embeddings
for i,text in enumerate(encoded_inputs['input_ids']):
    
    tokens_tensor = torch.tensor([encoded_inputs['input_ids'][i]])
    segments_tensors = torch.tensor([encoded_inputs['attention_mask'][i]])
    print (tokens_tensor)
    print (segments_tensors)

    # Run the text through BERT, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # Calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)
    print (sentence_embedding[:10])
    storage.append((text,sentence_embedding))
</code></pre>
<p>I could update first 2 lines from the for loop to below. But they work only if all sentences have same length after tokenization</p>
<pre><code>tokens_tensor = torch.tensor([encoded_inputs['input_ids']])
segments_tensors = torch.tensor([encoded_inputs['attention_mask']])
</code></pre>
<p>moreover in that case <code>outputs = model(tokens_tensor, segments_tensors) </code> fails.</p>
<p>How could I fully perform batch processing in such case?</p>
"
69544570,Input/output format for Fine Tuning Huggingface RobertaForQuestionAnswering,"<p>I'm trying to fine-tune &quot;RobertaForQuestionAnswering&quot; on my custom dataset and I'm confused about the input params it takes. Here's the sample code.</p>
<pre><code>&gt;&gt;&gt; from transformers import RobertaTokenizer, RobertaForQuestionAnswering
&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
&gt;&gt;&gt; model = RobertaForQuestionAnswering.from_pretrained('roberta-base')

&gt;&gt;&gt; question, text = &quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;
&gt;&gt;&gt; inputs = tokenizer(question, text, return_tensors='pt')
&gt;&gt;&gt; start_positions = torch.tensor([1])
&gt;&gt;&gt; end_positions = torch.tensor([3])

&gt;&gt;&gt; outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
&gt;&gt;&gt; loss = outputs.loss
&gt;&gt;&gt; start_scores = outputs.start_logits
&gt;&gt;&gt; end_scores = outputs.end_logits
</code></pre>
<p>I'm not able to understand variables <strong>start_positions</strong> &amp; <strong>end_positions</strong> which are being given in the model as input and variables <strong>start_scores</strong> &amp; <strong>end_scores</strong> that are being generated.</p>
"
69626196,Train Hugging face AutoModel defined using AutoConfig,"<p>I have defined the configration for a model in <code>transformers</code>. Later, I have used this configration to initialise the classifier as follows</p>
<pre><code>from transformers import AutoConfig, AutoModel

config = AutoConfig.from_pretrained('bert-base-uncased')
classifier = AutoModel.from_config(config)
</code></pre>
<p>I have check the list of functions available for this class which are</p>
<pre><code>&gt;&gt;&gt; dir(classifier)

&gt;&gt;&gt;
['add_memory_hooks',
 'add_module',
 'adjust_logits_during_generation',
 'apply',
 'base_model',
 'base_model_prefix',
 'beam_sample',
 'beam_search',
 'bfloat16',
 'buffers',
 'children',
 'config',
 'config_class',
 'cpu',
 'cuda',
 'device',
 'double',
 'dtype',
 'dummy_inputs',
 'dump_patches',
 'embeddings',
 'encoder',
 'estimate_tokens',
 'eval',
 'extra_repr',
 'float',
 'floating_point_ops',
 'forward',
 'from_pretrained',
 'generate',
 'get_buffer',
 'get_extended_attention_mask',
 'get_head_mask',
 'get_input_embeddings',
 'get_output_embeddings',
 'get_parameter',
 'get_position_embeddings',
 'get_submodule',
 'gradient_checkpointing_disable',
 'gradient_checkpointing_enable',
 'greedy_search',
 'group_beam_search',
 'half',
 'init_weights',
 'invert_attention_mask',
 'is_parallelizable',
 'load_state_dict',
 'load_tf_weights',
 'modules',
 'name_or_path',
 'named_buffers',
 'named_children',
 'named_modules',
 'named_parameters',
 'num_parameters',
 'parameters',
 'pooler',
 'prepare_inputs_for_generation',
 'prune_heads',
 'push_to_hub',
 'register_backward_hook',
 'register_buffer',
 'register_forward_hook',
 'register_forward_pre_hook',
 'register_full_backward_hook',
 'register_parameter',
 'requires_grad_',
 'reset_memory_hooks_state',
 'resize_position_embeddings',
 'resize_token_embeddings',
 'retrieve_modules_from_names',
 'sample',
 'save_pretrained',
 'set_input_embeddings',
 'share_memory',
 'state_dict',
 'supports_gradient_checkpointing',
 'tie_weights',
 'to',
 'to_empty',
 'train',
 'training',
 'type',
 'xpu',
 'zero_grad']
</code></pre>
<p>Out of this only <code>train</code> method seemed relevant. However, upon checking the doc string for the function, I got</p>
<pre><code>&gt;&gt;&gt; print(classifier.train.__doc__)
&gt;&gt;&gt; Sets the module in training mode.

        This has any effect only on certain modules. See documentations of
        particular modules for details of their behaviors in training/evaluation
        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
        etc.

        Args:
            mode (bool): whether to set training mode (``True``) or evaluation
                         mode (``False``). Default: ``True``.

        Returns:
            Module: self
</code></pre>
<p>How do I train this classifier on custom dataset (preferably in the <code>transformers</code> or in <code>tensorflow</code>)?</p>
"
69677322,pretrained roberta relation extraction attribute error,"<p>I am trying to get the following pretrained huggingface model to work: <a href=""https://huggingface.co/mmoradi/Robust-Biomed-RoBERTa-RelationClassification"" rel=""nofollow noreferrer"">https://huggingface.co/mmoradi/Robust-Biomed-RoBERTa-RelationClassification</a></p>
<p>I use the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
  
tokenizer = AutoTokenizer.from_pretrained(&quot;mmoradi/Robust-Biomed-RoBERTa-RelationClassification&quot;)

model = AutoModel.from_pretrained(&quot;mmoradi/Robust-Biomed-RoBERTa-RelationClassification&quot;)

inputs = tokenizer(&quot;&quot;&quot;The colorectal cancer was caused by mutations in angina&quot;&quot;&quot;)
outputs = model(**inputs)
</code></pre>
<p>For some reason, I get the following error when trying to produce outputs, so in the last line of my code:</p>
<blockquote>
<p>--&gt; 796             input_shape = input_ids.size()
797         elif inputs_embeds is not None:
798             input_shape = inputs_embeds.size()[:-1]</p>
<p>AttributeError: 'list' object has no attribute 'size'</p>
</blockquote>
<p>The inputs look like this:</p>
<pre><code>{'input_ids': [0, 133, 11311, 1688, 3894, 337, 1668, 21, 1726, 30, 28513, 11, 1480, 347, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>I have no idea how to go about debugging this, so any help or hints are welcomed!</p>
"
69709015,EncoderDecoderModel converts classifier layer of decoder,"<p>I am trying to do named entity recognition using a Sequence-to-Sequence-model. My output is simple IOB-tags, and thus I only want to predict probabilities for 3 labels for each token (IOB).</p>
<p>I am trying a EncoderDecoderModel using the HuggingFace-implementation with a DistilBert as my encoder, and a BertForTokenClassification as my decoder.</p>
<p>First, I import my encoder and decoder:</p>
<pre><code>encoder = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)
encoder.save_pretrained(&quot;Encoder&quot;)

decoder = BertForTokenClassification.from_pretrained('bert-base-uncased',
                                                     num_labels=3,
                                                     output_hidden_states=False,
                                                     output_attentions=False)
decoder.save_pretrained(&quot;Decoder&quot;)
decoder
</code></pre>
<p>When I check my decoder model as shown, I can clearly see the linear classification layer that has out_features=3:</p>
<pre><code>## sample of output:
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=3, bias=True)
)
</code></pre>
<p>However, when I combine the two models in my EncoderDecoderModel, it seems that the decoder is converted into a different kind of classifier - now with out_features as the size of my vocabulary:</p>
<pre><code>bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(&quot;./Encoder&quot;,&quot;./Decoder&quot;)
bert2bert

## sample of output:
(cls): BertOnlyMLMHead(
      (predictions): BertLMPredictionHead(
        (transform): BertPredictionHeadTransform(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (decoder): Linear(in_features=768, out_features=30522, bias=True)
      )
    )
</code></pre>
<p>Why is that? And how can I keep out_features = 3 in my model?</p>
"
69720454,Questions when training language models from scratch with Huggingface,"<p>I'm following the guide here (<a href=""https://github.com/huggingface/blog/blob/master/how-to-train.md"" rel=""nofollow noreferrer"">https://github.com/huggingface/blog/blob/master/how-to-train.md</a>, <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a>) to train a RoBERTa-like model from scratch. (With my own tokenizer and dataset)</p>
<p>However, when I run <strong>run_mlm.py</strong> (<a href=""https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py</a>) to train my model with masking task, the following messages appear:</p>
<pre><code>All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.

If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
</code></pre>
<p>I'm wondering does it mean that I'm training from scratch with <strong>&quot;the pretrained weight&quot;</strong> of RoBERTa? And if it's training from the pretrained weights, is there a way to use randomly initiated weights rather than the pretrained ones?</p>
<p>==== 2021/10/26 Updated ===</p>
<p>I  am training the model with Masked Language Modeling task by following commands:</p>
<pre><code>python transformer_run_mlm.py \
--model_name_or_path roberta-base  \
--config_name ./my_dir/ \
--tokenizer_name ./my_dir/ \
--no_use_fast_tokenizer \
--train_file ./my_own_training_file.txt \
--validation_split_percentage 10 \
--line_by_line \
--output_dir /my_output_dir/ \
--do_train \
--do_eval \
--per_device_train_batch_size 64 \
--per_device_eval_batch_size 16 \
--learning_rate 1e-4 \
--max_seq_length 1024 \
--seed 42 \
--num_train_epochs 100 
</code></pre>
<p>The  <strong>./my_dir/</strong> consists of three files:</p>
<p><strong>config.json</strong> produced by the following codes:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaModel

model = RobertaModel.from_pretrained('roberta-base')
model.config.save_pretrained(MODEL_CONFIG_PATH)
</code></pre>
<p>And here's the content:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;roberta-base&quot;,
  &quot;architectures&quot;: [
    &quot;RobertaForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;bos_token_id&quot;: 0,
  &quot;classifier_dropout&quot;: null,
  &quot;eos_token_id&quot;: 2,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-05,
  &quot;max_position_embeddings&quot;: 514,
  &quot;model_type&quot;: &quot;roberta&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 1,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;transformers_version&quot;: &quot;4.12.0.dev0&quot;,
  &quot;type_vocab_size&quot;: 1,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50265
}

</code></pre>
<p><strong>vocab.json, merges.tx</strong>t produced by the following codes:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers.implementations import ByteLevelBPETokenizer

tokenizer = ByteLevelBPETokenizer()

tokenizer.train(files=OUTPUT_DIR + &quot;seed.txt&quot;, vocab_size=52_000, min_frequency=2, special_tokens=[
    &quot;&lt;s&gt;&quot;,
    &quot;&lt;pad&gt;&quot;,
    &quot;&lt;/s&gt;&quot;,
    &quot;&lt;unk&gt;&quot;,
    &quot;&lt;mask&gt;&quot;,
])

# Save files to disk
tokenizer.save_model(MODEL_CONFIG_PATH)
</code></pre>
<p>And here's the content of <strong>vocab.json</strong> (A proportion of)</p>
<pre><code>{&quot;&lt;s&gt;&quot;:0,&quot;&lt;pad&gt;&quot;:1,&quot;&lt;/s&gt;&quot;:2,&quot;&lt;unk&gt;&quot;:3,&quot;&lt;mask&gt;&quot;:4,&quot;!&quot;:5,&quot;\&quot;&quot;:6,&quot;#&quot;:7,&quot;$&quot;:8,&quot;%&quot;:9,&quot;&amp;&quot;:10,&quot;'&quot;:11,&quot;(&quot;:12,&quot;)&quot;:13,&quot;*&quot;:14,&quot;+&quot;:15,&quot;,&quot;:16,&quot;-&quot;:17,&quot;.&quot;:18,&quot;/&quot;:19,&quot;0&quot;:20,&quot;1&quot;:21,&quot;2&quot;:22,&quot;3&quot;:23,&quot;4&quot;:24,&quot;5&quot;:25,&quot;6&quot;:26,&quot;7&quot;:27,&quot;8&quot;:28,&quot;9&quot;:29,&quot;:&quot;:30,&quot;;&quot;:31,&quot;&lt;&quot;:32,&quot;=&quot;:33,&quot;&gt;&quot;:34,&quot;?&quot;:35,&quot;@&quot;:36,&quot;A&quot;:37,&quot;B&quot;:38,&quot;C&quot;:39,&quot;D&quot;:40,&quot;E&quot;:41,&quot;F&quot;:42,&quot;G&quot;:43,&quot;H&quot;:44,&quot;I&quot;:45,&quot;J&quot;:46,&quot;K&quot;:47,&quot;L&quot;:48,&quot;M&quot;:49,&quot;N&quot;:50,&quot;O&quot;:51,&quot;P&quot;:52,&quot;Q&quot;:53,&quot;R&quot;:54,&quot;S&quot;:55,&quot;T&quot;:56,&quot;U&quot;:57,&quot;V&quot;:58,&quot;W&quot;:59,&quot;X&quot;:60,&quot;Y&quot;:61,&quot;Z&quot;:62,&quot;[&quot;:63,&quot;\\&quot;:64,&quot;]&quot;:65,&quot;^&quot;:66,&quot;_&quot;:67,&quot;`&quot;:68,&quot;a&quot;:69,&quot;b&quot;:70,&quot;c&quot;:71,&quot;d&quot;:72,&quot;e&quot;:73,&quot;f&quot;:74,&quot;g&quot;:75,&quot;h&quot;:76,&quot;i&quot;:77,&quot;j&quot;:78,&quot;k&quot;:79,&quot;l&quot;:80,&quot;m&quot;:81,&quot;n&quot;:82,&quot;o&quot;:83,&quot;p&quot;:84,&quot;q&quot;:85,&quot;r&quot;:86,&quot;s&quot;:87,&quot;t&quot;:88,&quot;u&quot;:89,&quot;v&quot;:90,&quot;w&quot;:91,&quot;x&quot;:92,&quot;y&quot;:93,&quot;z&quot;:94,&quot;{&quot;:95,&quot;|&quot;:96,&quot;}&quot;:97,&quot;~&quot;:98,&quot;Â¡&quot;:99,&quot;Â¢&quot;:100,&quot;Â£&quot;:101,&quot;Â¤&quot;:102,&quot;Â¥&quot;:103,&quot;Â¦&quot;:104,&quot;Â§&quot;:105,&quot;Â¨&quot;:106,&quot;Â©&quot;:107,&quot;Âª&quot;:108,&quot;Â«&quot;:109,&quot;Â¬&quot;:110,&quot;Â®&quot;:111,&quot;Â¯&quot;:112,&quot;Â°&quot;:113,&quot;Â±&quot;:114,&quot;Â²&quot;:115,&quot;Â³&quot;:116,&quot;Â´&quot;:117,&quot;Âµ&quot;:118,&quot;Â¶&quot;:119,&quot;Â·&quot;:120,&quot;Â¸&quot;:121,&quot;Â¹&quot;:122,&quot;Âº&quot;:123,&quot;Â»&quot;:124,&quot;Â¼&quot;:125,&quot;Â½&quot;:126,&quot;Â¾&quot;:12
</code></pre>
<p>And here's the content of <strong>merges.txt</strong> (A proportion of)</p>
<pre><code>#version: 0.2 - Trained by `huggingface/tokenizers`
e n
T o
k en
Ä  To
Ä To ken
E R
V ER
VER B
a t
P R
PR O
P N
PRO PN
Ä  n
U N
N O
NO UN
E n
i t
t it
En tit
Entit y
b j
c o
Ä  a

</code></pre>
"
69757539,"Deploying huggingface zero-shot classification in Sagemaker using template returns error, missing positional argument 'candidate_labels'","<p>I'm using the generated code from huggingface, Task: <code>Zero-Shot Classification</code>, Configuration: <code>AWS</code> and running it in Sagemaker's jupyterlab</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.huggingface import HuggingFaceModel
import sagemaker

role = sagemaker.get_execution_role()
# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID':'facebook/bart-large-mnli',
    'HF_TASK':'zero-shot-classification'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    transformers_version='4.6.1',
    pytorch_version='1.7.1',
    py_version='py36',
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)

predictor.predict({
    'inputs': &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;
})
</code></pre>
<p>The following error returned:</p>
<blockquote>
<p>ModelError: An error occurred (ModelError) when calling the
InvokeEndpoint operation: Received client error (400) from primary
with message &quot;{   &quot;code&quot;: 400,   &quot;type&quot;: &quot;InternalServerException&quot;,<br />
&quot;message&quot;: &quot;<strong>call</strong>() missing 1 required positional argument:
\u0027candidate_labels\u0027&quot; } &quot;. See ...
in account **** for more information.</p>
</blockquote>
<p>I tried running them differently such as this,</p>
<pre><code>predictor.predict({
    'inputs': &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,
    'candidate_labels': ['science', 'life']
})
</code></pre>
<p>but still don't work. How should I run it?</p>
"
69780823,Tokenizers change vocabulary entry,"<p>I have some text which I want to perform NLP on. To do so, I download a pre-trained tokenizer like so:</p>
<pre class=""lang-py prettyprint-override""><code>import transformers as ts

pr_tokenizer = ts.AutoTokenizer.from_pretrained('distilbert-base-uncased', cache_dir='tmp')
</code></pre>
<p>Then I create my own tokenizer with my data like this:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token=&quot;[UNK]&quot;))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

tokenizer.train(['transcripts.raw'], trainer)
</code></pre>
<p>Now comes the part where I get confused... I need to update the entries in the pretraned tokenizer (<code>pr_tokenizer</code>) where they are the keys are the same as in my tokenizer (<code>tokenizer</code>). I have tried several methods, so here is one of them:</p>
<pre class=""lang-py prettyprint-override""><code>new_vocab = pr_tokenizer.vocab
v = tokenizer.get_vocab()

for i in v:
    if i in new_vocab:
        new_vocab[i] = v[i]
</code></pre>
<p>So what do I do now? I was thinking something like:</p>
<pre><code>pr_tokenizer.vocab.update(new_vocab)
</code></pre>
<p>or</p>
<pre><code>pr_tokenizer.vocab = new_vocab
</code></pre>
<p>Neither work. Does anyone know a good way of doing this?</p>
"
69781810,Adding Special Tokens Changes all Embeddings - TF Bert Hugging Face,"<p>Given the following,</p>
<pre><code>from transformers import TFAutoModel
from transformers import BertTokenizer


bert = TFAutoModel.from_pretrained('bert-base-cased')
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
</code></pre>
<p>I expected that if special tokens are added to the tokens, the remaining tokens would remain the same and yet they do not. For example I expected that the following should be equal but all the tokens change. Why is this?</p>
<pre><code>tokens = tokenizer(['this product is no good'], add_special_tokens=True,return_tensors='tf')
output = bert(tokens)

output[0][0][1]
</code></pre>
<p><a href=""https://i.stack.imgur.com/Ud3OR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ud3OR.png"" alt=""enter image description here"" /></a></p>
<pre><code>tokens = tokenizer(['this product is no good'], add_special_tokens=False,return_tensors='tf')
output = bert(tokens)

output[0][0][0]
</code></pre>
<p><a href=""https://i.stack.imgur.com/sC4ap.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sC4ap.png"" alt=""enter image description here"" /></a></p>
"
69820318,"Predicting Sentiment of Raw Text using Trained BERT Model, Hugging Face","<p>I'm predicting sentiment analysis of Tweets with positive, negative, and neutral classes. I've trained a BERT model using Hugging Face. Now I'd like to make predictions on a dataframe of unlabeled Twitter text and I'm having difficulty.</p>
<p>I've followed the following tutorial (<a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""nofollow noreferrer"">https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/</a>) and was able to train a BERT model using Hugging Face.</p>
<p>Here's an example of predicting on raw text however it's only one sentence and I would like to use a column of Tweets. <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/#predicting-on-raw-text"" rel=""nofollow noreferrer"">https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/#predicting-on-raw-text</a></p>
<pre><code>review_text = &quot;I love completing my todos! Best app ever!!!&quot;

encoded_review = tokenizer.encode_plus(
  review_text,
  max_length=MAX_LEN,
  add_special_tokens=True,
  return_token_type_ids=False,
  pad_to_max_length=True,
  return_attention_mask=True,
  return_tensors='pt',
)

input_ids = encoded_review['input_ids'].to(device)
attention_mask = encoded_review['attention_mask'].to(device)
output = model(input_ids, attention_mask)
_, prediction = torch.max(output, dim=1)
print(f'Review text: {review_text}')
print(f'Sentiment  : {class_names[prediction]}')

Review text: I love completing my todos! Best app ever!!!
Sentiment  : positive

</code></pre>
<p>Bill's response works. Here's the solution.</p>
<pre><code>def predictionPipeline(text):
  encoded_review = tokenizer.encode_plus(
      text,
      max_length=MAX_LEN,
      add_special_tokens=True,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

  input_ids = encoded_review['input_ids'].to(device)
  attention_mask = encoded_review['attention_mask'].to(device)

  output = model(input_ids, attention_mask)
  _, prediction = torch.max(output, dim=1)

  return(class_names[prediction])

df2['prediction']=df2['cleaned_tweet'].apply(predictionPipeline)
</code></pre>
"
69825418,NonMatchingSplitsSizesError loading huggingface BookCorpus,"<p>I want to load <code>bookcorpus</code> like this:</p>
<pre><code>train_ds, test_ds = load_dataset('bookcorpus', split=['train', 'test']),
</code></pre>
<p>however, get the following error:</p>
<pre><code>Traceback (most recent call last):             
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/marcelbraasch/.local/lib/python3.8/site-packages/datasets/load.py&quot;, line 1627, in load_dataset
    builder_instance.download_and_prepare(
  File &quot;/home/marcelbraasch/.local/lib/python3.8/site-packages/datasets/builder.py&quot;, line 607, in download_and_prepare
    self._download_and_prepare(
  File &quot;/home/marcelbraasch/.local/lib/python3.8/site-packages/datasets/builder.py&quot;, line 709, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File &quot;/home/marcelbraasch/.local/lib/python3.8/site-packages/datasets/utils/info_utils.py&quot;, line 74, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=4853859824, num_examples=74004228, dataset_name='bookcorpus'), 'recorded': SplitInfo(name='train', num_bytes=2982081448, num_examples=45726619, dataset_name='bookcorpus')}]
</code></pre>
<p>I want to proceed to save this to disk as I don't want to download this every time I use it. What causes this error?</p>
"
69835532,Dropping layers in Transformer models (PyTorch / HuggingFace),"<p>I came across this interesting <a href=""https://arxiv.org/pdf/2004.03844.pdf"" rel=""nofollow noreferrer"">paper</a> on layers dropping in Transformer models and I am actually trying to implement it. However, I am wondering what would be a good practice to perform &quot;layer dropping&quot;.</p>
<p>I have have a couple of ideas but have no idea what would be the cleanest/safest way to go here:</p>
<ul>
<li>masking the unwanted layers (some sort of pruning)</li>
<li>copying the wanted layers into a new model</li>
</ul>
<p>If anyone has already done this before or has suggestion I'm all ears!</p>
<p>Cheers</p>
"
69842980,Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation,"<p>I'm learning NLP following this sequence classification tutorial from HuggingFace <a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews</a>
The original code runs without problem. But when I tried to load a different tokenizer , such as the one from <code>google/bert_uncased_L-4_H-256_A-4</code>, the following warning appears:</p>
<p><code>Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation. </code></p>
<pre><code>from transformers import AutoTokenizer
from pathlib import Path

def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is &quot;neg&quot; else 1)

    return texts[:50], labels[:50]

if __name__ == '__main__':
    test_texts, test_labels = read_imdb_split('aclImdb/test')
    tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4')
    test_encodings = tokenizer(test_texts, truncation=True, padding=True)
    for input_id in test_encodings[&quot;input_ids&quot;]:
        print(len(input_id))
</code></pre>
<p>The output shows all <code>input_id</code> has len = 1288. It seems they have all been padded to 1288. But how could I specify the truncation target length such as 512?</p>
"
69866866,What is the best way to compute metrics for the transformers results?,"<p>Here is simple example of hugging face transformers for NER:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-large-NER&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-large-NER&quot;)

nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
example = &quot;My name is jonathan davis and I live in Chicago, Illinois&quot;

ner_results = nlp(example)
print(ner_results)

    output: [{'entity': 'B-PER', 'score': 0.95571744, 'index': 4, 'word': 'j', 'start': 11, 'end':
 12}, {'entity': 'B-PER', 'score': 0.6131773, 'index': 5, 'word': '##ona', 'start': 12, 'end': 
15}, {'entity': 'I-PER', 'score': 0.6707376, 'index': 6, 'word': '##than', 'start': 15, 'end':
 19}, {'entity': 'I-PER', 'score': 0.97754997, 'index': 7, 'word': 'da', 'start': 20, 'end': 22},
 {'entity': 'I-PER', 'score': 0.4608973, 'index': 8, 'word': '##vis', 'start': 22, 'end': 25}, 
{'entity': 'B-LOC', 'score': 0.9990302, 'index': 13, 'word': 'Chicago', 'start': 40, 'end': 47}]
</code></pre>
<p>For example I have information about my sentence:</p>
<pre><code>jonathan davis - PER
Chicago - LOC
Illinois - LOC (The model did not recognize this entity)
</code></pre>
<p>How do I correctly calculate the <strong>Precision</strong> and <strong>Recall</strong>, given that my data is split as follows:</p>
<pre><code>j, ##ona, ##than
</code></pre>
<p>Before that, I used regular expressions and used a metric, the point of which is described in this <a href=""https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/"" rel=""nofollow noreferrer"">article</a>. But I do not know if it is suitable for this task.</p>
<p>Please help me find the correct way to calculate the metric. Perhaps there are some built-in features in hugging-face I'm missing out on?</p>
"
69874436,PyInstaller problem making exe files that using transformers and PyQt5 library,"<p>So I'm working on an AI project using huggingface library, and I need to convert it into an exe file. I'm using PyQt5 for the interface, and transformers and datasets library from huggingface. I tried using PyInstaller to convert it into an exe file, it does finish building the exe files of the project, but it gives me this error when I run the exe file:</p>
<pre><code>Traceback (most recent call last):
  File &quot;transformers\utils\versions.py&quot;, line 105, in require_version
  File &quot;importlib\metadata.py&quot;, line 530, in version
  File &quot;importlib\metadata.py&quot;, line 503, in distribution
  File &quot;importlib\metadata.py&quot;, line 177, in from_name
importlib.metadata.PackageNotFoundError: tqdm

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;App.py&quot;, line 5, in &lt;module&gt;
  File &quot;PyInstaller\loader\pyimod03_importers.py&quot;, line 476, in exec_module
  File &quot;transformers\__init__.py&quot;, line 43, in &lt;module&gt;
  File &quot;PyInstaller\loader\pyimod03_importers.py&quot;, line 476, in exec_module
  File &quot;transformers\dependency_versions_check.py&quot;, line 41, in &lt;module&gt;
  File &quot;transformers\utils\versions.py&quot;, line 120, in require_version_core
  File &quot;transformers\utils\versions.py&quot;, line 107, in require_version
importlib.metadata.PackageNotFoundError: The 'tqdm&gt;=4.27' distribution was not found and is required by this application.
Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master
[736] Failed to execute script 'App' due to unhandled exception!

[process exited with code 1]
</code></pre>
<p>Line 5 on my code was a line of code for importing the transformers library.</p>
<pre class=""lang-py prettyprint-override""><code>...
4| from PyQt5.QtCore import QThread, QObject, pyqtSignal
5| from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
...
...
</code></pre>
<p>And this is my .spec file:</p>
<pre><code># -*- mode: python ; coding: utf-8 -*-

block_cipher = None

a = Analysis(['App.py'],
             pathex=[],
             binaries=[],
             datas=[
                ('./resources/images/logo.png', '.'), 
                ('./resources/model/config.json', '.'), 
                ('./resources/model/pytorch_model.bin', '.'), 
                ('./resources/model/special_tokens_map.json', '.'), 
                ('./resources/model/tokenizer.json', '.'), 
                ('./resources/model/tokenizer_config.json', '.'), 
                ('./resources/model/vocab.txt', '.')
            ],
             hiddenimports=[],
             hookspath=[],
             hooksconfig={},
             runtime_hooks=[],
             excludes=[],
             win_no_prefer_redirects=False,
             win_private_assemblies=False,
             cipher=block_cipher,
             noarchive=False)
pyz = PYZ(a.pure, a.zipped_data,
             cipher=block_cipher)

exe = EXE(pyz,
          a.scripts, 
          [],
          exclude_binaries=True,
          name='App',
          debug=False,
          bootloader_ignore_signals=False,
          strip=False,
          upx=True,
          console=True,
          disable_windowed_traceback=False,
          target_arch=None,
          codesign_identity=None,
          entitlements_file=None , icon='logo.ico')
coll = COLLECT(exe,
               a.binaries,
               a.zipfiles,
               a.datas, 
               strip=False,
               upx=True,
               upx_exclude=[],
               name='App')
</code></pre>
<p>I would really appreciate any help given, thanks :D</p>
"
69876688,Loading a HuggingFace model into AllenNLP gives different predictions,"<p>I have a custom classification model trained using <code>transformers</code> library based on a BERT model. The model classifies text into 7 different categories. It is persisted in a directory using:</p>
<pre class=""lang-py prettyprint-override""><code>trainer.save_model(model_name)
tokenizer.save_pretrained(model_name)
</code></pre>
<p>I'm trying to load such persisted model using the <code>allennlp</code> library for further analysis. I managed to do so after a lot of work. However, when running the model inside the <code>allennlp</code> framework, the model tends to predict very different from the predictions I get when I run it using <code>transformers</code>, which lead me think some part of the loading was not done correctly. There are no errors during the inference, it is just that the predictions don't match.</p>
<p>There is little documentation about how to load an existing model, so I'm wondering if someone faced the same situation before. There is just one example of how to do QA classification with ROBERTA, but couldn't extrapolate to what I'm looking for. Anyone have an idea if the steps are following are correct?</p>
<p><strong>This is how I'm loading the trained model:</strong></p>
<pre class=""lang-py prettyprint-override""><code>transformer_vocab = Vocabulary.from_pretrained_transformer(model_name)
transformer_tokenizer = PretrainedTransformerTokenizer(model_name)
transformer_encoder = BertPooler(model_name)

params = Params(
    {
     &quot;token_embedders&quot;: {
        &quot;tokens&quot;: {
          &quot;type&quot;: &quot;pretrained_transformer&quot;,
          &quot;model_name&quot;: model_name,
        }
      }
    }
)
token_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=params)
token_indexer = PretrainedTransformerIndexer(model_name)

transformer_model = BasicClassifier(vocab=transformer_vocab,
                                    text_field_embedder=token_embedder, 
                                    seq2vec_encoder=transformer_encoder, 
                                    dropout=0.1, 
                                    num_labels=7)
</code></pre>
<p>I also had to implement my own <code>DatasetReader</code> as follows:</p>
<pre class=""lang-py prettyprint-override""><code>class ClassificationTransformerReader(DatasetReader):
    def __init__(
        self,
        tokenizer: Tokenizer,
        token_indexer: TokenIndexer,
        max_tokens: int,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.tokenizer = tokenizer
        self.token_indexers: Dict[str, TokenIndexer] = { &quot;tokens&quot;: token_indexer }
        self.max_tokens = max_tokens
        self.vocab = vocab

    def text_to_instance(self, text: str, label: str = None) -&gt; Instance:
        tokens = self.tokenizer.tokenize(text)
        if self.max_tokens:
            tokens = tokens[: self.max_tokens]
        
        inputs = TextField(tokens, self.token_indexers)
        fields: Dict[str, Field] = { &quot;tokens&quot;: inputs }
            
        if label:
            fields[&quot;label&quot;] = LabelField(label)
            
        return Instance(fields)
</code></pre>
<p>It is instantiated as follows:</p>
<pre class=""lang-py prettyprint-override""><code>dataset_reader = ClassificationTransformerReader(tokenizer=transformer_tokenizer,
                                                 token_indexer=token_indexer,
                                                 max_tokens=400)
</code></pre>
<p>To run the model and test out if it works I'm doing the following:</p>
<pre class=""lang-py prettyprint-override""><code>instance = dataset_reader.text_to_instance(&quot;some sample text here&quot;)
dataset = Batch([instance])
dataset.index_instances(transformer_vocab)
model_input = util.move_to_device(dataset.as_tensor_dict(), 
                                  transformer_model._get_prediction_device())

outputs = transformer_model.make_output_human_readable(transformer_model(**model_input))
</code></pre>
<p>This works and returns the probabilities correctly, but there don't match what I would get running the model using transformers directly. Any idea what's going on?</p>
"
69907682,What are differences between AutoModelForSequenceClassification vs AutoModel,"<p>We can create a model from AutoModel(TFAutoModel) function:</p>
<pre><code>from transformers import AutoModel 
model = AutoModel.from_pretrained('distilbert-base-uncase')
</code></pre>
<p>In other hand, a model is created by AutoModelForSequenceClassification(TFAutoModelForSequenceClassification):</p>
<pre><code>from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification('distilbert-base-uncase')
</code></pre>
<p>As I know, both models use distilbert-base-uncase library to create models.
From name of methods, the second class( <strong>AutoModelForSequenceClassification</strong> ) is created for Sequence Classification.</p>
<p>But what are really differences in 2 classes? And how to use them correctly?</p>
<p>(I searched in huggingface but it is not clear)</p>
"
69914131,I want to analysis with classification algoritms using BERT's hidden state,"<p>I'm using the <a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">Huggingface Transformer</a> package and BERT with PyTorch. <br>I try to do text classification with  <a href=""https://huggingface.co/transformers/_modules/transformers/models/camembert/modeling_camembert.html#CamembertForSequenceClassification"" rel=""nofollow noreferrer"">CamembertForSequenceClassification</a>.
I can get the result, but I want to challenge more difficult task.
<br>I refer to this <a href=""https://aclanthology.org/2021.woah-1.13.pdf"" rel=""nofollow noreferrer"">literature</a>. In section 4.1 of this document, it is stated that</p>
<blockquote>
<p>After training, we drop the softmax activation layer and use BERT's hidden state as the feature vector, which we then use as input for different classification algorithms.</p>
</blockquote>
<p>So, I check the <a href=""https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html"" rel=""nofollow noreferrer"">modeling_bert.py</a>. There is <br><code>attention_probs = nn.Softmax(dim=-1)(attention_scores)</code><br>If I look at it as per the paper, does it mean to use the <strong>attention_scores</strong> before passing it through Softmax function? If so, how can I use the <strong>attention_scores</strong> and apply it to the classification algorithm?<br><br>In short, what I want to do is to use the hidden state of BERT and apply it to Logistic Regression and so on.<br><br>Thanks for any help.</p>
"
69921629,transformers AutoTokenizer.tokenize introducing extra characters,"<p>I am using HuggingFace transformers AutoTokenizer to tokenize small segments of text. However this tokenization is splitting incorrectly in the middle of words and introducing # characters to the tokens. I have tried several different models with the same results.</p>
<p>Here is an example of a piece of text and the tokens that were created from it.</p>
<pre><code>CTO at TLR Communications Pty Ltd
['[CLS]', 'CT', '##O', 'at', 'T', '##LR', 'Communications', 'P', '##ty', 'Ltd', '[SEP]']
</code></pre>
<p>And here is the code I am using to generate the tokens</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;tokenizer_bert.json&quot;)
tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))
</code></pre>
"
69983684,List index out of range when saving a fine tuned bert model,"<p>Model is created using the following function definition<br></p>
<pre><code>def create_model(max_length = 256):
  bert_model = TFBertModel.from_pretrained('bert-base-uncased')
  for layer in bert_model.layers:
    layer.trainable = False
  input_ids = tf.keras.Input(shape = (max_length, ), dtype = tf.int32, name = 'input_ids')
  attention_masks = tf.keras.Input(shape = (max_length, ), dtype = tf.int32, name = 'attention_masks')
  x = bert_model.bert([input_ids, attention_masks])
  x = x.pooler_output
  x = tf.keras.layers.Dropout(0.2)(x)
  x = tf.keras.layers.Dense(256, activation = 'relu')(x)
  x = tf.keras.layers.Dropout(0.2)(x)
  x = tf.keras.layers.Dense(33)(x)
  out = tf.keras.layers.Activation('sigmoid')(x)
  model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = out)
  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5),
                loss = tf.keras.losses.BinaryCrossentropy(),
                metrics = tf.metrics.BinaryAccuracy())
  return model
</code></pre>
<p>On trying to save the model using <code>tf.keras.models.save_model</code>, I run into the following error:<br>
<code>IndexError: Exception encountered when calling layer 'bert' (type TFBertMainLayer). list index out of range</code></p>
"
70055966,Chatbot using Huggingface Transformers,"<p>I would like to use Huggingface Transformers to implement a chatbot. Currently, I have the code shown below. The transformer model already takes into account the history of past user input.</p>
<p>Is there something else (additional code) I have to take into account for building the chatbot?</p>
<p>Second, how can I modify my code to run with TensorFlow instead of PyTorch?</p>
<p>Later on, I also plan to fine-tune the model on other data. I also plan to test different models such as BlenderBot and GPT2. I think to test this different models it should be as easy as replacing the corresponding model in <code>AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)</code> and <code>AutoModelForCausalLM.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)</code></p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)

for step in range(5):
    # encode the new user input, add the eos_token and return a tensor in Pytorch
    new_user_input_ids = tokenizer.encode(input(&quot;&gt;&gt; User:&quot;) + tokenizer.eos_token, return_tensors='pt')

    # append the new user input tokens to the chat history
    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids

    # generated a response while limiting the total chat history to 1000 tokens, 
    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    # pretty print last ouput tokens from bot
    print(&quot;DialoGPT: {}&quot;.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))
</code></pre>
"
70102323,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! When predicting with my model","<p>I trained a model for sequence classification using transformers <strong>(BertForSequenceClassification)</strong> and I get the error:</p>
<p><em>Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)</em></p>
<p>I don't really get where is the problem, if it's on my model, on how I tokenize the data, or what.</p>
<p>Here is my code:</p>
<p><strong>LOADING THE PRETRAINED MODEL</strong></p>
<pre><code>model_state_dict = torch.load(&quot;../MODELOS/TRANSFORMERS/TransformersNormal&quot;,  map_location='cpu') #Doesnt work with map_location='cuda:0' neither
model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path=&quot;bert-base-uncased&quot;, state_dict=model_state_dict, cache_dir='./data')
</code></pre>
<p><strong>CREATING DATALOAD</strong></p>
<pre><code>def crearDataLoad(dfv,tokenizer): 

  dft=dfv  # usamos el del validacion para que nos salga los resultados y no tener que cambiar mucho codigo

  #validation=dfv['text']  
  validation=dfv['text'].str.lower()  # para modelos uncased  # el fichero que hemos llamado test es usado en la red neuronal
  validation_labels=dfv['label']
  
  validation_inputs = crearinputs (validation,tokenizer)
  validation_masks= crearmask (validation_inputs)
  
  validation_inputs = torch.tensor(validation_inputs)
  
  validation_labels = torch.tensor(validation_labels.values)
  
 
  validation_masks = torch.tensor(validation_masks)
  
  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler# The DataLoader needs to know our batch size for training, so we specify it 

  #Colab
  batch_size = 32
  #local
  #batch_size = 15
  
  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
  validation_sampler = SequentialSampler(validation_data)
  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

  return validation_dataloader
</code></pre>
<p><strong>SHOWING RESULTS</strong></p>
<pre><code>def resultados(validation_dataloader, model, tokenizer):
    
  model.eval()

  # Tracking variables 
  predictions , true_labels = [], []
  pred = []
  t_label =[]
  # Predict 
  for batch in validation_dataloader:    
    # Add batch to GPU , como no tengo lo dejo aquÃ­
    batch = tuple(t.to(device) for t in batch)
  
    # Unpack the inputs from our dataloader
    b_input_ids, b_input_mask, b_labels = batch
  
    # Telling the model not to compute or store gradients, saving memory and 
    # speeding up prediction
    with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, #toktype_ids=None, #
                      attention_mask=b_input_mask) #I GET THE ERROR HERE
     
    logits = outputs[0]
 
  
    # Move logits and labels to CPU
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()
  
    # Store predictions and true labels
    # Store predictions and true labels
    predictions.append(logits)
    true_labels.append(label_ids)
 
    for l in logits:
      # para cada tupla del logits, se selecciona 0 o 1 dependiendo del valor
      # que sea el mayor (argmax)
      pred_labels_i = np.argmax(l).item()
      pred.append(pred_labels_i)
  
  #Si no me equivoco, en pred guardamos las predicciones hechas por el modelo
  pred=np.asarray(pred).tolist()
  t_label = [val for sublist in true_labels for val in sublist] # para aplanar la lista de etiquetas
  #print('predicciones',pred)
  #print('t_labels',t_label)
  #print('validation_labels',validation_labels )
  print(&quot;RESULTADOS KFOLD validacion cruzada&quot;)
  from sklearn.metrics import confusion_matrix
  from sklearn.metrics import classification_report
  print(classification_report(t_label, pred))
  print (&quot;Distribution test {}&quot;.format(Counter(t_label)))
  from sklearn.metrics import confusion_matrix
  print(confusion_matrix(t_label, pred))
  from sklearn.metrics import roc_auc_score
  print('AUC ROC:')
  print(roc_auc_score(t_label, pred))
  from sklearn.metrics import f1_score
  result=f1_score(t_label, pred, average='binary',labels=[0,1],pos_label=1,zero_division=0)
  print('f1-score macro:')
  print(result)
  print(&quot;****************************************************************&quot;)
  return result
</code></pre>
<p>I get the error at this line in function <em>resultados:</em></p>
<pre><code>with torch.no_grad():
     # Forward pass, calculate logit predictions
     outputs = model(b_input_ids, #toktype_ids=None, #
                     attention_mask=b_input_mask) #Esto falla
</code></pre>
<p><strong>MAIN PROGRAM</strong></p>
<pre><code>trial_data = pd.DataFrame(trial_dataset)

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print('no hay gpu')
print('Found GPU at: {}'.format(device_name))

#import torch# If there's a GPU available...
if torch.cuda.is_available():  # Tell PyTorch to use the GPU. 
 device = torch.device(&quot;cuda&quot;) 
 print('There are %d GPU(s) available.' % torch.cuda.device_count()) 
 print('We will use the GPU:', torch.cuda.get_device_name(0)) # If not...
else:
 print('No GPU available, using the CPU instead.')
 device = torch.device(&quot;cpu&quot;)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

validation_dataloader = crearDataLoad(trial_data,tokenizer)
# obteniendo metricas del modelo generado en el paso anterior
model.eval() 
result= resultados(validation_dataloader, model,tokenizer)
</code></pre>
"
70107997,Mapping huggingface tokens to original input text,"<p>How can I map the tokens I get from huggingface <code>DistilBertTokenizer</code> to the positions of the input text?</p>
<p>e.g. <code>I have a new GPU</code> -&gt; <code>[&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;, &quot;gp&quot;, &quot;##u&quot;]</code> -&gt; <code>[(0, 1), (2, 6), ...]</code></p>
<p>I'm interested in this because suppose that I have some attention values assigned to each token, I would like to show which part of the original text it actually corresponds to, since the tokenized version is not non-ML people friendly.</p>
<p>I have not found any solution to this yet especially when there is <code>[UNK]</code> token. Any insights would be appreciated. Thank you!</p>
"
70149699,transformers BartTokenizer::add_tokens() Doesn't Work as I'd Expect for Suffixes,"<p>I seem to be able to add tokens without issue but if I try to add a suffix (ie.. one that doesn't have the init character <code>'Ä '</code> at the front), the tokenizer doesn't put spaces in the right spots.  Here's some very simplified test code.</p>
<pre><code>from   copy import deepcopy
from   transformers import BartTokenizer
# Get the different tokenizers
tokenizer     = BartTokenizer.from_pretrained('facebook/bart-base')
tokenizer_ext = deepcopy(tokenizer)
# Note that putting Ä  after the token causes the token not to be used
num_added     = tokenizer_ext.add_tokens(['-of', '_01', 'WXYZ'])
# Original sentence
print('Original')
serial  = ':ARG0-of ( sense_01 :ARG1 ( urgencyWXYZ )'
print(serial)
print()
# Baseline tokenizer
print('Bart default tokenizer')
tokens  = tokenizer.tokenize(serial)
out_str = tokenizer.convert_tokens_to_string(tokens)
print(tokens)
print(out_str)
print()
# extended tokenizer
print('Extended tokenizer')
tokens  = tokenizer_ext.tokenize(serial)
out_str = tokenizer_ext.convert_tokens_to_string(tokens)
print(tokens)
print(out_str) 
</code></pre>
<p>This gives...</p>
<pre><code>Original
:ARG0-of ( sense_01 :ARG1 ( urgencyWXYZ )

Bart default tokenizer
[':', 'AR', 'G', '0', '-', 'of', 'Ä (', 'Ä sense', '_', '01', 'Ä :', 'AR', 'G', '1', 'Ä (', 'Ä urgency', 'W', 'XY', 'Z', 'Ä )']
:ARG0-of ( sense_01 :ARG1 ( urgencyWXYZ )

Extended tokenizer
[':', 'AR', 'G', '0', '-of', '(', 'Ä sense', '_01', ':', 'AR', 'G', '1', 'Ä (', 'Ä urgency', 'WXYZ', ')']
:ARG0-of( sense_01:ARG1 ( urgencyWXYZ)
</code></pre>
<p>Notice that the default bart tokenizer produces the same output as the original sentence but the extended tokenizer doesn't put in spaces after the new suffix tokens.  ie.. it selects <code>'('</code> instead of <code>'Ä ('</code>.  Any idea why this is and what's the right way to add suffix tokens?</p>
"
70201921,BERT Domain Adaptation,"<p>I am using <code>transformers.BertForMaskedLM</code> to further pre-train the BERT model on my custom dataset. I first serialize all the text to a <code>.txt</code> file by separating the words by a whitespace. Then, I am using <code>transformers.TextDataset</code> to load the serialized data with a BERT tokenizer given as <code>tokenizer</code> argument. Then, I am using <code>BertForMaskedLM.from_pretrained()</code> to load the pre-trained model (which is what <code>transformers</code> library presents). Then, I am using <code>transformers.Trainer</code> to further pre-train the model on my custom dataset, i.e., domain adaptation, for 3 epochs. I save the model withÂ <code>trainer.save_model()</code>. Then, I want to load the further pre-trained model to get the embeddings of the words in my custom dataset. To load the model, I am using <code>AutoModel.from_pretrained()</code> but this pops up a warning.</p>
<pre><code>Some weights of the model checkpoint at {path to my further pre-trained model} were not used when initializing BertModel
</code></pre>
<p>So, I know why this pops up. Because I further pre-trained using <code>transformers.BertForMaskedLM</code> but when I load with <code>transformers.AutoModel</code>, it loads it as <code>transformers.BertModel</code>. What I do not understand is if this is a problem or not. I just want to get the embeddings, e.g., embedding vector with a size of 768.</p>
"
70205979,ModuleNotFoundError: No module named 'h5py.utils',"<p>So I am trying to run a chat-bot which I built using Tkinter and transformers as a standalone exe file [I am using Windows 10] but I would get a run time error every-time I execute it. Is there something I am doing wrong? I have been trying different commands for nearly 2 days.</p>
<p>Error generated below:</p>
<pre><code>Traceback (most recent call last):
 RuntimeError: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):
No module named 'h5py.utils'

Traceback (most recent call last): RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback): Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback): No module named 'h5py.utils'
</code></pre>
<p>I have tried using the follow commands and added and removed additional hidden-imports but that didn't work:</p>
<pre><code>pyinstaller -w  --icon=logo.ico --hidden-import=&quot;pkg_resources.py2_warn&quot; --hidden-import=&quot;h5py.defs&quot; --hidden-import=&quot;googleapiclient&quot; --hidden-import=&quot;apiclient&quot; --hidden-import=tensorflow --hidden-import=pytorch --hidden-import=transformers --hidden-import=tqdm --collect-data tensorflow --collect-data torch --copy-metadata tensorflow --copy-metadata torch --copy-metadata h5py --copy-metadata tqdm --copy-metadata regex --copy-metadata sacremoses --copy-metadata requests --copy-metadata packaging --copy-metadata filelock --copy-metadata numpy --copy-metadata tokenizers --copy-metadata importlib_metadata --hidden-import=â€œsklearn.utils._cython_blasâ€ --hidden-import=â€œsklearn.neighbors.typedefsâ€ --hidden-import=â€œsklearn.neighbors.quad_treeâ€ --hidden-import=â€œsklearn.treeâ€ --hidden-import=â€œsklearn.tree._utilsâ€ chatbot.py
</code></pre>
"
70274841,Streamlit Unhashable TypeError when i use st.cache,"<p>when i use the st.cache decorator to cash hugging-face transformer model i get</p>
<blockquote>
<p><strong>Unhashable TypeError</strong></p>
</blockquote>
<p>this is the code</p>
<pre><code>from transformers import pipeline 
import streamlit as st 
from io import StringIO

@st.cache(hash_funcs={StringIO: StringIO.getvalue})
def model() :
    return pipeline(&quot;sentiment-analysis&quot;, model='akhooli/xlm-r-large-arabic-sent')
</code></pre>
"
70299537,HuggingFace API and ReactJS For Summary,"<p>I'm trying to make a call to the large-bert model to do a summary task but the response is always the same generic &quot;CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of the U.S. for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots. Please share your best photos of the United States with CNN iReport.&quot; Which has nothing to do with my test input from wikipedia. I tried modeling my code off of &quot;https://api-inference.huggingface.co/docs/node/html/detailed_parameters.html#summarization-task&quot; which is more specific to nodeJS but I figured should be very similar.</p>
<p>I was wondering if there was an explanation. Am I missing some input or passing the data wrong?</p>
<p>The following is the attempted code</p>
<pre><code>const response = await fetch(

Â  Â  Â  Â  Â  Â  Â  Â  &quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;,

Â  Â  Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  headers: { Authorization: `Bearer ${API_TOKEN}` },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  method: &quot;POST&quot;,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  &quot;inputs&quot;: JSON.stringify(script),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  &quot;parameters&quot;: {&quot;do_sample&quot;: false},

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  );

Â  Â  Â  Â  Â  Â  const result = await response.json();

Â  Â  Â  Â  Â  Â  setSummation(JSON.stringify(result[0].summary_text))
</code></pre>
"
70306493,View train error metrics for Hugging Face Sagemaker model,"<p>I have trained a model using Hugging Face's integration with Amazon Sagemaker <a href=""https://huggingface.co/docs/sagemaker/train"" rel=""nofollow noreferrer"">and their Hello World example</a>.</p>
<p>I can easily calculate and view the metrics generated on the evaluation test set: accuracy, f-score, precision, recall etc. by calling <code>training_job_analytics</code> on the trained model: <code>huggingface_estimator.training_job_analytics.dataframe()</code></p>
<p>How can I also see the same metrics on training sets (or even training error for each epoch)?</p>
<p>Training code is basically the same as the link with extra parts of the docs added:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.huggingface import HuggingFace

# optionally parse logs for key metrics
# from the docs: https://huggingface.co/docs/sagemaker/train#sagemaker-metrics
metric_definitions = [
    {'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\-)[0-9]+),?&quot;}
]

# hyperparameters, which are passed into the training job
hyperparameters={
    'epochs': 5,
    'train_batch_size': batch_size,
    'model_name': model_checkpoint,
    'task': task,
}

# init the model (but not yet trained)
huggingface_estimator = HuggingFace(
    entry_point='train.py',
    source_dir='./scripts',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    transformers_version='4.6',
    pytorch_version='1.7',
    py_version='py36',
    hyperparameters = hyperparameters,
    metric_definitions=metric_definitions
)
# starting the train job with our uploaded datasets as input
huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})

# does not return metrics on training - only on eval!
huggingface_estimator.training_job_analytics.dataframe()
</code></pre>
"
70335049,Sagemaker Serverless Inference & custom container: Model archiver subprocess fails,"<p>I would like to host a model on Sagemaker using the new <a href=""https://aws.amazon.com/about-aws/whats-new/2021/12/amazon-sagemaker-serverless-inference/?nc1=h_ls"" rel=""nofollow noreferrer"">Serverless Inference</a>.</p>
<p>I wrote my own container for inference and handler following several guides. These are the requirements:</p>
<pre><code>mxnet
multi-model-server
sagemaker-inference
retrying
nltk
transformers==4.12.4
torch==1.10.0
</code></pre>
<p>On non-serverless endpoints, this container works perfectly well. However, with the serverless version I get the following error message when loading the model:</p>
<pre><code>ERROR - /.sagemaker/mms/models/model already exists.
</code></pre>
<p>The error is thrown by the following subprocess</p>
<pre><code>['model-archiver', '--model-name', 'model', '--handler', '/home/model-server/handler_service.py:handle', '--model-path', '/opt/ml/model', '--export-path', '/.sagemaker/mms/models', '--archive-format', 'no-archive']
</code></pre>
<p>So something that has to do with the <code>model-archiver</code> (which I guess is a process from the MMS package?).</p>
"
70367816,What is the difference between MarianMT and OpusMT?,"<p>I'm currently comparing various pre-trained NMT models and can't help but wonder what the difference between MarianMT and OpusMT is. According to OpusMT's <a href=""https://github.com/Helsinki-NLP/Opus-MT"" rel=""noreferrer"">Github</a> it is based on MarianMT. However in the <a href=""https://huggingface.co/docs/transformers/model_doc/marian"" rel=""noreferrer"">Huggingface transformers implementation</a> all pretrained MarianMT models start with &quot;Helsinki-NLP/opus-mt&quot;. So I thought it was the same, but even though they're roughly the same size, they yield different translation results.</p>
<p>If someone could please shed some light on what the differences are I would be very thankful.</p>
"
70371140,"Machine Learning, Transformer, Multi-class classification, number of classes is inconsistent in test data and training data","<p>For example, suppose I'm building a transformer model (from huggingface) and there are 20 classes in the training data however there are only 5 classes in the testing data. To configure the transformer model from huggingface, for example, BertConfig, we need to provide a parameter: num_labels. Should I set num_labels to 20 or 5 because there are only 5 classes in the testing data?</p>
"
70449122,Change last layer on pretrained huggingface model,"<p>I want to re-finetuned a transformer model but I get an unknown error when I tried to train the model.
I can't change the &quot;num_labels&quot; on loading the model.
So, I tried to change it manually</p>
<pre><code>model_name = &quot;mrm8488/flaubert-small-finetuned-movie-review-sentiment-analysis&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name).to('cuda')

num_labels = 3
model.sequence_summary.summary = torch.nn.Linear(in_features=model.sequence_summary.summary.in_features, out_features=num_labels, bias=True)




trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train['train'],
    eval_dataset=tokenized_test['train'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    #data_collator=data_collator,
)

trainer.train()
</code></pre>
<p>The error</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-93-8139f38c5ec6&gt; in &lt;module&gt;()
     20 )
     21 
---&gt; 22 trainer.train()

7 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   2844     if size_average is not None or reduce is not None:
   2845         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2846     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
   2847 
   2848 

ValueError: Expected input batch_size (24) to match target batch_size (16).
</code></pre>
"
70496137,Can we calculate feature importance in Huggingface Bert?,"<blockquote>
<p>We can fit a LinearRegression model on the regression dataset and retrieve the coeff_ property that contains the coefficients found for each input variable. These coefficients can provide the basis for a crude feature importance score. This assumes that the input variables have the same scale or have been scaled prior to fitting a model.</p>
</blockquote>
<p>What about Bert? Can we get coef_ variable from the model and use it to calculate feature importance like LinearRegression model in text classification task?</p>
"
70518826,ONNX runtime bert inference: RuntimeError: Input must be a list of dictionaries or a single numpy array for input 'attention_mask',"<p>I am trying to use Huggingface Bert model using onnx runtime. I have used the the <a href=""https://huggingface.co/docs/transformers/serialization"" rel=""nofollow noreferrer"">docs</a> to convert the model and I am trying to run inference. My inference code is:</p>
<pre><code>from transformers import BertTokenizer, BertModel, BertTokenizerFast
import onnxruntime

sess = onnxruntime.InferenceSession(&quot;onnx/bert-base-cased/model.onnx&quot;)
tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')

encoded_input = tokenizer(text, return_tensors='pt', padding='max_length')
output = sess.run([i.name for i in sess.get_outputs()], dict(encoded_input)) # or sess.run(None, input_dict)
</code></pre>
<p>I am getting the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/srg/glib-repos/invoice_locality_extraction/cloud_run_functions/name_extraction/main.py&quot;, line 94, in invoice_extractor
    inference_results = infer.infer(v)
  File &quot;/home/srg/glib-repos/invoice_locality_extraction/cloud_run_functions/name_extraction/infer.py&quot;, line 111, in infer
    emb, call = process(tokenizer, model, item_text_results[i:i+batch_size], call+1)
  File &quot;/home/srg/glib-repos/invoice_locality_extraction/cloud_run_functions/name_extraction/get_embeddings.py&quot;, line 50, in process
    output = model.run([i.name for i in model.get_outputs()], input_dict)
  File &quot;/home/sajan/pdf2words-env/lib/python3.7/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 192, in run
    return self._sess.run(output_names, input_feed, run_options)
RuntimeError: Input must be a list of dictionaries or a single numpy array for input 'attention_mask'.
</code></pre>
"
70520725,RuntimeError: The expanded size of the tensor (585) must match the existing size (514) at non-singleton dimension 1,"<p>I want to predict the sentiment of thousands of sentences using huggingface.</p>
<pre><code>
from transformers import pipeline
model_path = &quot;cardiffnlp/twitter-xlm-roberta-base-sentiment&quot;
pipe = pipeline(&quot;sentiment-analysis&quot;, model=model_path, tokenizer=model_path)

from datasets import load_dataset

data_files = {
    &quot;train&quot;: &quot;/content/data_customer.csv&quot;
}

dataset = load_dataset(&quot;csv&quot;, data_files=data_files)

dataset = dataset.map(lambda examples: dict(pipe(examples['text'])))

</code></pre>
<p>but I am getting the following error.</p>
<pre><code>RuntimeError: The expanded size of the tensor (585) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 585].  Tensor sizes: [1, 514]
</code></pre>
<p>This post suggests a way to fix the issue but doesn't say how to fix it in pipeline.
<a href=""https://stackoverflow.com/questions/64320883/the-size-of-tensor-a-707-must-match-the-size-of-tensor-b-512-at-non-singleto"">The size of tensor a (707) must match the size of tensor b (512) at non-singleton dimension 1</a></p>
"
70532485,ERROR WHEN IMPORTING PYTORCH (The filename or extension is too long),"<p>I'm using Anconda to run my Transformers project locally in google colab.</p>
<p>I've created a new environment (tf_gpu) and installed (supposedly) everything I need.</p>
<p>And everything works fine, but when I try to simply import pytorch, this error appears:</p>
<pre><code>[WinError 206] The filename or extension is too long: 'C:\\Users\\34662\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\lib'
</code></pre>
<p>When clearly the path is not long enough to trigger this error.</p>
<p>My python version is 3.8, and my GPU is a Nvidia GeForce GTX 1650, so it shouldn't be a GPU problem</p>
<p>Does anybody knows why this happens?</p>
<p>Any help is good at this point, I don't know how to solve this.</p>
<p><a href=""https://i.stack.imgur.com/QbQ3n.png"" rel=""nofollow noreferrer"">Here I leave a screenshot of the complete error message</a></p>
<p>Thank you in advance.</p>
"
70577285,"""ValueError: You have to specify either input_ids or inputs_embeds"" when training AutoModelWithLMHead Model (GPT-2)","<p>I want to fine-tune the AutoModelWithLMHead model from <a href=""https://huggingface.co/dbmdz/german-gpt2"" rel=""nofollow noreferrer"">this repository</a>, which is a German GPT-2 model. I have followed the tutorials for pre-processing and fine-tuning. I have prepocessed a bunch of text passages for the fine-tuning, but when beginning training, I receive the following error:</p>
<pre><code>File &quot;GPT\lib\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;GPT\lib\site-packages\transformers\models\gpt2\modeling_gpt2.py&quot;, line 774, in forward
    raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)
ValueError: You have to specify either input_ids or inputs_embeds
</code></pre>
<p>Here is my code for reference:</p>
<pre><code># Load data
with open(&quot;Fine-Tuning Dataset/train.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as train_file:
    train_data = train_file.read().split(&quot;--&quot;)

with open(&quot;Fine-Tuning Dataset/test.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as test_file:
    test_data = test_file.read().split(&quot;--&quot;)

# Load pre-trained tokenizer and prepare input
tokenizer = AutoTokenizer.from_pretrained('dbmdz/german-gpt2')

tokenizer.pad_token = tokenizer.eos_token
train_input = tokenizer(train_data, padding=&quot;longest&quot;)
test_input = tokenizer(test_data, padding=&quot;longest&quot;)

# Define model

model = AutoModelWithLMHead.from_pretrained(&quot;dbmdz/german-gpt2&quot;)
training_args = TrainingArguments(&quot;test_trainer&quot;)


# Evaluation

metric = load_metric(&quot;accuracy&quot;)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = numpy.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_input,
    eval_dataset=test_input,
    compute_metrics=compute_metrics,
)
trainer.train()
trainer.evaluate()

</code></pre>
<p>Does anyone know the reason for this? Any help is welcome!</p>
"
70578679,InvalidConfigException: Can't load class for name 'HFTransformersNLP'. in rasa,"<h1>how to implement BERT in rasa with huggingface transformers and what are needed for running the Bert model in rasa ?</h1>
<pre><code>recipe: default.v1
*# Configuration for Rasa NLU.
# https://rasa.com/docs/rasa/nlu/components/*
language: en
pipeline:
*# how to implement this BERT in rasa* 
  - name: HFTransformersNLP
    model_weights: &quot;bert-base-uncased&quot;
    model_name: &quot;bert&quot;
  - name: LanguageModelTokenizer
  - name: LanguageModelFeaturizer
  - name: DIETClassifier
    epochs: 200
</code></pre>
"
70594724,HuggingFace | PipelineException: No mask_token (<mask>) found on the input,"<p>Goal: to <code>for-loop</code> over multiple <code>models</code>, <code>print()</code> elapsed time.</p>
<p>Processing one Model works fine:</p>
<pre><code>i=0
start = time.time()
unmasker = pipeline('fill-mask', model=models[i])
unmasker(&quot;Hello I'm a [MASK] model.&quot;, top_k=1)
end = time.time() 
df = df.append({'Model': models[i], 'Time': end-start}, ignore_index=True)
</code></pre>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>However, iterating over many model names causes the titled error.</p>
<hr />
<p><strong>Code:</strong></p>
<pre><code>from transformers import pipeline
import time

models = ['bert-base-uncased', 'roberta-base', 'distilbert-base-uncased', 'bert-base-cased', 'albert-base-v2', 'roberta-large', 'bert-large-uncased albert-large-v2', 'albert-base-v2', 'bert-large-cased', 'albert-base-v1', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking', 'albert-xxlarge-v2', 'google/bigbird-roberta-large', 'albert-xlarge-v2', 'albert-xxlarge-v1', 'facebook/muppet-roberta-large', 'facebook/muppet-roberta-base', 'albert-large-v1', 'albert-xlarge-v1']

for _model in models:
    start = time.time()
    unmasker = pipeline('fill-mask', model=_model)
    unmasker(&quot;Hello I'm a [MASK] model.&quot;, top_k=1)  # default: top_k=5
    end = time.time()

    print(end-start)
</code></pre>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
---------------------------------------------------------------------------
PipelineException                         Traceback (most recent call last)
&lt;ipython-input-19-13b5f651657e&gt; in &lt;module&gt;
      3     start = time.time()
      4     unmasker = pipeline('fill-mask', model=_model)
----&gt; 5     unmasker(&quot;Hello I'm a [MASK] model.&quot;, top_k=1)  # default: top_k=5
      6     end = time.time()
      7 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/pipelines/fill_mask.py in __call__(self, inputs, *args, **kwargs)
    224             - **token** (`str`) -- The predicted token (to replace the masked one).
    225         &quot;&quot;&quot;
--&gt; 226         outputs = super().__call__(inputs, **kwargs)
    227         if isinstance(inputs, list) and len(inputs) == 1:
    228             return outputs[0]

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/pipelines/base.py in __call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1099                 return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)
   1100         else:
-&gt; 1101             return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
   1102 
   1103     def run_multi(self, inputs, preprocess_params, forward_params, postprocess_params):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/pipelines/base.py in run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1105 
   1106     def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
-&gt; 1107         model_inputs = self.preprocess(inputs, **preprocess_params)
   1108         model_outputs = self.forward(model_inputs, **forward_params)
   1109         outputs = self.postprocess(model_outputs, **postprocess_params)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/pipelines/fill_mask.py in preprocess(self, inputs, return_tensors, **preprocess_parameters)
     82             return_tensors = self.framework
     83         model_inputs = self.tokenizer(inputs, return_tensors=return_tensors)
---&gt; 84         self.ensure_exactly_one_mask_token(model_inputs)
     85         return model_inputs
     86 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/pipelines/fill_mask.py in ensure_exactly_one_mask_token(self, model_inputs)
     76         else:
     77             for input_ids in model_inputs[&quot;input_ids&quot;]:
---&gt; 78                 self._ensure_exactly_one_mask_token(input_ids)
     79 
     80     def preprocess(self, inputs, return_tensors=None, **preprocess_parameters) -&gt; Dict[str, GenericTensor]:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/pipelines/fill_mask.py in _ensure_exactly_one_mask_token(self, input_ids)
     67                 &quot;fill-mask&quot;,
     68                 self.model.base_model_prefix,
---&gt; 69                 f&quot;No mask_token ({self.tokenizer.mask_token}) found on the input&quot;,
     70             )
     71 

PipelineException: No mask_token (&lt;mask&gt;) found on the input
</code></pre>
<p>Please let me know if there's anything else I can add to post to clarify.</p>
"
70606666,"Solving ""CUDA out of memory"" when fine-tuning GPT-2 (HuggingFace)","<p>I get the reoccuring CUDA out of memory error when using the HuggingFace Transformers library to fine-tune a GPT-2 model and can't seem to solve it, despite my 6 GB GPU capacity, which I thought should be enough for fine-tuning on texts. The error reads as follows:</p>
<pre><code>File &quot;GPT\lib\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;GPT\lib\site-packages\transformers\modeling_utils.py&quot;, line 1763, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 6.00 GiB total capacity; 4.28 GiB already allocated; 24.50 MiB free; 4.33 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>I already set batch size to as low as 2 and reduced training examples without success. I also tried to migrate the code to Colab, where the 12GB RAM were quickly consumed.
My examples are rather long, some counting 2.400 characters, but they should be truncated by the model automatically. My (German) examples look like this:</p>
<pre><code> Er geht in fremde Wohnungen, balgt sich mit Freund und Feind, ist
zudringlich zu unsern SÃ¤mereien und Kirschen.  Wenn die Gesellschaft nicht groÃŸ
ist, lasse ich sie gelten und streue ihnen sogar Getreide.  Sollten sie hier
aber doch zu viel werden, so hilft die WindbÃ¼chse, und sie werden in den
Meierhof hinabgescheucht.  Als einen bÃ¶sen Feind zeigte sich der Rotschwanz.  Er
flog zu dem Bienenhause und schnappte die Tierchen weg.  Da half nichts, als ihn
ohne Gnade mit der WindbÃ¼chse zu tÃ¶ten.

 Ich wollte
Ihnen mein Wort halten, liebe Mama, aber die Versuchung war zu groÃŸ.  Da bin ich
eines Abends in den Keller gegangen und hab' aus allen FÃ¤ssern den Spund
herausgeklopft.  Bis auf den letzten Tropfen ist das Gift ausgeronnen aus den
FÃ¤ssern.  Der Schade war groÃŸ, aber der Teufel war aus dem Haus. Â«

Andor lachte.  Â»Mama, das Geschrei hÃ¤tten Sie hÃ¶ren sollen! Als ob der
Weltuntergang gekommen wÃ¤re. Er bedauerte beinahe seine
Schroffheit.  Nun, nachlaufen wird er ihnen nicht, die werden schon selber
kommen.  Aber bewachen wird er seine Kolonie bei Tag und bei Nacht lassen
mÃ¼ssen.  Hol' der Teufel diesen Mercy.  MuÃŸ der gerade in HÃ¶gyÃ©sz ein Kastell
haben.  Wenn einer von den SchwarzwÃ¤ldern dahin kommt und ihn verklagt.
</code></pre>
<p>Is there a problem with the data formatting maybe?
If anyone has a hint on how to solve this, it would be very welcome.</p>
<p>EDIT: Thank you <a href=""https://stackoverflow.com/users/6117017/timbus-calin"">Timbus Calin</a> for the answer, I described in the comment how adding the <strong><code>block_size</code></strong> flag to the config.json solved the problem. Here is the whole configuration for reference:</p>
<pre><code>{
    &quot;model_name_or_path&quot;: &quot;dbmdz/german-gpt2&quot;,
    &quot;train_file&quot;: &quot;Fine-Tuning Dataset/train.txt&quot;,
    &quot;validation_file&quot;: &quot;Fine-Tuning Dataset/test.txt&quot;,
    &quot;output_dir&quot;: &quot;Models&quot;,
    &quot;overwrite_output_dir&quot;: true,
    &quot;per_device_eval_batch_size&quot;: 8,
    &quot;per_device_train_batch_size&quot;: 8,
    &quot;block_size&quot;: 100, 
    &quot;task_type&quot;: &quot;text-generation&quot;,
    &quot;do_train&quot;: true,
    &quot;do_eval&quot;: true
}

</code></pre>
"
70607224,HuggingFace - 'optimum' ModuleNotFoundError,"<p>I want to run the 3 code snippets from this <a href=""https://huggingface.co/hardware"" rel=""nofollow noreferrer"">webpage</a>.</p>
<p>I've made all 3 one post, as I am assuming it all stems from the same problem of <code>optimum</code> not having been imported correctly?</p>
<p>Kernel: <code>conda_pytorch_p36</code></p>
<hr />
<p>Installations:</p>
<pre><code>pip install optimum
</code></pre>
<p>OR</p>
<pre><code>! pip install datasets transformers optimum[intel]
</code></pre>
<p>Both provide same Traceback:</p>
<pre><code>Requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.3)
Requirement already satisfied: transformers&gt;=4.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (4.15.0)
Requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (15.0.1)
Requirement already satisfied: torch&gt;=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.10.1)
Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.8)
Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch&gt;=1.9-&gt;optimum) (3.10.0.0)
Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch&gt;=1.9-&gt;optimum) (0.8)
Requirement already satisfied: numpy&gt;=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (1.19.5)
Requirement already satisfied: packaging&gt;=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (21.3)
Requirement already satisfied: pyyaml&gt;=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (5.4.1)
Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (0.0.46)
Requirement already satisfied: tqdm&gt;=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (4.62.3)
Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (2021.4.4)
Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (2.25.1)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (0.2.1)
Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (0.10.3)
Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (4.5.0)
Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (3.0.12)
Requirement already satisfied: humanfriendly&gt;=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from coloredlogs-&gt;optimum) (10.0)
Requirement already satisfied: mpmath&gt;=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sympy-&gt;optimum) (1.2.1)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging&gt;=20.0-&gt;transformers&gt;=4.12.0-&gt;optimum) (2.4.7)
Requirement already satisfied: zipp&gt;=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata-&gt;transformers&gt;=4.12.0-&gt;optimum) (3.4.1)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (2.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (2021.5.30)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (4.0.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (1.26.5)
Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses-&gt;transformers&gt;=4.12.0-&gt;optimum) (1.0.1)
Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses-&gt;transformers&gt;=4.12.0-&gt;optimum) (8.0.1)
Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses-&gt;transformers&gt;=4.12.0-&gt;optimum) (1.16.0)
Note: you may need to restart the kernel to use updated packages.
</code></pre>
<hr />
<pre class=""lang-py prettyprint-override""><code>from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification

# Create quantizer from config 
quantizer = LpotQuantizerForSequenceClassification.from_config(
    &quot;echarlaix/quantize-dynamic-test&quot;,
    &quot;quantization.yml&quot;,
    model_name_or_path=&quot;textattack/bert-base-uncased-SST-2&quot;,
)

model = quantizer.fit_dynamic()
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-6-9dcf25f181ea&gt; in &lt;module&gt;
----&gt; 1 from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification
      2 
      3 # Create quantizer from config
      4 quantizer = LpotQuantizerForSequenceClassification.from_config(
      5     &quot;echarlaix/quantize-dynamic-test&quot;,

ModuleNotFoundError: No module named 'optimum.intel.lpot'
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification

# Create pruner from config 
pruner = LpotPrunerForSequenceClassification.from_config(
    &quot;echarlaix/magnitude-pruning-test&quot;,
    &quot;prune.yml&quot;,
    model_name_or_path=&quot;textattack/bert-base-uncased-SST-2&quot;,
)

model = pruner.fit()
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-7-e9872c164aee&gt; in &lt;module&gt;
----&gt; 1 from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification
      2 
      3 # Create pruner from config
      4 pruner = LpotPrunerForSequenceClassification.from_config(
      5     &quot;echarlaix/magnitude-pruning-test&quot;,

ModuleNotFoundError: No module named 'optimum.intel.lpot'
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from optimum.graphcore import IPUTrainer
from optimum.graphcore.bert import BertIPUConfig
from transformers import BertForMaskedLM, BertTokenizer
from poptorch.optim import AdamW

# Allocate model and tokenizer as usual
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
model = BertForMaskedLM.from_pretrained(&quot;bert-base-cased&quot;)

# Trainer + poptorch custom configuration optional 
ipu_config = BertIPUConfig()
trainer = IPUTrainer(model, trainings_args, config=ipu_config)
optimizer = AdamW(model.parameters)

# This is hidden from the user, it will be handled by the Trainer
with trainer.compile(some_data_loader) as model_f:
    for steps in range(10):  # !
        outputs = trainer.step(optimizer)    

# Save the model and/or push to hub
model.save_pretrained(&quot;...&quot;)
model.push_to_hub(&quot;...&quot;)
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-8-921e03245390&gt; in &lt;module&gt;
----&gt; 1 from optimum.graphcore import IPUTrainer
      2 from optimum.graphcore.bert import BertIPUConfig
      3 from transformers import BertForMaskedLM, BertTokenizer
      4 from poptorch.optim import AdamW
      5 

ModuleNotFoundError: No module named 'optimum.graphcore'
</code></pre>
<p>Please let me know if there's anything else I can add to post.</p>
"
70609579,Use Quantization on HuggingFace Transformers models,"<p>I'm learning <strong>Quantization</strong>, and am experimenting with <strong>Section 1</strong> of this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>I want to use this code on my own models.</p>
<p>Hypothetically, I only need to assign to <code>model</code> variable in <strong>Section 1.2</strong></p>
<hr />
<pre class=""lang-py prettyprint-override""><code># load model
model = BertForSequenceClassification.from_pretrained(configs.output_dir)
model.to(configs.device)
</code></pre>
<p>My models are from a different library: <code>from transformers import pipeline</code>. So <code>.to()</code> throws an <code>AttributeError</code>.</p>
<p>My Model:</p>
<pre><code>pip install transformers
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
model = unmasker(&quot;Hello I'm a [MASK] model.&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<hr />
<p><strong>How might I run the linked Quantization code on my example model?</strong></p>
<p>Please let me know if there's anything else I should clarify in this post.</p>
"
70612932,Can I make a transformer based chatbot which is pretrained on some other dataset?,"<p>I'm currently making a medical transformer chatbot from the tutorial from this page:</p>
<p><a href=""https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html"" rel=""nofollow noreferrer"">https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html</a></p>
<p>I'm using a text corpus of medical questions/answers. When I train the model and test it, it just gives answers to questions relevant to medical domain. However, I want to create a conversational chatbot which can answer basic questions like 'How are you' and 'I need help'. Is there a way I can us some pretrained weights, then train the model on my medical dataset?
I'm pretty new to natural language processing domain so could really use some guidance. Thanks!</p>
"
70619634,Error(s) in loading state_dict for RobertaForSequenceClassification,"<p>I am using a fine-tuned Roberta Model that is <strong>unbiased-toxic-roberta</strong> trained on Jigsaw Data:</p>
<p><a href=""https://huggingface.co/unitary/unbiased-toxic-roberta"" rel=""nofollow noreferrer"">https://huggingface.co/unitary/unbiased-toxic-roberta</a></p>
<p>It is fine-tuned on 16 classes.</p>
<p><strong>I am writing my code for binary classification:</strong></p>
<p>Metrics to calculate loss on binary labels as accuracy</p>
<pre><code>def compute_metrics(eval_pred):
    
    logits, labels = eval_pred
   

    predictions = np.argmax(logits, axis=-1)
    
    acc = np.sum(predictions == labels) / predictions.shape[0]
    
    return {&quot;accuracy&quot; : acc}

import torch.nn as nn
model = tr.RobertaForSequenceClassification.from_pretrained(&quot;/home/pc/unbiased_toxic_roberta&quot;,num_labels=2)

model.to(device)



training_args = tr.TrainingArguments(
#     report_to = 'wandb',
    output_dir='/home/pc/1_Proj_hate_speech/results_roberta',          # output directory
    overwrite_output_dir = True,
    num_train_epochs=20,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    learning_rate=2e-5,
    warmup_steps=1000,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs3',            # directory for storing logs
    logging_steps=1000,
    evaluation_strategy=&quot;epoch&quot;
    ,save_strategy=&quot;epoch&quot;
    ,load_best_model_at_end=True
)


trainer = tr.Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_data,         # training dataset
    eval_dataset=val_data,             # evaluation dataset
    compute_metrics=compute_metrics
</code></pre>
<p>)</p>
<p>When I run this, I get an error:</p>
<pre><code>loading weights file /home/pc/unbiased_toxic_roberta/pytorch_model.bin
RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification:
    size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([16, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).
    size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([2]).
</code></pre>
<p><strong>How can I add a linear layer and solve this error ?</strong></p>
"
70621833,ModuleNotFoundError: No module named 'nn_pruning.modules.quantization',"<p>Goal: install <code>nn_pruning</code>.</p>
<p>Kernel: <code>conda_pytorch_p36</code>. I performed Restart &amp; Run All.</p>
<p>It seems to recognise the <code>optimize_model</code> import, but not other functions. Even though they are from the same <code>nn_pruning</code> library.</p>
<pre><code>pip install nn_pruning | pip install -U nn_pruning
</code></pre>
<pre><code>Requirement already satisfied: nn_pruning in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.2)
Requirement already satisfied: torch&gt;=1.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nn_pruning) (1.8.1+cpu)
Requirement already satisfied: transformers&gt;=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nn_pruning) (4.15.0)
Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nn_pruning) (8.0.1)
Requirement already satisfied: scikit-learn&gt;=0.24 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nn_pruning) (0.24.2)
Requirement already satisfied: joblib&gt;=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn&gt;=0.24-&gt;nn_pruning) (1.0.1)
Requirement already satisfied: numpy&gt;=1.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn&gt;=0.24-&gt;nn_pruning) (1.19.5)
Requirement already satisfied: scipy&gt;=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn&gt;=0.24-&gt;nn_pruning) (1.5.4)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn&gt;=0.24-&gt;nn_pruning) (2.1.0)
Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch&gt;=1.6-&gt;nn_pruning) (0.8)
Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch&gt;=1.6-&gt;nn_pruning) (3.10.0.0)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (0.2.1)
Requirement already satisfied: tqdm&gt;=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (4.62.3)
Requirement already satisfied: packaging&gt;=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (21.3)
Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (3.0.12)
Requirement already satisfied: pyyaml&gt;=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (5.4.1)
Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (2.25.1)
Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (0.0.46)
Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (0.10.3)
Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (4.5.0)
Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.3.0-&gt;nn_pruning) (2021.4.4)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging&gt;=20.0-&gt;transformers&gt;=4.3.0-&gt;nn_pruning) (2.4.7)
Requirement already satisfied: zipp&gt;=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata-&gt;transformers&gt;=4.3.0-&gt;nn_pruning) (3.4.1)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.3.0-&gt;nn_pruning) (2.10)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.3.0-&gt;nn_pruning) (4.0.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.3.0-&gt;nn_pruning) (1.26.5)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.3.0-&gt;nn_pruning) (2021.5.30)
Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses-&gt;transformers&gt;=4.3.0-&gt;nn_pruning) (1.16.0)
Note: you may need to restart the kernel to use updated packages.
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import functools
from tqdm import tqdm

import torch 

#from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

from nn_pruning.inference_model_patcher import optimize_model
from nn_pruning.modules.quantization import prepare_static, quantize  # !
</code></pre>
<p>Traceback:</p>
<pre><code>ModuleNotFoundError: No module named 'nn_pruning.modules.quantization'
</code></pre>
"
70663782,"ValueError: Layer weight shape (30522, 768) not compatible with provided weight shape ()","<p>I got word-embedding using BERT and need to feed it as an embedding layer in the Keras model, and the error I got is</p>
<pre><code>ValueError: Layer weight shape (30522, 768) not compatible with provided weight shape ()
</code></pre>
<p>the model is</p>
<pre><code>embedding = Embedding(30522, 768, mask_zero=True)(sentence)
model.layers[1].set_weights([embedding_matrix])
</code></pre>
"
70672460,Hugging face - Efficient tokenization of unknown token in GPT2,"<p>I am trying to train a dialog system using GPT2. For tokenization, I am using the following configuration for adding the special tokens.</p>
<pre><code>from transformers import (
     AdamW,
     AutoConfig,
     AutoTokenizer,
     PreTrainedModel,
     PreTrainedTokenizer,
     get_linear_schedule_with_warmup,
)

SPECIAL_TOKENS = {
    &quot;bos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;eos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;pad_token&quot;: &quot;[PAD]&quot;,
    &quot;additional_special_tokens&quot;: [&quot;[SYS]&quot;, &quot;[USR]&quot;, &quot;[KG]&quot;, &quot;[SUB]&quot;, &quot;[PRED]&quot;, &quot;[OBJ]&quot;, &quot;[TRIPLE]&quot;, &quot;[SEP]&quot;, &quot;[Q]&quot;,&quot;[DOM]&quot;]
}
tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)
tokenizer.add_special_tokens(SPECIAL_TOKENS)
</code></pre>
<p>Next, when I am trying to tokenize a sequence(dialog's utterance) and later convert into ids, some of the most important tokens in my sequence are getting mapped as unknown tokens, since the ids of these important tokens becomes the same as bos and eos as they all map to &lt;|endoftext|&gt; as in the GPT2's <a href=""https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/gpt2/tokenization_gpt2.py#L104"" rel=""nofollow noreferrer"">source code</a>.</p>
<p>Here is a working example -</p>
<pre><code>tokenized_sequence = ['[PRED]', 'name', '[SUB]', 'frankie_and_bennys', '[PRED]', 'address', '[SUB]', 'cambridge_leisure_park_clifton_way_cherry_hinton', '[PRED]', 'area', '[SUB]', 'south', '[PRED]', 'food', '[SUB]', 'italian', '[PRED]', 'phone', '[SUB]', '01223_412430', '[PRED]', 'pricerange', '[SUB]', 'expensive', '[PRED]', 'postcode', '[SUB]', 'cb17dy']
important_tokens = ['frankie_and_bennys','cambridge_leisure_park_clifton_way_cherry_hinton','italian','postcode', 'cb17dy']
tokens_to_ids = [50262, 3672, 50261, 50256, 50262, 21975, 50261, 50256, 50262, 20337, 50261, 35782, 50262, 19425, 50261, 50256, 50262, 4862, 50261, 50256, 50262, 50256, 50261, 22031, 50262, 50256, 50261, 50256]
ids_to_tokens = [PRED]name[SUB]&lt;|endoftext|&gt;[PRED]address[SUB]&lt;|endoftext|&gt;[PRED]area[SUB]south[PRED]food[SUB]&lt;|endoftext|&gt;[PRED]phone[SUB]&lt;|endoftext|&gt;[PRED]&lt;|endoftext|&gt;[SUB]expensive[PRED]&lt;|endoftext|&gt;[SUB]&lt;|endoftext|&gt;
</code></pre>
<p>As you can see the important_tokens are being mapped to the id  50256 (that is to |endoftext|), the model fails to see and learn these important tokens and hence generate very poor and often hallucinated responses.</p>
<p>What could be a quick and efficient fix for this issue?</p>
"
70697470,"ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name","<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>Albert</strong> and <strong>Distilbert</strong> models</p>
<p>Kernel: <code>conda_pytorch_p36</code>. I did Restart &amp; Run All, and refreshed file view in working directory.</p>
<p>Error occurs in <strong>Section 1.2</strong>, only for these 2 new models.</p>
<p>For filenames etc., I've created a variable used everywhere:</p>
<pre class=""lang-py prettyprint-override""><code>MODEL_NAME = 'albert-base-v2'  # 'distilbert-base-uncased', 'bert-base-uncased'
</code></pre>
<p>I replaced imports with:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import (AutoConfig, AutoModel, AutoTokenizer)
#from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)
</code></pre>
<p>As suggested in <a href=""https://huggingface.co/docs/transformers/model_doc/auto"" rel=""nofollow noreferrer"">Transformers Documentation - Auto Classes</a>.</p>
<blockquote>
<p>Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.</p>
</blockquote>
<hr />
<p><strong>Section 1.2:</strong></p>
<pre class=""lang-py prettyprint-override""><code># load model
model = AutoModel.from_pretrained(configs.output_dir)  # BertForSequenceClassification
model.to(configs.device)


# quantize model
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

#print(quantized_model)

def print_size_of_model(model):
    torch.save(model.state_dict(), &quot;temp.p&quot;)
    print('Size (MB):', os.path.getsize(&quot;temp.p&quot;)/(1024*1024))
    os.remove('temp.p')

print_size_of_model(model)
print_size_of_model(quantized_model)
</code></pre>
<p>Traceback:</p>
<pre><code>ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm
</code></pre>
<p>Please let me know if there's anything else I can add to post.</p>
"
70698407,HuggingFace AutoTokenizer | ValueError: Couldn't instantiate the backend tokenizer,"<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>albert-base-v2</strong> model</p>
<p>Error occurs in <strong>Section 1.3</strong>.</p>
<p>Kernel: <code>conda_pytorch_p36</code>. I did Restart &amp; Run All, and refreshed file view in working directory.</p>
<hr />
<p>There are 3 listed ways this error can be caused. I'm not sure which my case falls under.</p>
<p>Section 1.3:</p>
<pre class=""lang-py prettyprint-override""><code># define the tokenizer
tokenizer = AutoTokenizer.from_pretrained(
        configs.output_dir, do_lower_case=configs.do_lower_case)
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-25-1f864e3046eb&gt; in &lt;module&gt;
    140 # define the tokenizer
    141 tokenizer = AutoTokenizer.from_pretrained(
--&gt; 142         configs.output_dir, do_lower_case=configs.do_lower_case)
    143 
    144 # Evaluate the original FP32 BERT model

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    548             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]
    549             if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):
--&gt; 550                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    551             else:
    552                 if tokenizer_class_py is not None:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1752             use_auth_token=use_auth_token,
   1753             cache_dir=cache_dir,
-&gt; 1754             **kwargs,
   1755         )
   1756 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1880         # Instantiate tokenizer.
   1881         try:
-&gt; 1882             tokenizer = cls(*init_inputs, **init_kwargs)
   1883         except OSError:
   1884             raise OSError(

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/albert/tokenization_albert_fast.py in __init__(self, vocab_file, tokenizer_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)
    159             cls_token=cls_token,
    160             mask_token=mask_token,
--&gt; 161             **kwargs,
    162         )
    163 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_fast.py in __init__(self, *args, **kwargs)
    116         else:
    117             raise ValueError(
--&gt; 118                 &quot;Couldn't instantiate the backend tokenizer from one of: \n&quot;
    119                 &quot;(1) a `tokenizers` library serialization file, \n&quot;
    120                 &quot;(2) a slow tokenizer instance to convert or \n&quot;

ValueError: Couldn't instantiate the backend tokenizer from one of: 
(1) a `tokenizers` library serialization file, 
(2) a slow tokenizer instance to convert or 
(3) an equivalent slow tokenizer class to instantiate and convert. 
You need to have sentencepiece installed to convert a slow tokenizer to a fast one.
</code></pre>
<p>Please let me know if there's anything else I can add to post.</p>
"
70699247,TypeError: an integer is required (got type NoneType),"<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>distilbert-base-uncased</strong> model</p>
<p>Error occurs in <strong>Section 1.3</strong>.</p>
<p>Kernel: <code>conda_pytorch_p36</code>. I did Restart &amp; Run All, and refreshed file view in working directory.</p>
<hr />
<p>Section 1.3:</p>
<pre class=""lang-py prettyprint-override""><code># define the tokenizer
tokenizer = AutoTokenizer.from_pretrained(
        configs.output_dir, do_lower_case=configs.do_lower_case)
</code></pre>
<p>Traceback:</p>
<pre><code>Evaluating PyTorch full precision accuracy and performance:
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING.format(&quot;function&quot;), FutureWarning)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-31-1f864e3046eb&gt; in &lt;module&gt;
    144 # Evaluate the original FP32 BERT model
    145 print('Evaluating PyTorch full precision accuracy and performance:')
--&gt; 146 time_model_evaluation(model, configs, tokenizer)
    147 
    148 # Evaluate the INT8 BERT model after the dynamic quantization

&lt;ipython-input-31-1f864e3046eb&gt; in time_model_evaluation(model, configs, tokenizer)
    132 def time_model_evaluation(model, configs, tokenizer):
    133     eval_start_time = time.time()
--&gt; 134     result = evaluate(configs, model, tokenizer, prefix=&quot;&quot;)
    135     eval_end_time = time.time()
    136     eval_duration_time = eval_end_time - eval_start_time

&lt;ipython-input-31-1f864e3046eb&gt; in evaluate(args, model, tokenizer, prefix)
     22     results = {}
     23     for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):
---&gt; 24         eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)
     25 
     26         if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:

&lt;ipython-input-31-1f864e3046eb&gt; in load_and_cache_examples(args, task, tokenizer, evaluate)
    121     all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
    122     all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)
--&gt; 123     all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
    124     if output_mode == &quot;classification&quot;:
    125         all_labels = torch.tensor([f.label for f in features], dtype=torch.long)

TypeError: an integer is required (got type NoneType)
</code></pre>
<p>Please let me know if there's anything else I can add to post.</p>
"
70709572,TypeError: not a string | parameters in AutoTokenizer.from_pretrained(),"<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>albert-base-v2</strong> model.</p>
<p>Kernel: <code>conda_pytorch_p36</code>. I did Restart &amp; Run All, and refreshed file view in working directory.</p>
<hr />
<p>In order to evaluate and to export this Quantised model, I need to setup a Tokenizer.</p>
<p>Error occurs in <strong>Section 1.3</strong>.</p>
<p>Both parameters in <code>AutoTokenizer.from_pretrained()</code> throw the same error.</p>
<hr />
<p><strong>Section 1.3 Code:</strong></p>
<pre><code># define the tokenizer
tokenizer = AutoTokenizer.from_pretrained(
        configs.output_dir, do_lower_case=configs.do_lower_case)
</code></pre>
<p>Parameters:</p>
<pre><code># The output directory for the fine-tuned model, $OUT_DIR.
configs.output_dir = &quot;./MRPC/&quot;
</code></pre>
<pre><code># Prepare GLUE task
...
configs.do_lower_case = True
</code></pre>
<p>Values and DTypes:</p>
<pre><code>-- configs.output_dir --
./MRPC/
&lt;class 'str'&gt;

-- configs.do_lower_case --
True
&lt;class 'bool'&gt;
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-13-18c5137aacf4&gt; in &lt;module&gt;
    140 # define the tokenizer
    141 tokenizer = AutoTokenizer.from_pretrained(
--&gt; 142         configs.output_dir, do_lower_case=configs.do_lower_case)
    143 
    144 # Evaluate the original FP32 BERT model

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    548             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]
    549             if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):
--&gt; 550                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    551             else:
    552                 if tokenizer_class_py is not None:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1752             use_auth_token=use_auth_token,
   1753             cache_dir=cache_dir,
-&gt; 1754             **kwargs,
   1755         )
   1756 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1776                 copy.deepcopy(init_configuration),
   1777                 *init_inputs,
-&gt; 1778                 **(copy.deepcopy(kwargs)),
   1779             )
   1780         else:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1880         # Instantiate tokenizer.
   1881         try:
-&gt; 1882             tokenizer = cls(*init_inputs, **init_kwargs)
   1883         except OSError:
   1884             raise OSError(

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/albert/tokenization_albert.py in __init__(self, vocab_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, sp_model_kwargs, **kwargs)
    179 
    180         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)
--&gt; 181         self.sp_model.Load(vocab_file)
    182 
    183     @property

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in Load(self, model_file, model_proto)
    365       if model_proto:
    366         return self.LoadFromSerializedProto(model_proto)
--&gt; 367       return self.LoadFromFile(model_file)
    368 
    369 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in LoadFromFile(self, arg)
    169 
    170     def LoadFromFile(self, arg):
--&gt; 171         return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
    172 
    173     def DecodeIdsWithCheck(self, ids):

TypeError: not a string
</code></pre>
<p>Please let me know if there's anything else I can add to post.</p>
"
70716702,Using sentence transformers with limited access to internet,"<p>I have access to the latest packages but I cannot access internet from my python enviroment.</p>
<p>Package versions that I have are as below</p>
<pre><code>huggingface-hub-0.4.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0
sentence-transformers-2.1.0 sentencepiece-0.1.96 torchvision-0.11.2

print (torch.__version__)
1.10.1+cu102
</code></pre>
<p>I went to the <a href=""https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1/tree/main"" rel=""nofollow noreferrer"">location</a> and copied all the files in a folder</p>
<pre><code>os.listdir('multi-qa-mpnet-base-dot-v1_Jan2022/')

['config_sentence_transformers.json',
 'config.json',
 'gitattributes',
 'modules.json',
 'data_config.json',
 'sentence_bert_config.json',
 'README.md',
 'special_tokens_map.json',
 'tokenizer_config.json',
 'train_script.py',
 'vocab.txt',
 'tokenizer.json',
 '1_Pooling',
 '.ipynb_checkpoints',
 '9e1e76b7a067f72e49c7f571cd8e811f7a1567bec49f17e5eaaea899e7bc2c9e']
</code></pre>
<p>Then I went to the <a href=""https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1"" rel=""nofollow noreferrer"">url</a> and tried to execute the code listed there</p>
<p>But I get below error</p>
<pre><code>model = SentenceTransformer('multi-qa-mpnet-base-dot-v1_Jan2022/')

OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory multi-qa-mpnet-base-dot-v1_Jan2022/ or `from_tf` and `from_flax` set to False.
</code></pre>
<p>Where could I get those 4 files (<code>'pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'</code>) or what else do I need to change? These files are not available at the first URL mentioned above</p>
"
70740565,Optimize Albert HuggingFace model,"<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>albert-base-v2</strong> model</p>
<p>Kernel: <code>conda_pytorch_p36</code>.</p>
<p><strong>Section 2.1</strong> exports the finalised model. It too uses a BERT specific function. However, I cannot find an equivalent for Albert.</p>
<p>I've successfully implemented alternatives for Albert up until this section.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code># optimize transformer-based models with onnxruntime-tools
from onnxruntime_tools import optimizer
from onnxruntime_tools.transformers.onnx_model_bert import BertOptimizationOptions

# disable embedding layer norm optimization for better model size reduction
opt_options = BertOptimizationOptions('bert')
opt_options.enable_embed_layer_norm = False
...
</code></pre>
<p><strong>Do functions for Optimizing and Quantizing an Albert model exist?</strong></p>
<p>Update: You can run Quantization in the notebook, without running Optimization. You just need to remove '.opt.' from code, that is an indicative of optimised filenames.</p>
"
70746737,"TypeError: nll_loss_nd(): argument 'input' (position 1) must be Tensor, not tuple","<p>So I'm trying to train my BigBird model (BigBirdForSequenceClassification) and I got to the moment of the training, which ends with below error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\######&quot;, line 189, in &lt;module&gt;
    train_loss, _ = train()  
  File &quot;C:\Users\######&quot;, line 152, in train
    loss = cross_entropy(preds, labels)
  File &quot;C:\Users\#####\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\######\venv\lib\site-packages\torch\nn\modules\loss.py&quot;, line 211, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File &quot;C:\Users\######\venv\lib\site-packages\torch\nn\functional.py&quot;, line 2532, in nll_loss
    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
TypeError: nll_loss_nd(): argument 'input' (position 1) must be Tensor, not tuple
</code></pre>
<p>From what I understand, the problem happens because the train() function returns the tuple. Now - my question is how I should approach such issue? How do I change the output of train() function to return tensor instead of tuple?
I have seen similar issues posted here but none of the solutions seems to be helpful in my case, not even</p>
<pre><code>model = BigBirdForSequenceClassification(config).from_pretrained(checkpoint, return_dict=False)
</code></pre>
<p>(When I don't add return_dict=False I got similiar error message but it says &quot;<code>TypeError: nll_loss_nd(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput</code>&quot;
Please see my train code below:</p>
<pre><code>def train():
    model.train()
    total_loss = 0
    total_preds = []
    
    for step, batch in enumerate(train_dataloader):
        
        if step % 10 == 0 and not step == 0:
            print('Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))
            
        batch = [r.to(device) for r in batch]
        sent_id, mask, labels = batch

        preds = model(sent_id, mask)

        loss = cross_entropy(preds, labels)
        total_loss = total_loss + loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()
        preds = preds.detach().cpu().numpy()
        total_preds.append(preds)

    avg_loss = total_loss / len(train_dataloader)
    total_preds = np.concatenate(total_preds, axis=0)
    return avg_loss, total_preds
</code></pre>
<p>and then:</p>
<pre><code>for epoch in range(epochs):
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))

    train_loss, _ = train()  
    train_losses.append(train_loss)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
</code></pre>
<p>I will really appreciate any help on this case and thank you in advance!</p>
"
70754085,ValueError: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?,"<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>albert-base-v2</strong> model</p>
<p>Kernel: <code>conda_pytorch_p36</code>.</p>
<p><strong>Section 1.2</strong> instantiates a model from files in <code>./MRPC/</code> dir.</p>
<p>However, I <em>think</em> it is for a <strong>BERT</strong> model, not <strong>Albert</strong>. So, I downloaded an Albert <code>config.json</code> file from <a href=""https://huggingface.co/albert-base-v2/tree/main"" rel=""nofollow noreferrer"">here</a>. It is this chnage that causes the error.</p>
<p><strong>What else do I need to do in order to instantiate an Albert model?</strong></p>
<hr />
<p><code>./MRPC/</code> dir:</p>
<pre><code>!curl https://download.pytorch.org/tutorial/MRPC.zip --output MPRC.zip
!unzip -n MPRC.zip
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from os import listdir
from os.path import isfile, join
â€‹
mypath = './MRPC/'
onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]
onlyfiles
---

['tokenizer_config.json',
 'special_tokens_map.json',
 'pytorch_model.bin',
 'config.json',
 'training_args.bin',
 'added_tokens.json',
 'vocab.txt']
</code></pre>
<p>Configs:</p>
<pre class=""lang-py prettyprint-override""><code># The output directory for the fine-tuned model, $OUT_DIR.
configs.output_dir = &quot;./MRPC/&quot;

# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.
configs.data_dir = &quot;./glue_data/MRPC&quot;

# The model name or path for the pre-trained model.
configs.model_name_or_path = &quot;albert-base-v2&quot;
# The maximum length of an input sequence
configs.max_seq_length = 128

# Prepare GLUE task.
configs.task_name = &quot;MRPC&quot;.lower()
configs.processor = processors[configs.task_name]()
configs.output_mode = output_modes[configs.task_name]
configs.label_list = configs.processor.get_labels()
configs.model_type = &quot;albert&quot;.lower()
configs.do_lower_case = True

# Set the device, batch size, topology, and caching flags.
configs.device = &quot;cpu&quot;
configs.eval_batch_size = 1
configs.n_gpu = 0
configs.local_rank = -1
configs.overwrite_cache = False
</code></pre>
<p>Model:</p>
<pre class=""lang-py prettyprint-override""><code>model = AlbertForSequenceClassification.from_pretrained(configs.output_dir)  # !
model.to(configs.device)
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-36-0936fd8cbb17&gt; in &lt;module&gt;
      1 # load model
----&gt; 2 model = AlbertForSequenceClassification.from_pretrained(configs.output_dir)
      3 model.to(configs.device)
      4 
      5 # quantize model

~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1460                     pretrained_model_name_or_path,
   1461                     ignore_mismatched_sizes=ignore_mismatched_sizes,
-&gt; 1462                     _fast_init=_fast_init,
   1463                 )
   1464 

~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, ignore_mismatched_sizes, _fast_init)
   1601             if any(key in expected_keys_not_prefixed for key in loaded_keys):
   1602                 raise ValueError(
-&gt; 1603                     &quot;The state dictionary of the model you are training to load is corrupted. Are you sure it was &quot;
   1604                     &quot;properly saved?&quot;
   1605                 )

ValueError: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?
</code></pre>
"
70769151,Google mT5-small configuration error because number attention heads is not divider of model dimension,"<p>The configuration file for the HuggingFace <strong>google/mt5-small</strong> Model (<a href=""https://huggingface.co/google/mt5-small"" rel=""nofollow noreferrer"">https://huggingface.co/google/mt5-small</a>)</p>
<p>defines</p>
<pre><code>{
...
  &quot;d_model&quot;: 512,
...
  &quot;num_heads&quot;: 6,
...
}
</code></pre>
<p>Link to the config file: <a href=""https://huggingface.co/google/mt5-small/resolve/main/config.json"" rel=""nofollow noreferrer"">https://huggingface.co/google/mt5-small/resolve/main/config.json</a></p>
<p><strong>Question:</strong></p>
<p>As far as I understood, the number of attention-head should be a divider of the model dimension. This is clearly not true in this config file.</p>
<p>Do I misunderstand how self-attention is applied in mT5?</p>
<p>When I use the AllenNLP model (<a href=""https://github.com/allenai/allennlp-models/blob/main/allennlp_models/generation/models/t5.py"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp-models/blob/main/allennlp_models/generation/models/t5.py</a>)
as sequence-to-sequence model, I receive an error message</p>
<p><strong>Summary:</strong></p>
<pre><code>allennlp.common.checks.ConfigurationError: The hidden size (512) is not a multiple of the number of attention heads (6)
</code></pre>
<p><strong>Full</strong></p>
<pre><code>Traceback (most recent call last):
  File &quot;/snap/pycharm-professional/269/plugins/python/helpers/pydev/pydevd.py&quot;, line 1500, in _exec
    runpy._run_module_as_main(module_name, alter_argv=False)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/runpy.py&quot;, line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/__main__.py&quot;, line 50, in &lt;module&gt;
    run()
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/__main__.py&quot;, line 46, in run
    main(prog=&quot;allennlp&quot;)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/commands/__init__.py&quot;, line 123, in main
    args.func(args)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/commands/train.py&quot;, line 112, in train_model_from_args
    train_model_from_file(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/commands/train.py&quot;, line 178, in train_model_from_file
    return train_model(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/commands/train.py&quot;, line 254, in train_model
    model = _train_worker(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/commands/train.py&quot;, line 490, in _train_worker
    train_loop = TrainModel.from_params(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/from_params.py&quot;, line 652, in from_params
    return retyped_subclass.from_params(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/from_params.py&quot;, line 686, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/commands/train.py&quot;, line 766, in from_partial_objects
    model_ = model.construct(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/lazy.py&quot;, line 82, in construct
    return self.constructor(**contructor_kwargs)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/lazy.py&quot;, line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/from_params.py&quot;, line 652, in from_params
    return retyped_subclass.from_params(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/from_params.py&quot;, line 686, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp_models/generation/models/t5.py&quot;, line 32, in __init__
    self.t5 = T5Module.from_pretrained_module(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/modules/transformer/transformer_module.py&quot;, line 251, in from_pretrained_module
    model = cls._from_config(config, **kwargs)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/modules/transformer/t5.py&quot;, line 852, in _from_config
    return cls(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/modules/transformer/t5.py&quot;, line 783, in __init__
    self.encoder: T5EncoderStack = encoder.construct(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/lazy.py&quot;, line 82, in construct
    return self.constructor(**contructor_kwargs)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/modules/transformer/t5.py&quot;, line 600, in basic_encoder
    self_attention=block_self_attention.construct(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/lazy.py&quot;, line 82, in construct
    return self.constructor(**contructor_kwargs)
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/lazy.py&quot;, line 66, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/common/from_params.py&quot;, line 686, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/modules/transformer/attention_module.py&quot;, line 471, in __init__
    super().__init__(
  File &quot;/home/lars/anaconda3/envs/mare2/lib/python3.9/site-packages/allennlp/modules/transformer/attention_module.py&quot;, line 91, in __init__
    raise ConfigurationError(
allennlp.common.checks.ConfigurationError: The hidden size (512) is not a multiple of the number of attention heads (6)
</code></pre>
"
70913886,ALBERT: first word associations,"<p>I have a list of words (e.g., &quot;apple,&quot; &quot;banana,&quot; &quot;mango&quot;) and would like to use ALBERT (<a href=""https://huggingface.co/albert-base-v2"" rel=""nofollow noreferrer"">https://huggingface.co/albert-base-v2</a>) to identify the 10 words that are most strongly associated with each word in my list. In simple terms: &quot;Hey ALBERT, what's the first word that comes to your mind when hearing apple/banana/mango?&quot;</p>
<p>My first idea was using a prompt like &quot;apple is related to [MASK].&quot; but some top predictions are quite weird or not proper words like 'evalle'.</p>
<p>My second idea was to use a k-nearest neighbors approach. However, I don't know how to implement that into the Hugginface transformers. Is it possible to do that without fine-tuning? Do you have another idea?</p>
"
70932914,Hugging face whoami endpoint returns unauthorized,"<p>Can't make <code>/whoami</code> endpoint of hugging face API work. I'm using <code>curl</code> and pass a token that I've got from the UI in the <code>Authorization</code> header.</p>
<pre><code>$ curl https://huggingface.co/api/whoami -H &quot;Authorization: Bearer &lt;token&gt;&quot;
&gt; { &quot;error&quot;: &quot;Unauthorized&quot; }
</code></pre>
<p>At the same time I can use the same token to get private models</p>
<pre class=""lang-sh prettyprint-override""><code>$ curl https://huggingface.co/api/models/private/model -H &quot;Authorization: Bearer &lt;token&gt;&quot;
&gt; {&quot;id&quot;: &quot;private/model&quot;, &quot;modelId&quot;: &quot;private/model&quot;, ...}
</code></pre>
<p>Is there something I do wrong?</p>
"
70957390,Organize data for transformer fine-tuning,"<p>I have a corpus of synonyms and non-synonyms. These are stored in a list of python dictionaries like <code>{&quot;sentence1&quot;: &lt;string&gt;, &quot;sentence2&quot;: &lt;string&gt;, &quot;label&quot;: &lt;1.0 or 0.0&gt; }</code>. Note that this words (or sentences) do not have to be a single token in the tokenizer.</p>
<p>I want to fine-tune a BERT-based model to take both sentences like: <code>[[CLS], &lt;sentence1_token1&gt;], ...,&lt;sentence1_tokenN&gt;, [SEP], &lt;sentence2_token1&gt;], ..., &lt;sentence2_tokenM&gt;, [SEP]]</code> and predict the &quot;label&quot; (a measurement between 0.0 and 1.0).</p>
<p><strong>What is the best approach to organized this data to facilitate the fine-tuning of the huggingface transformer?</strong></p>
"
70960986,Is there any way to change parameters in pretrained DetrFeatureExtractor?,"<p>For my model, I load pretrained version of <a href=""https://huggingface.co/docs/transformers/model_doc/detr#transformers.DetrFeatureExtractor"" rel=""nofollow noreferrer"">DetrFeatureExtractor</a>:</p>
<pre><code>feature_extractor = DetrFeatureExtractor(return_tensors=&quot;pt&quot;
                                        ,do_normalize = True
                                        ,size = 400).from_pretrained(&quot;facebook/detr-resnet-50&quot;)
</code></pre>
<p>But when I output parameters of this variable, I get:</p>
<pre><code>DetrFeatureExtractor {
  &quot;do_normalize&quot;: true,
  &quot;do_resize&quot;: true,
  &quot;feature_extractor_type&quot;: &quot;DetrFeatureExtractor&quot;,
  &quot;format&quot;: &quot;coco_detection&quot;,
  &quot;image_mean&quot;: [
    0.485,
    0.456,
    0.406
  ],
  &quot;image_std&quot;: [
    0.229,
    0.224,
    0.225
  ],
  &quot;max_size&quot;: 1333,
  &quot;size&quot;: 800
}
</code></pre>
<p>which still has <strong>size</strong> = 800. Is that possible to change parameters of pretrained feature extractor and, if yes, how can I change them?</p>
"
70978468,how to get mentions in pytorch NER instead of toknes?,"<p>I am using PyTorch and a pre-trained model.</p>
<p>Here is my code:</p>
<pre><code>class NER(object):
    def __init__(self, model_name_or_path, tokenizer_name_or_path):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
        self.model = AutoModelForTokenClassification.from_pretrained(
            model_name_or_path)
        self.nlp = pipeline(&quot;ner&quot;, model=self.model, tokenizer=self.tokenizer)

    def get_mention_entities(self, query):
        return self.nlp(query)

</code></pre>
<p>when I call <code>get_mention_entities</code> and print its output for &quot;Ø§ÛŒÙ†Ø¬Ø§ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ± Ø§Ø³Øª.&quot;</p>
<p>it gives:</p>
<pre><code>[{'entity': 'B-FAC', 'score': 0.9454591, 'index': 2, 'word': 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡', 'start': 6, 'end': 13}, {'entity': 'I-FAC', 'score': 0.9713519, 'index': 3, 'word': 'ØµÙ†Ø¹ØªÛŒ', 'start': 14, 'end': 19}, {'entity': 'I-FAC', 'score': 0.9860724, 'index': 4, 'word': 'Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ±', 'start': 20, 'end': 28}]
</code></pre>
<p>As you can see, it can recognize the university name, but there are three tokens in the list.</p>
<p>Is there any standard way to combine these tokens based on the &quot;entity&quot; attribute?</p>
<p>desired output is something like:</p>
<pre><code>[{'entity': 'FAC', 'word': 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ±', 'start': 6, 'end': 28}]
</code></pre>
<p>Finally, I can write a function to iterate, compare, and merge the tokens based on the &quot;entity&quot; attribute, but I want a standard way like an internal PyTorch function or something like this.</p>
<p>my question is similar to <a href=""https://stackoverflow.com/questions/57576640/getting-full-names-from-ner"">this question</a>.</p>
<p>PS: &quot;Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ±&quot; is a university name.</p>
"
70990722,Which model/technique to use for specific sentence extraction?,"<p>I have a dataset of tens of thousands of dialogues / conversations between a customer and customer support. These dialogues, which could be forum posts, or long-winded email conversations, have been hand-annotated to highlight the sentence containing the customers problem. For example:</p>
<blockquote>
<p>Dear agent, I am writing to you because I have a very annoying problem with my washing machine. I bought it three weeks ago and was very happy with it. However, this morning the door does not lock properly. Please help</p>
</blockquote>
<blockquote>
<p>Dear customer.... etc</p>
</blockquote>
<p>The highlighted sentence would be:</p>
<blockquote>
<p>However, this morning the door does not lock properly.</p>
</blockquote>
<ol>
<li>What approaches can I take to model this, so that in future I can automatically extract the customers problem? The domain of the datasets are broad, but within the hardware space, so it could be appliances, gadgets, machinery etc.</li>
<li>What is this type of problem called?
I thought this might be called &quot;intent recognition&quot;, but most guides seem to refer to multiclass classification. The sentence either is or isn't the customers problem. I considered analysing each sentence and performing binary classification, but I'd like to explore options that take into account the context of the rest of the conversation if possible.</li>
<li>What resources are available to research how to implement this in Python (using tensorflow or pytorch)</li>
</ol>
<p>I found a <a href=""https://huggingface.co/TODBERT"" rel=""nofollow noreferrer"">model on HuggingFace</a> which has been pre-trained with customer dialogues, and have read the research paper, so I was considering fine-tuning this as a starting point, but I only have experience with text (multiclass/multilabel) classification when it comes to transformers.</p>
"
71009675,'NoneType' error when using PegasusTokenizer,"<p>I tried to use the transformers <code>PegasusTokenizer</code> <code>from_pretrained</code> method to load the &quot;google/pegasus-large&quot; tokenizer:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import PegasusTokenizer

tokenizer = PegasusTokenizer.from_pretrained(&quot;google/pegasus-large&quot;)
</code></pre>
<p>This raised the following error:</p>
<blockquote>
<p>TypeError: 'NoneType' object is not callable</p>
</blockquote>
<p><strong>Important</strong>: I am using a python notebook.</p>
"
71024254,Jupyter kernel dies when importing pipeline function from transformers class on Mac OS,"<p>I'm unable to import pipeline function of transformers class as my jupyter kernel keeps dying. Tried on transformers-4.15.0 and 4.16.2. Anyone faced this issue?</p>
<p>I tried importing the class in a new notebook as you can see in the image and it keeps killing the kernel.</p>
<p><img src=""https://i.stack.imgur.com/Aa1DN.png"" alt=""Jupyter Lab Screenshot"" /></p>
"
71039902,huggingface return probability and class label Trainer.predict,"<p>Is there any way to return probabilities and actual class using <code>Trainer.predict</code> ?</p>
<p>I checked the documentation at this <a href=""https://huggingface.co/docs/transformers/main_classes/trainer"" rel=""nofollow noreferrer"">page</a> but couldn't figure out. As of now it seems to be returning logits</p>
<p>Obviously both probabilities and actual class could be computed using additional coding but wondering if there is any prebuilt method to do the same</p>
<p>my current output as below</p>
<pre><code>new_predictions=trainer.predict(dataset_for_future_predicition_after_tokenizer)

new_predictions


PredictionOutput(predictions=array([[-0.43005577,  3.646306  , -0.8073783 , -1.0651836 , -1.3480505 ,
        -1.108013  ],
       [ 3.5415223 , -0.8513837 , -1.8553216 , -0.18011567, -0.35627165,
        -1.8364134 ],
       [-1.0167522 , -0.8911268 , -1.7115675 ,  0.01204597,  1.7177908 ,
         1.0401527 ],
       [-0.82407415, -0.46043932, -1.089274  ,  2.6252217 ,  0.33935028,
        -1.3623345 ]], dtype=float32), label_ids=None, metrics={'test_runtime': 0.0182, 'test_samples_per_second': 219.931, 'test_steps_per_second': 54.983})
</code></pre>
"
71067376,"When doing pre-training of a transformer model, how can I add words to the vocabulary?","<p>Given a DistilBERT trained language model for a given language, taken from the Huggingface hub, I want to pre-train the model on a specific domain, and I want to add new words that are:</p>
<ul>
<li>definitely non existing in the original training set</li>
<li>and impossible to handle via word piece toeknization - basically you can think of these words as &quot;codes&quot; that are a normalized form of a named entity</li>
</ul>
<p>Consider that:</p>
<ul>
<li>I would like to avoid to learn a <em>new</em> tokenizer: I am fine to add the new words, and then let the model learn their embeddings via pre-training</li>
<li>the number of the &quot;words&quot; is way larger that the &quot;unused&quot; tokens in the &quot;stock&quot; vocabulary</li>
</ul>
<p>The only advice that I have found is the one reported <a href=""https://github.com/huggingface/transformers/issues/237"" rel=""nofollow noreferrer"">here</a>:</p>
<blockquote>
<p>Append it to the end of the vocab, and write a script which generates a new checkpoint that is identical to the pre-trained checkpoint, but but with a bigger vocab where the new embeddings are randomly initialized (for initialized we used tf.truncated_normal_initializer(stddev=0.02)). This will likely require mucking around with some tf.concat() and tf.assign() calls.</p>
</blockquote>
<p>Do you think this is the only way of achieve my goal?</p>
<p>If yes, I do not have any idea of how to write this &quot;script&quot;: does someone has some hints at how to proceeed (sample code, documentation etc)?</p>
"
71078218,ValueError: No gradients provided for any variable (TFCamemBERT),"<p>Currently I am working on Named Entity Recognition in the medical domain using Camembert, precisely using the model: <a href=""https://huggingface.co/jplu/tf-camembert-base"" rel=""nofollow noreferrer"">TFCamembert</a>.</p>
<p>However I have some problems with the fine-tuning of the model for my task as I am using a private dataset not available on Hugging Face.</p>
<p>The data is divided into text files and annotation files. The text file contains for example:</p>
<pre><code>Le cas prÃ©sentÃ© concerne un homme Ã¢gÃ© de 61 ans (71 kg, 172 cm, soit un indice de masse corporelle de 23,9 kg/mÂ²) admissible Ã  une transplantation pulmonaire en raison dâ€™une insuffisance respiratoire chronique terminale sur emphysÃ¨me post-tabagique, sous oxygÃ©nothÃ©rapie continue (1 L/min) et ventilation non invasive nocturne. Il prÃ©sente, comme principaux antÃ©cÃ©dents, une dyslipidÃ©mie, une hypertension artÃ©rielle et un tabagisme sevrÃ© estimÃ© Ã  21 paquets-annÃ©es (facteurs de risque cardiovasculaires). Le bilan prÃ©opÃ©ratoire a rÃ©vÃ©lÃ© une hypertension artÃ©rielle pulmonaire essentiellement postcapillaire conduisant Ã  lâ€™ajout du pÃ©rindopril (2 mg par jour) et du furosÃ©mide (40 mg par jour). La mise en Ã©vidence dâ€™un Elispot (enzyme-linked immunospot) positif pour la tuberculose a motivÃ© lâ€™introduction dâ€™un traitement prophylactique par lâ€™association rifampicine-isoniazide (600-300 mg par jour) pour une durÃ©e de trois mois.
Deux mois aprÃ¨s le bilan prÃ©opÃ©ratoire, le patient a bÃ©nÃ©ficiÃ© dâ€™une transplantation mono-pulmonaire gauche sans dysfonction primaire du greffon5,6. Le donneur et le receveur prÃ©sentaient tous deux un statut sÃ©rologique positif pour cytomegalovirus (CMV) et Epstein Barr Virus (EBV). Une sÃ©rologie positive de la toxoplasmose a Ã©tÃ© mise en Ã©vidence uniquement chez le receveur. Le traitement immunosuppresseur dâ€™induction associait la mÃ©thylprednisolone (500 mg Ã  jour 0 et 375 mg Ã  jour +1 post-transplantation) et le basiliximab, anticorps monoclonal dirigÃ© contre lâ€™interleukine-2 (20 mg Ã  jour 0 et jour +4 posttransplantation). Ã€ partir de jour +2 post-transplantation, lâ€™immunosuppression a Ã©tÃ© maintenue par une trithÃ©rapie par voie orale comprenant le tacrolimus Ã  une posologie initiale de 5 mg par jour, le mofÃ©til mycophÃ©nolate (MMF) 2000 mg par jour et la prednisone 20 mg par jour. Les traitements associÃ©s sont prÃ©sentÃ©s dans le tableau I.
Lâ€™Ã©volution est marquÃ©e par la survenue, au jour +5 posttransplantation, dâ€™une dÃ©gradation respiratoire sur Å“dÃ¨me pulmonaire gauche de reperfusion, avec possible participation cardiogÃ©nique. Le rejet aigu de grade III, Ã©voquÃ© par la prÃ©sence dâ€™infiltrats lymphocytaires aux biopsies transbronchiques, a Ã©tÃ© confirmÃ© par lâ€™anatomopathologie.
</code></pre>
<p>While the annotation file looks like:</p>
<pre><code>T1 genre 28 33 homme
T2 age 41 47 61 ans
A1 genre T1 masculin
T3 origine 127 326 une transplantation pulmonaire en raison dâ€™une insuffisance respiratoire chronique terminale sur emphysÃ¨me post-tabagique, sous oxygÃ©nothÃ©rapie continue (1 L/min) et ventilation non invasive nocturne
T4 issue 1962 2104 une dÃ©gradation respiratoire sur Å“dÃ¨me pulmonaire gauche de reperfusion, avec possible participation cardiogÃ©nique. Le rejet aigu de grade III
A2 issue T4 dÃ©tÃ©rioration
</code></pre>
<p>More details about the prepossessing of <a href=""https://drive.google.com/file/d/1Odq6eTexLg9ZXCjbWbnZiWMGAqg9Ut45/view?usp=sharing"" rel=""nofollow noreferrer"">the data</a> can be found in this <a href=""https://colab.research.google.com/drive/1oqCRFFvzSjDBpfCk5nS2KjBvzCr34Rqz?usp=sharing"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>The things is that the internal loss of my model does not work, if I run the training of the model without declaring a loss, it does not work, I have to define a loss to be able to run the training!</p>
<p>Here is my train_data converted to Tensor slice dataset.</p>
<pre><code>train_label_encodings = tf.convert_to_tensor(train_label_encodings, dtype=tf.int32)
train_label_encodings.data

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_text_encodings.data),
    train_label_encodings.data
))
train_dataset
</code></pre>
<pre><code>&lt;TensorSliceDataset shapes: ({input_ids: (512,), offset_mapping: (512, 2)}, (512,)), types: ({input_ids: tf.int32, offset_mapping: tf.int32}, tf.int32)&gt;
</code></pre>
<p>I define the model:</p>
<pre><code># Import the model and define an optimizer
from transformers import TFAutoModelForTokenClassification, TFCamembertModel, create_optimizer
import tensorflow as tf

num_train_steps = len(train_dataset) * 5
optimizer, lr_schedule = create_optimizer(
    init_lr = 5e-6,
    num_train_steps = num_train_steps,
    weight_decay_rate = 0.01,
    num_warmup_steps = 0
)

metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model = TFAutoModelForTokenClassification.from_pretrained(model_id, num_labels=len(unique_labels), label2id=label2id, id2label=id2label)

model.compile(optimizer=optimizer, metrics=['accuracy'])
</code></pre>
<p>Here is the summary of the model:</p>
<pre><code>Model: &quot;tf_camembert_for_token_classification_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 roberta (TFRobertaMainLayer  multiple                 110031360 
 )                                                               
                                                                 
 dropout_113 (Dropout)       multiple                  0         
                                                                 
 classifier (Dense)          multiple                  25377     
                                                                 
=================================================================
Total params: 110,056,737
Trainable params: 110,056,737
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>When I try to launch the training, I get the following error:</p>
<pre><code>import os
from tensorflow.keras.callbacks import TensorBoard

callbacks = []
callbacks.append(TensorBoard(log_dir=os.path.join(output_dir,&quot;logs&quot;)))

model.fit(
    train_dataset,
    callbacks = callbacks,
    epochs = 3,
)
</code></pre>
<pre><code>Epoch 1/3
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-71-54e2d25b9415&gt; in &lt;module&gt;()
      8     train_dataset,
      9     callbacks = callbacks,
---&gt; 10     epochs = 3,
     11 )

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 878, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 860, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 911, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py&quot;, line 532, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/optimization_tf.py&quot;, line 232, in apply_gradients
        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars), name=name, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py&quot;, line 633, in apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/utils.py&quot;, line 73, in filter_empty_gradients
        raise ValueError(f&quot;No gradients provided for any variable: {variable}. &quot;

    ValueError: No gradients provided for any variable: (['tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/La...
</code></pre>
<p>Any clues to solve this gradient issue? Thanks in advance!</p>
"
71086923,"transformers longformer classification problem with f1, precision and recall classification","<p>I am replicating code from this <a href=""https://github.com/jlealtru/website_tutorials/blob/main/notebooks/Longformer%20with%20IMDB.ipynb"" rel=""nofollow noreferrer"">page</a> and I am getting F1, precision and recall to be 0. I got accuracy as shown by the author. What could be reason?</p>
<p>I looked into <code>compute_metrics</code> function and it seems to be correct. I tried some toy data as below and <code>precision_recall_fscore_support</code> seems to be giving a correct answer</p>
<pre><code>from sklearn.metrics import accuracy_score, precision_recall_fscore_support

y_pred = [1, 1, 2]
y_true = [1, 2, 2]
print (accuracy_score(y_true, y_pred))

precision_recall_fscore_support(y_true, y_pred, average='binary')


0.6666666666666666
(0.5, 1.0, 0.6666666666666666, None)
</code></pre>
<p>as I am getting the accuracy it seems that the below part is working as expected</p>
<pre><code>labels = pred.label_ids
preds = pred.predictions.argmax(-1)

acc = accuracy_score(labels, preds)
</code></pre>
"
71145832,Properly evaluate a test dataset,"<p>I trained a machine translation model using huggingface library:</p>
<pre><code>def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {&quot;bleu&quot;: result[&quot;score&quot;]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result[&quot;gen_len&quot;] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result


trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()

model_dir = './models/'
trainer.save_model(model_dir)
</code></pre>
<p>The code above is taken from this <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb#scrollTo=IreSlFmlIrIm"" rel=""nofollow noreferrer"">Google Colab notebook</a>. After the training, I can see the trained model is saved to the folder <code>models</code> and the metric is calculated. Now I want to load the trained model and do the prediction on a new dataset, here is what I tried:</p>
<pre><code>dataset = load_dataset('csv', data_files='data/training_data.csv')
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Tokenize the test dataset
tokenized_datasets = train_test.map(preprocess_function_v2, batched=True)
test_dataset = tokenized_datasets['test']
model = AutoModelForSeq2SeqLM.from_pretrained('models')
model(test_dataset)
</code></pre>
<p>It threw the following error:</p>
<pre><code>*** AttributeError: 'Dataset' object has no attribute 'size'
</code></pre>
<p>I tried the <code>evaluate()</code> function as well, but it said:</p>
<pre><code>*** torch.nn.modules.module.ModuleAttributeError: 'MarianMTModel' object has no attribute 'evaluate'
</code></pre>
<p>And the function <code>eval</code> only prints the configuration of the model.
What is the proper way to evaluate the performance of the trained model on a new dataset?</p>
"
71147064,ArrowTypeError: Could not convert <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7F2223B6ED10>,"<p>When I execute vit model in google colab, I got some error.</p>
<pre><code>import numpy as np
from datasets import Features, ClassLabel, Array3D

def preprocess_images(examples):
  images = examples['img']  
  images = [np.array(image, dtype=np.uint8) for image in images] 
  images = [np.moveaxis(image, source=-1, destination=0) for image in images] 
  inputs = feature_extractor(images=images) 
  examples['pixel_values'] = inputs['pixel_values'] 

  return examples 

features = Features({ 
  'label': ClassLabel(
      names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']), 
  'img': Array3D(dtype=&quot;int64&quot;, shape=(3,32,32)), 
  'pixel_values': Array3D(dtype=&quot;float32&quot;, shape=(3, 224, 224)), 
}) 

preprocessed_train_ds = train_ds.map(preprocess_images, batched=True, features=features) 
preprocessed_val_ds = val_ds.map(preprocess_images, batched=True, features=features) 
preprocessed_test_ds = test_ds.map(preprocess_images, batched=True, features=features)
</code></pre>
<p>ArrowTypeError: Could not convert &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7F2223B6ED10&gt; with type PngImageFile: was not a sequence or recognized null for conversion to list type</p>
<p>The error occurs at <code>preprocessed_train_ds</code> this line, I guess it might be a feature extraction problem.</p>
<p>use the:</p>
<ol>
<li>huggingface/transformers</li>
<li>ViTFeatureExtractor: google/vit-base-patch16-224-in21k</li>
<li>cifar10, split=['train[:5000]', 'test[:2000]'])
(Consistent with most example settings)</li>
</ol>
<p>Can someone help me? This problem has been bothering me for a long time. :(
Thank you very much.</p>
"
71166789,HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128),"<p>I am trying to fine-tune the BERT language model on my own data. I've gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here's my code:</p>
<pre><code>from datasets import load_dataset
from transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer
import glob
import os


base_path = '../data/'
model_name = 'bert-base-uncased'
max_length = 512
checkpoints_dir = 'checkpoints'

tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)


def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)


dataset = load_dataset('text',
        data_files={
            'train': f'{base_path}train.txt',
            'test': f'{base_path}test.txt',
            'validation': f'{base_path}valid.txt'
        }
)

print('Tokenizing data. This may take a while...')
tokenized_dataset = dataset.map(tokenize_function, batched=True)
train_dataset = tokenized_dataset['train']
eval_dataset = tokenized_dataset['test']

model = AutoModel.from_pretrained(model_name)

training_args = TrainingArguments(checkpoints_dir)

print('Training the model...')
trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
trainer.train()
</code></pre>
<p>I get the following error:</p>
<pre><code>  File &quot;train_lm_hf.py&quot;, line 44, in &lt;module&gt;
    trainer.train()
...
  File &quot;/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py&quot;, line 130, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 165 at dim 1 (got 128)
</code></pre>
<p>What am I doing wrong?</p>
"
71167641,"In Allennlp, how can we set ""sub_module"" argument in PretrainedTransformerMismatchedEmbedder?","<p>As title. I wanted to use BART or T5 as the embedder, and I'd like to use PretrainedTransformerMismatchedEmbedder because after embedding I wanted to do some operations on token level. However I found that the &quot;sub_module&quot; argument, which seems to be needed when using EncDec PLMs as embedder, is not available in the mismatched embedder. How should we set this argument for the mismatched embedder? Or is there a reason that this argument is incompatible with the mismatched embedder?</p>
"
71210238,Speeding-up inference of T5-like model,"<p>I am currently using a model called T0pp (<a href=""https://huggingface.co/bigscience/T0pp"" rel=""nofollow noreferrer"">https://huggingface.co/bigscience/T0pp</a>) in production and would like to speed up inference.</p>
<p>I am running the following code on an on-demand EC2 g4dn.12xlarge instance (4 Nvidia T4 GPUs):</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/T0pp&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience/T0pp&quot;)

model.parallelize()

input_dict = tokenizer(generation_input.inputs, return_tensors=&quot;pt&quot;, padding=True)
inputs = input_dict.input_ids.to(&quot;cuda:0&quot;)
attention_mask = input_dict.attention_mask.to(&quot;cuda:0&quot;)
with torch.no_grad():
    outputs = model.generate(inputs, attention_mask=attention_mask)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
</code></pre>
<p>I wanted to know which alternative you would try in order to speed-up inference, and if you knew good tutorials to do so. The main alternatives I see to speed-up inference would be to use the underlying Pytorch models with:</p>
<ul>
<li>ONNX</li>
<li>Deepspeed</li>
<li>or using fp16 instead of fp32 parameters (with the main drawback of losing some quality)</li>
</ul>
<p>Would someone have experience in using these tools, and would know which is the best / simplest option?</p>
<p>All this is quite new for me, and I must admit I've been a bit lost in ONNX and Deepspeed tutorials.</p>
<p>PS:</p>
<ul>
<li>I already tried SageMaker, but this is not working for huge models like T0pp (40Gb).</li>
<li>Batching speeds up things, allowing to go from 1-2 seconds for batch
size 1, to 16 seconds for batch size 32. In an ideal world, even
batch size 32 would be under 1 or 2 seconds.</li>
</ul>
"
71267212,Is there a way to select a device when running a python script on Google Colab?,"<p>I am attempting to run run_language_modeling.py which is a <a href=""https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py"" rel=""nofollow noreferrer"">python script from hugging face</a>. However, when I try to run it, I've noticed I'm solely using my CPU instead of the GPU (even though the environment is set to use this. So I'm looking for a way to tell the script to use the GPU.</p>
<p>Here's what I have...</p>
<p>To verify that I am using a GPU: <code>!nvidia-smi</code></p>
<p>This shows:</p>
<pre><code>Fri Feb 25 11:55:13 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   32C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>Then, I'm running the following script which calls the <code>.py</code> file:</p>
<pre><code>!python ./run_language_modeling.py \
    --output_dir=output \
    --model_type=bert \
    --do_train \
    --train_data_file=train.txt \
    --do_eval \
    --eval_data_file=test.txt \
    --per_gpu_train_batch_size 8 \
    --per_gpu_eval_batch_size 4 \
    --num_train_epochs 20 \
    --output_dir ./ \
    --save_steps 1000 \
    --save_total_limit 2 \
    --mlm \
    --overwrite_output_dir \
    --block_size 128 \
    --line_by_line \
    --tokenizer_name bert-base-uncased 
</code></pre>
<p>This continues until the CPU usage goes up to 100%. I assume there might be something like <code>--device</code> but I haven't been able to found it. Some other posts I've seen online mention I can do:</p>
<pre><code>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;1&quot;
tf_device='/gpu:0'
</code></pre>
<p>To select the GPU I want, but it's not really doing anything that I can tell. I also tried doing:</p>
<pre><code>%%shell

export CUDA_VISIBLE_DEVICES=0
</code></pre>
<p>Any suggestions?</p>
"
71318599,"BERT Classifier ValueError: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([4, 2]))","<p>I'm training a Classifier Model but it's a few days that I cannot overcame a problem!
I have the ValueError: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([4, 2])) error but actually it seems correct to me ! Indeed I used unsqueeze(1) to put them of the same size. WHat else I can try? Thank you!</p>
<pre><code>class SequenceClassifier(nn.Module):

  def __init__(self, n_classes):
    super(SequenceClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)
    self.drop = nn.Dropout(p=0.3)
    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
  
  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask
    ) 
    output = self.drop(pooled_output)
    return self.out(output)

model = SequenceClassifier(len(class_names))
model = model.to(device)

EPOCHS = 10

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)
weights=[0.5,1]
pos_weight=torch.FloatTensor(weights).to(device)
loss_fn=nn.BCEWithLogitsLoss(pos_weight=pos_weight)

def train_epoch(
  model, 
  data_loader, 
  loss_fn, 
  optimizer, 
  device, 
  scheduler, 
  n_examples
):
  model = model.train()

  losses = []
  correct_predictions = 0
  
  for d in data_loader:
    input_ids = d[&quot;input_ids&quot;].to(device)
    attention_mask = d[&quot;attention_mask&quot;].to(device)
    targets = d[&quot;targets&quot;].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )

    _, preds = torch.max(outputs, dim=1)
    
    targets = targets.unsqueeze(1)
    loss = loss_fn(outputs, targets)
    

    correct_predictions += torch.sum(preds == targets)
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()

  return correct_predictions.double() / n_examples, np.mean(losses)
  

%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,    
    loss_fn, 
    optimizer, 
    device, 
    scheduler, 
    len(df_train)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn, 
    device, 
    len(df_val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc &gt; best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc
</code></pre>
<pre><code>ValueError: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([4, 2]))
</code></pre>
<p><strong>EDIT</strong>
I have a binary classifier problem, indeed i have 2 classes encoded 0 (&quot;bad&quot;) and 1 (&quot;good&quot;).</p>
"
71335585,"HuggingFace | ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet con","<p>Not always, but occasionally when running my code this error appears.</p>
<p>At first, I doubted it was a connectivity issue but to do with cashing issue, as discussed on an older <a href=""https://github.com/huggingface/transformers/issues/8690"" rel=""nofollow noreferrer"">Git Issue</a>.</p>
<p>Clearing cache didn't help runtime:</p>
<pre class=""lang-sh prettyprint-override""><code>$ rm ~/.cache/huggingface/transformers/ *
</code></pre>
<p>Traceback references:</p>
<ul>
<li>NLTK also gets <code>Error loading stopwords: &lt;urlopen error [Errno -2] Name or service not known</code>.</li>
<li>Last 2 lines re <code>cached_path</code> and <code>get_from_cache</code>.</li>
</ul>
<hr />
<p>Cache (before cleared):</p>
<pre class=""lang-sh prettyprint-override""><code>$ cd ~/.cache/huggingface/transformers/
(sdg) me@PF2DCSXD:~/.cache/huggingface/transformers$ ls
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.json
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.json
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.lock
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.json
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.json
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.json
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock
</code></pre>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, set_seed

generator = pipeline('text-generation', model='gpt2')  # Error
set_seed(42)
</code></pre>
<p>Traceback:</p>
<pre><code>2022-03-03 10:18:06.803989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-03-03 10:18:06.804057: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[nltk_data] Error loading stopwords: &lt;urlopen error [Errno -2] Name or
[nltk_data]     service not known&gt;
2022-03-03 10:18:09.216627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-03-03 10:18:09.216700: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-03-03 10:18:09.216751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PF2DCSXD): /proc/driver/nvidia/version does not exist
2022-03-03 10:18:09.217158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-03 10:18:09.235409: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
All model checkpoint layers were used when initializing TFGPT2LMHeadModel.

All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
Traceback (most recent call last):
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/mnt/c/Users/me/Documents/GitHub/project/foo/bar/__main__.py&quot;, line 26, in &lt;module&gt;
    nlp_setup()
  File &quot;/mnt/c/Users/me/Documents/GitHub/project/foo/bar/utils/Modeling.py&quot;, line 37, in nlp_setup
    generator = pipeline('text-generation', model='gpt2')
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/pipelines/__init__.py&quot;, line 590, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 463, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 324, in get_tokenizer_config
    resolved_config_file = get_file_from_repo(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 2235, in get_file_from_repo
    resolved_file = cached_path(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 1846, in cached_path
    output_path = get_from_cache(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 2102, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
</code></pre>
<hr />
<p><strong>Failed Attempts</strong></p>
<ol>
<li>I closed my IDE and bash terminal. Ran <code>wsl.exe --shutdown</code> in PowerShell. Relaunched IDE and bash terminal with same error.</li>
<li>Disconnecting/ different VPN.</li>
<li>Clear cache <code>$ rm ~/.cache/huggingface/transformers/ *</code>.</li>
</ol>
"
71338307,query() of generator `max_length` being succeeded,"<p>Goal: set <code>min_length</code> and <code>max_length</code> in Hugging Face Transformers generator query.</p>
<p>I've passed <code>50, 200</code> as these parameters. Yet, the length of my outputs are much higher...</p>
<p>There's no runtime failure.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)

def query(payload, multiple, min_char_len, max_char_len):
    print(min_char_len, max_char_len)
    list_dict = generator(payload, min_length=min_char_len, max_length=max_char_len, num_return_sequences=multiple)
    test = [d['generated_text'].split(payload)[1].strip() for d in list_dict]
    for t in test: print(len(t))
    return test

query('example', 1, 50, 200)
</code></pre>
<p>Output:</p>
<pre><code>50 200
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
1015
</code></pre>
"
71338750,what is the difference between using a hugging face estimator with training script and directly using a notebook in AWS sagemaker?,"<p>in tutorials like <a href=""https://aws.amazon.com/blogs/machine-learning/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">Fine-tuning a pytorch bert model and deploying it with sagemaker</a> and <a href=""https://aws.amazon.com/blogs/machine-learning/fine-tune-and-host-hugging-face-bert-models-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">fine-tune and host huggingface models on sagemaker</a>, a hugging face estimator is used to call a training script. What would be the difference if I just directly ran the script's code in the notebook itself? is it because the estimator makes it easier to deploy the model?</p>
"
71374635,"Wav2Vec2ForCTC were not initialized from the mode, pass 'sampling_rate' argument","<p>my code kind of working (it is listening and capturing my voice) but is warning me!! I am keeping having this issue with the code as:</p>
<pre><code>1)
UserWarning: positional arguments and argument &quot;destination&quot; are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
warnings.warn(
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>Listening Now..</p>
<pre><code>2)
It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.
</code></pre>
<hr />
<pre><code>import torch
import speech_recognition as sr
import io
from pydub import AudioSegment
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

tokenizer = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
model = Wav2Vec2ForCTC.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
recognizer = sr.Recognizer()

while True:
    audio = recognizer.listen(source)
    data = io.BytesIO(audio.get_wav_data())
    clip = AudioSegment.from_file(data)
    tensor = torch.FloatTensor(clip.get_array_of_samples())

    inputs = tokenizer(tensor, sample_rate=16000, return_tensors=&quot;pt&quot;, padding=&quot;longest&quot;).input_values
    logits = model(inputs).logits
    tokens = torch.argmax(logits, dim=-1)
    text = tokenizer.batch_decode(tokens)

    print(str(text).lower())
</code></pre>
"
71398882,CUDA: RuntimeError: CUDA out of memory - BERT sagemaker,"<p>I have been trying to train a BertSequenceForClassification Model using AWS Sagemaker. i'm using hugging face estimators. but I keep getting the error: <code>RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.73 GiB already allocated; 87.88 MiB free; 10.77 GiB reserved in total by PyTorch)</code> the same code runs fine on my laptop.</p>
<ol>
<li>how do I check what is occupying that 10GB of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). When I run nvidia-smi, i can only see &quot;No processes running&quot; and the GPU memory usage is zero. When I run <code>print(torch.cuda.memory_summary(device=None, abbreviated=False))</code> from within my training script (right before it throws the error) it prints</li>
</ol>
<pre><code>|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|
</code></pre>
<p>but I have no idea what it means or how to interpret it</p>
<ol start=""2"">
<li>when i run <code>!df -h</code> I can see:</li>
</ol>
<pre><code>Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         30G   72K   30G   1% /dev
tmpfs            30G     0   30G   0% /dev/shm
/dev/xvda1      109G   93G   16G  86% /
/dev/xvdf       196G   61M  186G   1% /home/ec2-user/SageMaker
</code></pre>
<p>how is this memory different from the GPU? if theres 200GB in /dev/xvdf is there anyway I can just use that..? in my test script I tried<br />
<code>model = BertForSequenceClassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(&quot;cpu&quot;)</code>
but that just gives the same error</p>
"
71401458,"Hugginface Dataloader BERT ValueError: too many values to unpack (expected 2), AX hyperparameters tuning with Pytorch","<p>I'm stacked with this error from one week now, I tried everything so the fact is that I'm not understanding deeply what is happening (I'm new at pytorch implementation). Anyway I'm trying to implement a Bert Classifier to discriminate between 2 sequences classes, with AX hyperparameters tuning.
This is all my code implemented anticipated by a sample of my datasets ( I have 3 csv, train-test-val). Thank you very much !</p>
<pre><code>                                           0        1
M A T T D R P T P D G T D A I D L T T R V R R...    1
M K K L F Q T E P L L E L F N C N E L R I I G...    0
M L V A A A V C P H P P L L I P E L A A G A A...    1
M I V A W G N S G S G L L I L I L S L A V S A...    0
M V E E G R R L A A L H P N I V V K L P T T E...    1
M G S K V S K N A L V F N V L Q A L R E G L T...    1
M P S K E T S P A E R M A R D E Y Y M R L A M...    1
M V K E Y A L E W I D G Y R E R L V K V S D A...    1
M G T A A S Q D R A A M A E A A Q R V G D S F...    0
</code></pre>
<pre><code>
df_train=pd.read_csv('CLASSIFIER_train',sep=',',header=None)
df_train
class SequenceDataset(Dataset):

  def __init__(self, sequences, targets, tokenizer, max_len):
    self.sequences = sequences
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.sequences)
  
  def __getitem__(self, item):
    sequences = str(self.sequences[item])
    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(
      sequences,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )


    return {
      'sequences_text': sequences,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }

def create_data_loader(df, tokenizer, max_len, batch_size):
  ds = SequenceDataset(
    sequences=df[0].to_numpy(),
    targets=df[1].to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=2,
    shuffle=True
  )

BATCH_SIZE = 16

train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

def net_train(net, train_data_loader, parameters, dtype, device):
  net.to(dtype=dtype, device=device)

  # Define loss and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(net.parameters(), # or any optimizer you prefer 
                        lr=parameters.get(&quot;lr&quot;, 0.001), # 0.001 is used if no lr is specified
                        momentum=parameters.get(&quot;momentum&quot;, 0.9)
  )

  scheduler = optim.lr_scheduler.StepLR(
      optimizer,
      step_size=int(parameters.get(&quot;step_size&quot;, 30)),
      gamma=parameters.get(&quot;gamma&quot;, 1.0),  # default is no learning rate decay
  )

  num_epochs = parameters.get(&quot;num_epochs&quot;, 3) # Play around with epoch number
  # Train Network
  for _ in range(num_epochs):
      for inputs, labels in train_data_loader:
          # move data to proper dtype and device
          inputs = inputs.to(dtype=dtype, device=device)
          labels = labels.to(device=device)

          # zero the parameter gradients
          optimizer.zero_grad()

          # forward + backward + optimize
          outputs = net(inputs)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
          scheduler.step()
  return net
  
def init_net(parameterization):

    model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) 

    # The depth of unfreezing is also a hyperparameter
    for param in model.parameters():
        param.requires_grad = False # Freeze feature extractor
        
    Hs = 512 # Hidden layer size; you can optimize this as well
                                  
    model.fc = nn.Sequential(nn.Linear(2048, Hs), # attach trainable classifier
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(Hs, 10),
                                 nn.LogSoftmax(dim=1))
    return model # return untrained model

def train_evaluate(parameterization):

    # constructing a new training data loader allows us to tune the batch size


    train_data_loader=create_data_loader(df_train, tokenizer, MAX_LEN, batch_size=parameterization.get(&quot;batchsize&quot;, 32))
    
    
    # Get neural net
    untrained_net = init_net(parameterization) 
    
    # train
    trained_net = net_train(net=untrained_net, train_data_loader=train_data_loader, 
                            parameters=parameterization, dtype=dtype, device=device)
    
    # return the accuracy of the model as it was trained in this run
    return evaluate(
        net=trained_net,
        data_loader=test_data_loader,
        dtype=dtype,
        device=device,
    )

classes=('0','1')

dtype = torch.float
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

best_parameters, values, experiment, model = optimize(
    parameters=[
        {&quot;name&quot;: &quot;lr&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [1e-6, 0.4], &quot;log_scale&quot;: True},
        {&quot;name&quot;: &quot;batchsize&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [16, 128]},
        {&quot;name&quot;: &quot;momentum&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0]},
        #{&quot;name&quot;: &quot;max_epoch&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [1, 30]},
        #{&quot;name&quot;: &quot;stepsize&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [20, 40]},        
    ],
  
    evaluation_function=train_evaluate,
    objective_name='accuracy',
)

print(best_parameters)
means, covariances = values
print(means)
print(covariances)
</code></pre>
<pre><code>  File &quot;&lt;ipython-input-71-e52ebc0d7b5b&gt;&quot;, line 14, in train_evaluate
    parameters=parameterization, dtype=dtype, device=device)
  File &quot;&lt;ipython-input-61-66c57e7138fa&gt;&quot;, line 20, in net_train
    for inputs, labels in train_data_loader:
ValueError: too many values to unpack (expected 2)
</code></pre>
"
71411065,Use `sentence-transformers` inside of a keras model,"<p>I would like to use a model from <code>sentence-transformers</code> inside of a larger Keras model.</p>
<p>Here is the full example:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModel


MODEL_PATH = 'sentence-transformers/all-MiniLM-L6-v2'
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = TFAutoModel.from_pretrained(MODEL_PATH, from_pt=True)


class SBert(tf.keras.layers.Layer):
    def __init__(self, tokenizer, model):
        super(SBert, self).__init__()
        
        self.tokenizer = tokenizer
        self.model = model
        
    def tf_encode(self, inputs):
        def encode(inputs):
            return self.tokenizer(
                inputs, padding=True, truncation=True, return_tensors='tf'
            )
        return tf.py_function(func=encode, inp=[inputs], Tout=[tf.int64])
    
    def mean_pooling(model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = tf.cast(
            tf.broadcast_to(tf.expand_dims(attention_mask, -1), token_embeddings.shape),
            tf.float32
        )
        a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1)
        b = tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max)
        embeddings = a / b
        embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)
        return embeddings

    def call(self, inputs):
        encoded_input = self.tf_encode(inputs)
        model_output = self.model(encoded_input)
        embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])
        return embeddings
    
    
sbert = SBert(tokenizer, model)
sbert(['some text', 'more text'])
</code></pre>
<p>I am able to use the model and tokenizer outside of TF / Keras with no problems, the issue seems to happen when we try and build the graph and TF passing a symbolic tensor to the tokenizer, generating an error - this is why I have tried to wrap in <code>tf.py_function</code> but with no success...</p>
<p>The error:</p>
<pre><code>---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-20-a0c4a906e456&gt; in &lt;module&gt;
     44 
     45 sbert = SBert(tokenizer, model)
---&gt; 46 sbert(['some text', 'more text'])

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

&lt;ipython-input-20-a0c4a906e456&gt; in call(self, inputs)
     36     def call(self, inputs):
     37         tf.print(inputs, output_stream=sys.stdout)
---&gt; 38         encoded_input = self.tf_encode(inputs)
     39         tf.print(encoded_input, output_stream=sys.stdout)
     40         model_output = self.model(encoded_input)

&lt;ipython-input-20-a0c4a906e456&gt; in tf_encode(self, inputs)
     20                 inputs, padding=True, truncation=True, return_tensors='tf'
     21             )
---&gt; 22         return tf.py_function(func=encode, inp=[inputs], Tout=[tf.int64])
     23 
     24     def mean_pooling(model_output, attention_mask):

InvalidArgumentError: Exception encountered when calling layer &quot;s_bert_6&quot; (type SBert).

ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
Traceback (most recent call last):

  File &quot;/Users/dennisyurkevich/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py&quot;, line 269, in __call__
    return func(device, token, args)

  File &quot;/Users/dennisyurkevich/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py&quot;, line 147, in __call__
    outputs = self._call(device, args)

  File &quot;/Users/dennisyurkevich/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py&quot;, line 154, in _call
    ret = self._func(*args)

  File &quot;/Users/dennisyurkevich/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py&quot;, line 642, in wrapper
    return func(*args, **kwargs)

  File &quot;&lt;ipython-input-20-a0c4a906e456&gt;&quot;, line 20, in encode
    inputs, padding=True, truncation=True, return_tensors='tf'

  File &quot;/Users/dennisyurkevich/.pyenv/versions/3.7.8/lib/python3.7/site-packages/transformers/tokenization_utils_base.py&quot;, line 2378, in __call__
    &quot;text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) &quot;

ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).

 [Op:EagerPyFunc]

Call arguments received:
  â€¢ inputs=[&quot;'some text'&quot;, &quot;'more text'&quot;]

</code></pre>
"
71414627,The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: <unknown>,"<p>I am having trouble when switching a model from some local dummy data to using a TF dataset.</p>
<p>Sorry for the long model code, I have tried to shorten it as much as possible.</p>
<p>The following works fine:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow_recommenders as tfrs
from transformers import AutoTokenizer, TFAutoModel


MODEL_PATH = 'sentence-transformers/all-MiniLM-L6-v2'
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = TFAutoModel.from_pretrained(MODEL_PATH, from_pt=True)


class SBert(tf.keras.layers.Layer):
    def __init__(self, tokenizer, model):
        super(SBert, self).__init__()
        
        self.tokenizer = tokenizer
        self.model = model
        
    def tf_encode(self, inputs):
        def encode(inputs):
            inputs = [x[0].decode(&quot;utf-8&quot;) for x in inputs.numpy()]
            outputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors='tf')
            return outputs['input_ids'], outputs['token_type_ids'], outputs['attention_mask']
        return tf.py_function(func=encode, inp=[inputs], Tout=[tf.int32, tf.int32, tf.int32])
    
    def process(self, i, t, a):
        def __call(i, t, a):
            model_output = self.model(
                {'input_ids': i.numpy(), 'token_type_ids': t.numpy(), 'attention_mask': a.numpy()}
            )
            return model_output[0]
        return tf.py_function(func=__call, inp=[i, t, a], Tout=[tf.float32])

    def mean_pooling(self, model_output, attention_mask):
        token_embeddings = tf.squeeze(tf.stack(model_output), axis=0)
        input_mask_expanded = tf.cast(
            tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)),
            tf.float32
        )
        a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1)
        b = tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max)
        embeddings = a / b
        embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)
        return embeddings
    
    def call(self, inputs):
        input_ids, token_type_ids, attention_mask = self.tf_encode(inputs)
        model_output = self.process(input_ids, token_type_ids, attention_mask)
        embeddings = self.mean_pooling(model_output, attention_mask)
        return embeddings


sbert = SBert(tokenizer, model)
inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)
outputs = sbert(inputs)
model = tf.keras.Model(inputs, outputs)
model(tf.constant(['some text', 'more text']))
</code></pre>
<p>The call to the model outputs tensors - yipee :)</p>
<p>Now I want to use this layer inside of a larger two tower model:</p>
<pre class=""lang-py prettyprint-override""><code>class Encoder(tf.keras.Model):
    def __init__(self):
        super().__init__()
        
        self.text_embedding = self._build_text_embedding()
    
    def _build_text_embedding(self):
        sbert = SBert(tokenizer, model)
        inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)
        outputs = sbert(inputs)
        return tf.keras.Model(inputs, outputs)
    
    def call(self, inputs):
        return self.text_embedding(inputs)

    
class RecModel(tfrs.models.Model):
    def __init__(self):
        super().__init__()
        
        self.query_model = tf.keras.Sequential([
            Encoder(),
            tf.keras.layers.Dense(32)
        ])
        
        self.candidate_model = tf.keras.Sequential([
            Encoder(),
            tf.keras.layers.Dense(32)
        ])
    
        self.retrieval_task = tfrs.tasks.Retrieval(
            metrics=tfrs.metrics.FactorizedTopK(
                candidates=tf.data.Dataset.from_tensor_slices(
                    data['text']
                ).batch(1).map(self.candidate_model),
            ),
            batch_metrics=[
                tf.keras.metrics.TopKCategoricalAccuracy(k=5)
            ]
        )

    def call(self, features):
        query_embeddings = self.query_model(features['query'])
        candidate_embeddings = self.candidate_model(features['text'])
        return (
            query_embeddings,
            candidate_embeddings,
        )   

    def compute_loss(self, features, training=False):
        query_embeddings, candidate_embeddings = self(features)
        retrieval_loss = self.retrieval_task(query_embeddings, candidate_embeddings)
        return retrieval_loss
</code></pre>
<p>Create a small dummy dataset:</p>
<pre class=""lang-py prettyprint-override""><code>data = {
    'query': ['blue', 'cat', 'football'],
    'text': ['a nice colour', 'a type of animal', 'a sport']
}

ds = tf.data.Dataset.from_tensor_slices(data).batch(1)
</code></pre>
<p>Try to compile:</p>
<pre class=""lang-py prettyprint-override""><code>model = RecModel()
model.compile(optimizer=tf.keras.optimizers.Adagrad())
</code></pre>
<p>And we hit the following error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-11-df4cc46e0307&gt; in &lt;module&gt;
----&gt; 1 model = RecModel()
      2 model.compile(optimizer=tf.keras.optimizers.Adagrad())

&lt;ipython-input-8-a774041744b9&gt; in __init__(self)
     33                 candidates=tf.data.Dataset.from_tensor_slices(
     34                     data['text']
---&gt; 35                 ).batch(1).map(self.candidate_model),
     36             ),
     37             batch_metrics=[

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic, name)
   2014         warnings.warn(&quot;The `deterministic` argument has no effect unless the &quot;
   2015                       &quot;`num_parallel_calls` argument is specified.&quot;)
-&gt; 2016       return MapDataset(self, map_func, preserve_cardinality=True, name=name)
   2017     else:
   2018       return ParallelMapDataset(

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)
   5193         self._transformation_name(),
   5194         dataset=input_dataset,
-&gt; 5195         use_legacy_function=use_legacy_function)
   5196     self._metadata = dataset_metadata_pb2.Metadata()
   5197     if name:

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
    269         fn_factory = trace_tf_function(defun_kwargs)
    270 
--&gt; 271     self._function = fn_factory()
    272     # There is no graph to add in eager mode.
    273     add_to_graph &amp;= not context.executing_eagerly()

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)
   3069     &quot;&quot;&quot;
   3070     graph_function = self._get_concrete_function_garbage_collected(
-&gt; 3071         *args, **kwargs)
   3072     graph_function._garbage_collector.release()  # pylint: disable=protected-access
   3073     return graph_function

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   3034       args, kwargs = None, None
   3035     with self._lock:
-&gt; 3036       graph_function, _ = self._maybe_define_function(args, kwargs)
   3037       seen_names = set()
   3038       captured = object_identity.ObjectIdentitySet(

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3290 
   3291           self._function_cache.add_call_context(cache_key.call_context)
-&gt; 3292           graph_function = self._create_graph_function(args, kwargs)
   3293           self._function_cache.add(cache_key, cache_key_deletion_observer,
   3294                                    graph_function)

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3138             arg_names=arg_names,
   3139             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3140             capture_by_value=self._capture_by_value),
   3141         self._function_attributes,
   3142         function_spec=self.function_spec,

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)
   1159         _, original_func = tf_decorator.unwrap(python_func)
   1160 
-&gt; 1161       func_outputs = python_func(*func_args, **func_kwargs)
   1162 
   1163       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in wrapped_fn(*args)
    246           attributes=defun_kwargs)
    247       def wrapped_fn(*args):  # pylint: disable=missing-docstring
--&gt; 248         ret = wrapper_helper(*args)
    249         ret = structure.to_tensor_list(self._output_structure, ret)
    250         return [ops.convert_to_tensor(t) for t in ret]

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in wrapper_helper(*args)
    175       if not _should_unpack(nested_args):
    176         nested_args = (nested_args,)
--&gt; 177       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
    178       if _should_pack(ret):
    179         ret = tuple(ret)

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    687       try:
    688         with conversion_ctx:
--&gt; 689           return converted_call(f, args, kwargs, options=options)
    690       except Exception as e:  # pylint:disable=broad-except
    691         if hasattr(e, 'ag_error_metadata'):

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    375 
    376   if not options.user_requested and conversion.is_allowlisted(f):
--&gt; 377     return _call_unconverted(f, args, kwargs, options)
    378 
    379   # internal_convert_user_code is for example turned off when issuing a dynamic

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)
    456 
    457   if kwargs is not None:
--&gt; 458     return f(*args, **kwargs)
    459   return f(*args)
    460 

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/keras/layers/core/dense.py in build(self, input_shape)
    137     last_dim = tf.compat.dimension_value(input_shape[-1])
    138     if last_dim is None:
--&gt; 139       raise ValueError('The last dimension of the inputs to a Dense layer '
    140                        'should be defined. Found None. '
    141                        f'Full input shape received: {input_shape}')

ValueError: Exception encountered when calling layer &quot;sequential_5&quot; (type Sequential).

The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: &lt;unknown&gt;

Call arguments received:
  â€¢ inputs=tf.Tensor(shape=(None,), dtype=string)
  â€¢ training=None
  â€¢ mask=None
</code></pre>
<p>I am not quite sure where I should set the shape - as using regular tensors and not TF dataset works ok.</p>
"
71456511,Pytorch GPU Out of memory on example script,"<p>I tried running <a href=""https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">the example script</a> from <strong>official</strong> huggingface transformers repository with installed Python 3.10.2, PyTorch 1.11.0 and CUDA 11.3 for Sber GPT-3 Large.
Without any file modifications I ran this script with arguments:</p>
<pre><code>--output_dir out --model_name_or_path sberbank-ai/rugpt3large_based_on_gpt2 --train_file dataset.txt --do_train --num_train_epochs 15 --overwrite_output_dir 
</code></pre>
<p>and got</p>
<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB
</code></pre>
<p>also tried <code>--block_size 32</code> and <code>--per_device_train_batch_size 1</code> but unsuccessfully.
My GPU is RTX 2060 6GB. Maybe it's a real lack of video memory? Can it be solved without buying a new GPU?</p>
"
71486242,AttributeError: 'T5Config' object has no attribute 'adapters',"<p>How to solve this error? I've created the <strong>.pkl</strong> object of the <strong>T5-base</strong> model and tried to execute it but suddenly I got this error message. I wondered a bit, tried to google it but didn't get any reason why I got this error!!!!</p>
"
71532653,understanding gpu usage huggingface classification,"<p>I am building a classifier using huggingface and would like to understand the line <code>Total train batch size (w. parallel, distributed &amp; accumulation) = 64</code> from below</p>
<pre><code> Num examples = 7000
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 327
</code></pre>
<p>i have 7000 rows of data, i have defined epochs to be 3 and <code> per_device_train_batch_size = 4</code> and  <code>per_device_eval_batch_size= 16</code>. I also get that <code>Total optimization steps = 327</code> - (7000*3/64)</p>
<p>But I am not clear about <code>Total train batch size (w. parallel, distributed &amp; accumulation) = 64</code>. Does it mean that there are 16 devices as 16*4(<code>Instantaneous batch size per device = 4</code>) comes to 64?</p>
"
71577525,huggingface sequence classification unfreezing layers,"<p>I am using longformer for sequence classification - binary problem</p>
<p>I have downloaded required files</p>
<pre><code># load model and tokenizer and define length of the text sequence
model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',
                                                           gradient_checkpointing=False,
                                                           attention_window = 512)
tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)
</code></pre>
<p>then as shown <a href=""https://discuss.huggingface.co/t/freeze-lower-layers-with-auto-classification-model/11386/2"" rel=""nofollow noreferrer"">here</a> I ran the below code</p>
<pre><code>for name, param in model.named_parameters():
     print(name, param.requires_grad)


longformer.embeddings.word_embeddings.weight True
longformer.embeddings.position_embeddings.weight True
longformer.embeddings.token_type_embeddings.weight True
longformer.embeddings.LayerNorm.weight True
longformer.embeddings.LayerNorm.bias True
longformer.encoder.layer.0.attention.self.query.weight True
longformer.encoder.layer.0.attention.self.query.bias True
longformer.encoder.layer.0.attention.self.key.weight True
longformer.encoder.layer.0.attention.self.key.bias True
longformer.encoder.layer.0.attention.self.value.weight True
longformer.encoder.layer.0.attention.self.value.bias True
longformer.encoder.layer.0.attention.self.query_global.weight True
longformer.encoder.layer.0.attention.self.query_global.bias True
longformer.encoder.layer.0.attention.self.key_global.weight True
longformer.encoder.layer.0.attention.self.key_global.bias True
longformer.encoder.layer.0.attention.self.value_global.weight True
longformer.encoder.layer.0.attention.self.value_global.bias True
longformer.encoder.layer.0.attention.output.dense.weight True
longformer.encoder.layer.0.attention.output.dense.bias True
longformer.encoder.layer.0.attention.output.LayerNorm.weight True
longformer.encoder.layer.0.attention.output.LayerNorm.bias True
longformer.encoder.layer.0.intermediate.dense.weight True
longformer.encoder.layer.0.intermediate.dense.bias True
longformer.encoder.layer.0.output.dense.weight True
longformer.encoder.layer.0.output.dense.bias True
longformer.encoder.layer.0.output.LayerNorm.weight True
longformer.encoder.layer.0.output.LayerNorm.bias True
longformer.encoder.layer.1.attention.self.query.weight True
longformer.encoder.layer.1.attention.self.query.bias True
longformer.encoder.layer.1.attention.self.key.weight True
longformer.encoder.layer.1.attention.self.key.bias True
longformer.encoder.layer.1.attention.self.value.weight True
longformer.encoder.layer.1.attention.self.value.bias True
longformer.encoder.layer.1.attention.self.query_global.weight True
longformer.encoder.layer.1.attention.self.query_global.bias True
longformer.encoder.layer.1.attention.self.key_global.weight True
longformer.encoder.layer.1.attention.self.key_global.bias True
longformer.encoder.layer.1.attention.self.value_global.weight True
longformer.encoder.layer.1.attention.self.value_global.bias True
longformer.encoder.layer.1.attention.output.dense.weight True
longformer.encoder.layer.1.attention.output.dense.bias True
longformer.encoder.layer.1.attention.output.LayerNorm.weight True
longformer.encoder.layer.1.attention.output.LayerNorm.bias True
longformer.encoder.layer.1.intermediate.dense.weight True
longformer.encoder.layer.1.intermediate.dense.bias True
longformer.encoder.layer.1.output.dense.weight True
longformer.encoder.layer.1.output.dense.bias True
longformer.encoder.layer.1.output.LayerNorm.weight True
longformer.encoder.layer.1.output.LayerNorm.bias True
longformer.encoder.layer.2.attention.self.query.weight True
longformer.encoder.layer.2.attention.self.query.bias True
longformer.encoder.layer.2.attention.self.key.weight True
longformer.encoder.layer.2.attention.self.key.bias True
longformer.encoder.layer.2.attention.self.value.weight True
longformer.encoder.layer.2.attention.self.value.bias True
longformer.encoder.layer.2.attention.self.query_global.weight True
longformer.encoder.layer.2.attention.self.query_global.bias True
longformer.encoder.layer.2.attention.self.key_global.weight True
longformer.encoder.layer.2.attention.self.key_global.bias True
longformer.encoder.layer.2.attention.self.value_global.weight True
longformer.encoder.layer.2.attention.self.value_global.bias True
longformer.encoder.layer.2.attention.output.dense.weight True
longformer.encoder.layer.2.attention.output.dense.bias True
longformer.encoder.layer.2.attention.output.LayerNorm.weight True
longformer.encoder.layer.2.attention.output.LayerNorm.bias True
longformer.encoder.layer.2.intermediate.dense.weight True
longformer.encoder.layer.2.intermediate.dense.bias True
longformer.encoder.layer.2.output.dense.weight True
longformer.encoder.layer.2.output.dense.bias True
longformer.encoder.layer.2.output.LayerNorm.weight True
longformer.encoder.layer.2.output.LayerNorm.bias True
longformer.encoder.layer.3.attention.self.query.weight True
longformer.encoder.layer.3.attention.self.query.bias True
longformer.encoder.layer.3.attention.self.key.weight True
longformer.encoder.layer.3.attention.self.key.bias True
longformer.encoder.layer.3.attention.self.value.weight True
longformer.encoder.layer.3.attention.self.value.bias True
longformer.encoder.layer.3.attention.self.query_global.weight True
longformer.encoder.layer.3.attention.self.query_global.bias True
longformer.encoder.layer.3.attention.self.key_global.weight True
longformer.encoder.layer.3.attention.self.key_global.bias True
longformer.encoder.layer.3.attention.self.value_global.weight True
longformer.encoder.layer.3.attention.self.value_global.bias True
longformer.encoder.layer.3.attention.output.dense.weight True
longformer.encoder.layer.3.attention.output.dense.bias True
longformer.encoder.layer.3.attention.output.LayerNorm.weight True
longformer.encoder.layer.3.attention.output.LayerNorm.bias True
longformer.encoder.layer.3.intermediate.dense.weight True
longformer.encoder.layer.3.intermediate.dense.bias True
longformer.encoder.layer.3.output.dense.weight True
longformer.encoder.layer.3.output.dense.bias True
longformer.encoder.layer.3.output.LayerNorm.weight True
longformer.encoder.layer.3.output.LayerNorm.bias True
longformer.encoder.layer.4.attention.self.query.weight True
longformer.encoder.layer.4.attention.self.query.bias True
longformer.encoder.layer.4.attention.self.key.weight True
longformer.encoder.layer.4.attention.self.key.bias True
longformer.encoder.layer.4.attention.self.value.weight True
longformer.encoder.layer.4.attention.self.value.bias True
longformer.encoder.layer.4.attention.self.query_global.weight True
longformer.encoder.layer.4.attention.self.query_global.bias True
longformer.encoder.layer.4.attention.self.key_global.weight True
longformer.encoder.layer.4.attention.self.key_global.bias True
longformer.encoder.layer.4.attention.self.value_global.weight True
longformer.encoder.layer.4.attention.self.value_global.bias True
longformer.encoder.layer.4.attention.output.dense.weight True
longformer.encoder.layer.4.attention.output.dense.bias True
longformer.encoder.layer.4.attention.output.LayerNorm.weight True
longformer.encoder.layer.4.attention.output.LayerNorm.bias True
longformer.encoder.layer.4.intermediate.dense.weight True
longformer.encoder.layer.4.intermediate.dense.bias True
longformer.encoder.layer.4.output.dense.weight True
longformer.encoder.layer.4.output.dense.bias True
longformer.encoder.layer.4.output.LayerNorm.weight True
longformer.encoder.layer.4.output.LayerNorm.bias True
longformer.encoder.layer.5.attention.self.query.weight True
longformer.encoder.layer.5.attention.self.query.bias True
longformer.encoder.layer.5.attention.self.key.weight True
longformer.encoder.layer.5.attention.self.key.bias True
longformer.encoder.layer.5.attention.self.value.weight True
longformer.encoder.layer.5.attention.self.value.bias True
longformer.encoder.layer.5.attention.self.query_global.weight True
longformer.encoder.layer.5.attention.self.query_global.bias True
longformer.encoder.layer.5.attention.self.key_global.weight True
longformer.encoder.layer.5.attention.self.key_global.bias True
longformer.encoder.layer.5.attention.self.value_global.weight True
longformer.encoder.layer.5.attention.self.value_global.bias True
longformer.encoder.layer.5.attention.output.dense.weight True
longformer.encoder.layer.5.attention.output.dense.bias True
longformer.encoder.layer.5.attention.output.LayerNorm.weight True
longformer.encoder.layer.5.attention.output.LayerNorm.bias True
longformer.encoder.layer.5.intermediate.dense.weight True
longformer.encoder.layer.5.intermediate.dense.bias True
longformer.encoder.layer.5.output.dense.weight True
longformer.encoder.layer.5.output.dense.bias True
longformer.encoder.layer.5.output.LayerNorm.weight True
longformer.encoder.layer.5.output.LayerNorm.bias True
longformer.encoder.layer.6.attention.self.query.weight True
longformer.encoder.layer.6.attention.self.query.bias True
longformer.encoder.layer.6.attention.self.key.weight True
longformer.encoder.layer.6.attention.self.key.bias True
longformer.encoder.layer.6.attention.self.value.weight True
longformer.encoder.layer.6.attention.self.value.bias True
longformer.encoder.layer.6.attention.self.query_global.weight True
longformer.encoder.layer.6.attention.self.query_global.bias True
longformer.encoder.layer.6.attention.self.key_global.weight True
longformer.encoder.layer.6.attention.self.key_global.bias True
longformer.encoder.layer.6.attention.self.value_global.weight True
longformer.encoder.layer.6.attention.self.value_global.bias True
longformer.encoder.layer.6.attention.output.dense.weight True
longformer.encoder.layer.6.attention.output.dense.bias True
longformer.encoder.layer.6.attention.output.LayerNorm.weight True
longformer.encoder.layer.6.attention.output.LayerNorm.bias True
longformer.encoder.layer.6.intermediate.dense.weight True
longformer.encoder.layer.6.intermediate.dense.bias True
longformer.encoder.layer.6.output.dense.weight True
longformer.encoder.layer.6.output.dense.bias True
longformer.encoder.layer.6.output.LayerNorm.weight True
longformer.encoder.layer.6.output.LayerNorm.bias True
longformer.encoder.layer.7.attention.self.query.weight True
longformer.encoder.layer.7.attention.self.query.bias True
longformer.encoder.layer.7.attention.self.key.weight True
longformer.encoder.layer.7.attention.self.key.bias True
longformer.encoder.layer.7.attention.self.value.weight True
longformer.encoder.layer.7.attention.self.value.bias True
longformer.encoder.layer.7.attention.self.query_global.weight True
longformer.encoder.layer.7.attention.self.query_global.bias True
longformer.encoder.layer.7.attention.self.key_global.weight True
longformer.encoder.layer.7.attention.self.key_global.bias True
longformer.encoder.layer.7.attention.self.value_global.weight True
longformer.encoder.layer.7.attention.self.value_global.bias True
longformer.encoder.layer.7.attention.output.dense.weight True
longformer.encoder.layer.7.attention.output.dense.bias True
longformer.encoder.layer.7.attention.output.LayerNorm.weight True
longformer.encoder.layer.7.attention.output.LayerNorm.bias True
longformer.encoder.layer.7.intermediate.dense.weight True
longformer.encoder.layer.7.intermediate.dense.bias True
longformer.encoder.layer.7.output.dense.weight True
longformer.encoder.layer.7.output.dense.bias True
longformer.encoder.layer.7.output.LayerNorm.weight True
longformer.encoder.layer.7.output.LayerNorm.bias True
longformer.encoder.layer.8.attention.self.query.weight True
longformer.encoder.layer.8.attention.self.query.bias True
longformer.encoder.layer.8.attention.self.key.weight True
longformer.encoder.layer.8.attention.self.key.bias True
longformer.encoder.layer.8.attention.self.value.weight True
longformer.encoder.layer.8.attention.self.value.bias True
longformer.encoder.layer.8.attention.self.query_global.weight True
longformer.encoder.layer.8.attention.self.query_global.bias True
longformer.encoder.layer.8.attention.self.key_global.weight True
longformer.encoder.layer.8.attention.self.key_global.bias True
longformer.encoder.layer.8.attention.self.value_global.weight True
longformer.encoder.layer.8.attention.self.value_global.bias True
longformer.encoder.layer.8.attention.output.dense.weight True
longformer.encoder.layer.8.attention.output.dense.bias True
longformer.encoder.layer.8.attention.output.LayerNorm.weight True
longformer.encoder.layer.8.attention.output.LayerNorm.bias True
longformer.encoder.layer.8.intermediate.dense.weight True
longformer.encoder.layer.8.intermediate.dense.bias True
longformer.encoder.layer.8.output.dense.weight True
longformer.encoder.layer.8.output.dense.bias True
longformer.encoder.layer.8.output.LayerNorm.weight True
longformer.encoder.layer.8.output.LayerNorm.bias True
longformer.encoder.layer.9.attention.self.query.weight True
longformer.encoder.layer.9.attention.self.query.bias True
longformer.encoder.layer.9.attention.self.key.weight True
longformer.encoder.layer.9.attention.self.key.bias True
longformer.encoder.layer.9.attention.self.value.weight True
longformer.encoder.layer.9.attention.self.value.bias True
longformer.encoder.layer.9.attention.self.query_global.weight True
longformer.encoder.layer.9.attention.self.query_global.bias True
longformer.encoder.layer.9.attention.self.key_global.weight True
longformer.encoder.layer.9.attention.self.key_global.bias True
longformer.encoder.layer.9.attention.self.value_global.weight True
longformer.encoder.layer.9.attention.self.value_global.bias True
longformer.encoder.layer.9.attention.output.dense.weight True
longformer.encoder.layer.9.attention.output.dense.bias True
longformer.encoder.layer.9.attention.output.LayerNorm.weight True
longformer.encoder.layer.9.attention.output.LayerNorm.bias True
longformer.encoder.layer.9.intermediate.dense.weight True
longformer.encoder.layer.9.intermediate.dense.bias True
longformer.encoder.layer.9.output.dense.weight True
longformer.encoder.layer.9.output.dense.bias True
longformer.encoder.layer.9.output.LayerNorm.weight True
longformer.encoder.layer.9.output.LayerNorm.bias True
longformer.encoder.layer.10.attention.self.query.weight True
longformer.encoder.layer.10.attention.self.query.bias True
longformer.encoder.layer.10.attention.self.key.weight True
longformer.encoder.layer.10.attention.self.key.bias True
longformer.encoder.layer.10.attention.self.value.weight True
longformer.encoder.layer.10.attention.self.value.bias True
longformer.encoder.layer.10.attention.self.query_global.weight True
longformer.encoder.layer.10.attention.self.query_global.bias True
longformer.encoder.layer.10.attention.self.key_global.weight True
longformer.encoder.layer.10.attention.self.key_global.bias True
longformer.encoder.layer.10.attention.self.value_global.weight True
longformer.encoder.layer.10.attention.self.value_global.bias True
longformer.encoder.layer.10.attention.output.dense.weight True
longformer.encoder.layer.10.attention.output.dense.bias True
longformer.encoder.layer.10.attention.output.LayerNorm.weight True
longformer.encoder.layer.10.attention.output.LayerNorm.bias True
longformer.encoder.layer.10.intermediate.dense.weight True
longformer.encoder.layer.10.intermediate.dense.bias True
longformer.encoder.layer.10.output.dense.weight True
longformer.encoder.layer.10.output.dense.bias True
longformer.encoder.layer.10.output.LayerNorm.weight True
longformer.encoder.layer.10.output.LayerNorm.bias True
longformer.encoder.layer.11.attention.self.query.weight True
longformer.encoder.layer.11.attention.self.query.bias True
longformer.encoder.layer.11.attention.self.key.weight True
longformer.encoder.layer.11.attention.self.key.bias True
longformer.encoder.layer.11.attention.self.value.weight True
longformer.encoder.layer.11.attention.self.value.bias True
longformer.encoder.layer.11.attention.self.query_global.weight True
longformer.encoder.layer.11.attention.self.query_global.bias True
longformer.encoder.layer.11.attention.self.key_global.weight True
longformer.encoder.layer.11.attention.self.key_global.bias True
longformer.encoder.layer.11.attention.self.value_global.weight True
longformer.encoder.layer.11.attention.self.value_global.bias True
longformer.encoder.layer.11.attention.output.dense.weight True
longformer.encoder.layer.11.attention.output.dense.bias True
longformer.encoder.layer.11.attention.output.LayerNorm.weight True
longformer.encoder.layer.11.attention.output.LayerNorm.bias True
longformer.encoder.layer.11.intermediate.dense.weight True
longformer.encoder.layer.11.intermediate.dense.bias True
longformer.encoder.layer.11.output.dense.weight True
longformer.encoder.layer.11.output.dense.bias True
longformer.encoder.layer.11.output.LayerNorm.weight True
longformer.encoder.layer.11.output.LayerNorm.bias True
classifier.dense.weight True
classifier.dense.bias True
classifier.out_proj.weight True
classifier.out_proj.bias True
</code></pre>
<p>My questions</p>
<ol>
<li>why for all layers <code>param.requires_grad</code> is <code>True</code>? Shouldnt it be <code>False</code> at least for <code>classifier.</code> layers? Aren't we training them?</li>
<li>Does <code>param.requires_grad</code>==<code>True</code> mean that particular layer is freezed? I am confused with wording <code>requires_grad</code>. Does it mean freezed?</li>
<li>if i want to train some of the previous layers as shown <a href=""https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_unbalanced.ipynb"" rel=""nofollow noreferrer"">here</a> , should I use below code?</li>
</ol>
<p><code>    for name, param in model.named_parameters():</code></p>
<p><code>         if name.startswith(&quot;...&quot;): # choose whatever you like here</code></p>
<p><code>            param.requires_grad = False</code></p>
<ol start=""4"">
<li>considering it takes a lot of time to train, is there specific recommendation regarding layers that I should train? To begin with I am planning to train -</li>
</ol>
<p>all layers starting with <code> longformer.encoder.layer.11.</code> and</p>
<pre><code>`classifier.dense.weight` 
`classifier.dense.bias` 
`classifier.out_proj.weight` 
`classifier.out_proj.bias`
</code></pre>
<ol start=""5"">
<li>Do i need add any additional layers such as <code>dropout</code> or is that already taken care by <code>LongformerForSequenceClassification.from_pretrained</code>? I am not seeing any dropout layers in the above output and that's why asking the question</li>
</ol>
<p><strong>#------------------
update 1</strong></p>
<p>How could I know which layers are frozen by using below code from the answer given by @joe32140 ? My guess is everything except last 4 layers from my output shown in my original question gets frozen. But is there any easier way to check?</p>
<pre><code>for param in model.base_model.parameters():
    param.requires_grad = False
</code></pre>
"
71607906,understanding gpu usage huggingface classification - Total optimization steps,"<p>I am training huggingface longformer for a classification problem and got below output.</p>
<ol>
<li><p>I am confused about <code>Total optimization steps</code>. As I have 7000 training data points and 5 epochs and <code>Total train batch size (w. parallel, distributed &amp; accumulation) = 64</code>, shouldn't I get
<code>7000*5/64</code> steps? that comes to <code>546.875</code>? why is it showing <code> Total optimization steps = 545</code></p>
</li>
<li><p>Why in the below output, there are 16 steps of <code>Input ids are automatically padded from 1500 to 1536 to be a multiple of config.attention_window: 512</code> then <code> [ 23/545 14:24 &lt; 5:58:16, 0.02 it/s, Epoch 0.20/5]</code>? what are these steps?</p>
</li>
</ol>
<p>==========================================================</p>
<pre><code>***** Running training *****
  Num examples = 7000
  Num Epochs = 5
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 545
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
 [ 23/545 14:24 &lt; 5:58:16, 0.02 it/s, Epoch 0.20/5]
Epoch   Training Loss   Validation Loss
</code></pre>
<hr />
<p><strong>#update</strong></p>
<p>adding <code>Trainer</code> and <code>TrainingArguments</code></p>
<pre><code>#class weights
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get(&quot;labels&quot;)
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get(&quot;logits&quot;)
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 0.5243])).to(device)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1)).to(device)
        return (loss, outputs) if return_outputs else loss

 trainer = CustomTrainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_df_tuning_dataset_tokenized,
        eval_dataset=val_dataset_tokenized
    )



# define the training arguments
training_args = TrainingArguments(
    
    
num_train_epochs = 5,# changed this from 5
per_device_train_batch_size = 4,#4,#8,
gradient_accumulation_steps = 16,
per_device_eval_batch_size= 16,#16
evaluation_strategy = &quot;epoch&quot;,

save_strategy = &quot;epoch&quot;,
learning_rate=2e-5,
load_best_model_at_end=True,
greater_is_better=False,

disable_tqdm = False, 

weight_decay=0.01,
optim=&quot;adamw_torch&quot;,#removing on 18 march from huggingface example notebook
run_name = 'longformer-classification-16March2022'
)
</code></pre>
"
71679626,what is so special about special tokens?,"<p>what exactly is the difference between &quot;token&quot; and a &quot;special token&quot;?</p>
<p>I understand the following:</p>
<ul>
<li>what is a typical token</li>
<li>what is a typical special token: MASK, UNK, SEP, etc</li>
<li>when do you add a token (when you want to expand your vocab)</li>
</ul>
<p>What I don't understand is, under what kind of capacity will you want to create a new special token, any examples what we need it for and when we want to create a special token other than those default special tokens? If an example uses a special token, why can't a normal token achieve the same objective?</p>
<pre><code>tokenizer.add_tokens(['[EOT]'], special_tokens=True)
</code></pre>
<p>And I also dont quite understand the following description in the source documentation.
what difference does it do to our model if we set add_special_tokens to False?</p>
<pre><code>add_special_tokens (bool, optional, defaults to True) â€” Whether or not to encode the sequences with the special tokens relative to their model.
</code></pre>
"
71691184,Huggingface pretrained model's tokenizer and model objects have different maximum input length,"<p>I'm using <em><strong>symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli</strong></em> pretrained model from huggingface. My task requires to use it on pretty large texts, so it's essential to know maximum input length.</p>
<p>The following code is supposed to load pretrained model and its tokenizer:</p>
<pre><code>encoding_model_name = &quot;symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli&quot;
encoding_tokenizer = AutoTokenizer.from_pretrained(encoding_model_name)
encoding_model = SentenceTransformer(encoding_model_name)
</code></pre>
<p>So, when I print info about them:</p>
<pre><code>encoding_tokenizer
encoding_model
</code></pre>
<p>I'm getting:</p>
<pre><code>PreTrainedTokenizerFast(name_or_path='symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli', vocab_size=250002, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': AddedToken(&quot;&lt;mask&gt;&quot;, rstrip=False, lstrip=True, single_word=False, normalized=False)})
</code></pre>
<pre><code>SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)
</code></pre>
<p>As you can see, <strong>model_max_len=512</strong> parameter in tokenizer doesn't match <strong>max_seq_length=128</strong> parameter in model</p>
<p>How can I figure out which one is true? Or, probably, if they somehow respond to different features, how I can check maximum input length for my model?</p>
"
71704422,Combine Camembert & CRF for token classification,"<p>I want to combine Camembert and CRF in order to perform named entity recognition on French medical data.
I am following this <a href=""https://github.com/shushanxingzhe/transformers_ner/blob/main/models.py"" rel=""nofollow noreferrer"">code</a> combining Bert and CRF, but I can't reproduce the same thing with Camembert as I didn't find a <code>PreTrainedCamembert</code> class to pass and use instead of the <code>BertPreTrainedModel</code> used in the shared code.
I have tried to use the <code>CamembertModel</code> but it gave me a model in which the camembert layers are duplicated as shown below.</p>
<pre><code>BertCRF(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(32005, 768, padding_idx=1)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (token_type_embeddings): Embedding(1, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): RobertaEncoder(
    (layer): ModuleList(
      (0): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
  (cmbert): CamembertModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(32005, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
  (crf): CRF(num_tags=21)
)
</code></pre>
<p>Any clues on how to fix this issue? I want to get a model similar to the BERT &amp; CRF one.</p>
<pre><code>BertCRF(
  (bert): BertPreTrainedModel()
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
  (crf): CRF(num_tags=21)
)
</code></pre>
"
71708136,Is it possible to access hugging face transformer embedding layer?,"<p>I want to use a pretrained hugging face transformer language model as an encoder in a sequence to sequence model.</p>
<p>The task is grammatical error correction, so both input and output come from the same language.</p>
<p>Therefore I was wondering if it is possible to access the embedding layer from the hugging face transformer encoder, and use it as the embedding layer for the decoder?</p>
<p>Or maybe there is some other approach that you'd recommend?</p>
"
71581197,What is the loss function used in Trainer from the Transformers library of Hugging Face?,"<p>What is the loss function used in Trainer from the Transformers library of Hugging Face?</p>
<p>I am trying to fine tine a BERT model using the <strong>Trainer class</strong> from the Transformers library of Hugging Face.</p>
<p>In their <a href=""https://huggingface.co/docs/transformers/main_classes/trainer"" rel=""nofollow noreferrer"">documentation</a>, they mention that one can specify a customized loss function by overriding the <code>compute_loss</code> method in the class. However, if I do not do the method override and use the Trainer to fine tine a BERT model directly for sentiment classification, what is the default loss function being use? Is it the categorical crossentropy? Thanks!</p>
"
71492980,Huggingface sagemaker,"<p>I am trying to use the <strong>text2text</strong> (translation) model <a href=""https://huggingface.co/facebook/m2m100_418M"" rel=""nofollow noreferrer"">facebook/m2m100_418M</a> to run on sagemaker.</p>
<p>So if you click on deploy and then sagemaker there is some boilerplate code that works well but I can't seem to find how to pass it the arguments <code>src_lang=&quot;en&quot;, tgt_lang=&quot;fr&quot;</code> just like when using the pipeline or transformers.
So right now it translates into random languages.</p>
<p>I'm guessing I should add it in here somehow but it's not documented.</p>
<pre><code>predictor.predict({
    'inputs': &quot;The answer to the universe is&quot;
})
</code></pre>
<p>Does anybody have an idea of how to pass arguments to the predict method?</p>
<p><strong>Edit</strong></p>
<p>This is the code that was wrong where you will need to change the HF_TASK:</p>
<pre><code>import sagemaker

role = sagemaker.get_execution_role()
# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID':'facebook/m2m100_418M',
    'HF_TASK':'text2text-generation'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    transformers_version='4.6.1',
    pytorch_version='1.7.1',
    py_version='py36',
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)```
</code></pre>
"
71744288,wandb getting logged without initiating,"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/summarization.ipynb#scrollTo=UmvbnJ9JIrJd"" rel=""nofollow noreferrer"">this notebook</a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.</p>
<p>However, when I do trainer.train() I get the following error : <a href=""https://i.stack.imgur.com/VPIZm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VPIZm.png"" alt=""enter image description here"" /></a></p>
<p>I don't understand where wandb.log is being called.
I even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.
Please help.</p>
"
71755535,huggingface classification struggling with prediction,"<p>I am fine tuning longformer and then making prediction using <code>TextClassificationPipeline</code> and <code>model(**inputs)</code> methods. I am not sure why I get different results</p>
<pre><code>import pandas as pd
import datasets
from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig
import torch.nn as nn
import torch
from torch.utils.data import DataLoader#Dataset, 
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
#import wandb
import os
from datasets import Dataset
from transformers import TextClassificationPipeline, AutoTokenizer, AutoModelForSequenceClassification

tokenizer = LongformerTokenizerFast.from_pretrained('folder_path/', max_length = maximum_len)
</code></pre>
<p>Loading the fine tuned model from a saved location. Using original tokenizer</p>
<pre><code>saved_location='c:/xyz'
model_saved=AutoModelForSequenceClassification.from_pretrained(saved_location)
pipe = TextClassificationPipeline(model=model_saved, tokenizer=tokenizer, device=0)#tokenizer_saved, padding=True, truncation=True)
prediction = pipe([&quot;The text to predict&quot;], return_all_scores=True)
prediction
[[{'label': 'LABEL_0', 'score': 0.7107483148574829},
  {'label': 'LABEL_1', 'score': 0.2892516553401947}]]
</code></pre>
<p>2nd method</p>
<pre><code>inputs = tokenizer(&quot;The text to predict&quot;, return_tensors=&quot;pt&quot;).to(device)
outputs = model_saved(**inputs)#, labels=labels)
print (outputs['logits'])
#tensor([[ 0.4552, -0.4438]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)
torch.sigmoid(outputs['logits'])
#tensor([[0.6119, 0.3908]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)
</code></pre>
<p><code>AutoModelForSequenceClassification</code> returns probabilities <code>0.71 and 0.29</code>. When I look at the 2nd method. It returns logits  <code>0.4552, -0.4438</code> which convert to probabilities <code>0.6119, 0.3908</code></p>
<p><em><strong>#update 1</strong></em></p>
<p>The first link <a href=""https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.TextClassificationPipeline"" rel=""nofollow noreferrer"">TextClassificationPipeline</a> from cronoik's answer says below</p>
<pre><code>function_to_apply (str, optional, defaults to &quot;default&quot;) â€” The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:
&quot;default&quot;: if the model has a single label, will apply the sigmoid function on the output. If the model has several labels, will apply the softmax function on the output.
&quot;sigmoid&quot;: Applies the sigmoid function on the output.
&quot;softmax&quot;: Applies the softmax function on the output.
&quot;none&quot;: Does not apply any function on the output.
</code></pre>
<p>as this is a binary classification problem (single label) shouldn't it apply sigmoid?</p>
"
71768061,huggingface transformers classification using num_labels 1 vs 2,"<p>question 1)</p>
<p>The answer to <a href=""https://stackoverflow.com/questions/71755535/huggingface-classification-struggling-with-prediction"">this question</a> suggested that for a binary classification problem I could use <code>num_labels</code> as 1 (positive or not) or 2 (positive and negative). Is there any guideline regarding which setting is better? It seems that if we use 1 then probability would be calculated using <code>sigmoid</code> function and if we use 2 then probabilities would be calculated using <code>softmax</code> function.</p>
<p>question 2)</p>
<p>In both cases are my y labels going to be same? each data point will have 0 or 1 and not one hot encoding? For example, if I have 2 data points then y would be <code>0,1</code> and not <code>[0,0],[0,1]</code></p>
<p>I have very unbalanced classification problem where class 1 is present only 2% of times. In my training data I am oversampling</p>
<p>question 3)</p>
<p>My data is in <code>pandas dataframe</code> and I am converting it to a <code>dataset</code> and creating y variable using below. How should I cast my y column - <code>label</code> if I am planning to use <code>num_labels</code>=1?</p>
<pre><code>`train_dataset=Dataset.from_pandas(train_df).cast_column(&quot;label&quot;, ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None))`
</code></pre>
"
71847442,Train an already trained model in Sagemaker and Huggingface without re-initialising,"<p>Let's say I have successfully trained a model on some training data for 10 epochs. How can I then access the very same model and train for a further 10 epochs?</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html"" rel=""nofollow noreferrer"">In the docs</a> it suggests &quot;you need to specify a checkpoint output path through hyperparameters&quot; --&gt; how?</p>
<pre class=""lang-py prettyprint-override""><code># define my estimator the standard way
huggingface_estimator = HuggingFace(
    entry_point='train.py',
    source_dir='./scripts',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    transformers_version='4.10',
    pytorch_version='1.9',
    py_version='py38',
    hyperparameters = hyperparameters,
    metric_definitions=metric_definitions
)

# train the model
huggingface_estimator.fit(
    {'train': training_input_path, 'test': test_input_path}
)
</code></pre>
<p>If I run <code>huggingface_estimator.fit</code> again it will just start the whole thing over again and overwrite my previous training.</p>
"
71892648,Electra sequence classification with pytorch lightning issues with 'pooler_output',"<p>I'm working on a sentence classification task with multiple binary labels attached to each sentence. I'm using Electra and pytorch lightning to do the job, but I've run into a problem. When I'm running the <code>trainer.fit(model, data)</code> I get the following error:</p>
<p><code>AttributeError: 'tuple' object has no attribute 'pooler_output'</code></p>
<p>The error is referring to line 13 in the section where I'm defining <code>pl.LightningModule</code>:</p>
<pre><code>class CrowdCodedTagger(pl.LightningModule):

  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):
    super().__init__()
    self.electra = ElectraModel.from_pretrained(ELECTRA_MODEL_NAME, return_dict=False) #changed ElectraModel to ElectraForSequenceClassification
    self.classifier = nn.Linear(self.electra.config.hidden_size, n_classes)
    self.n_training_steps = n_training_steps
    self.n_warmup_steps = n_warmup_steps
    self.criterion = nn.BCELoss()

  def forward(self, input_ids, attention_mask, labels=None):
    output = self.electra(input_ids, attention_mask=attention_mask)
    output = self.classifier(output.pooler_output) # &lt;---- this is the line the error is referring to.
    output = torch.sigmoid(output)
    loss = 0
    if labels is not None:
        loss = self.criterion(output, labels)
    return loss, output

  def training_step(self, batch, batch_idx):
    input_ids = batch[&quot;input_ids&quot;]
    attention_mask = batch[&quot;attention_mask&quot;]
    labels = batch[&quot;labels&quot;]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log(&quot;train_loss&quot;, loss, prog_bar=True, logger=True)
    return {&quot;loss&quot;: loss, &quot;predictions&quot;: outputs, &quot;labels&quot;: labels}

  def validation_step(self, batch, batch_idx):
    input_ids = batch[&quot;input_ids&quot;]
    attention_mask = batch[&quot;attention_mask&quot;]
    labels = batch[&quot;labels&quot;]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log(&quot;val_loss&quot;, loss, prog_bar=True, logger=True)
    return loss

  def test_step(self, batch, batch_idx):
    input_ids = batch[&quot;input_ids&quot;]
    attention_mask = batch[&quot;attention_mask&quot;]
    labels = batch[&quot;labels&quot;]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log(&quot;test_loss&quot;, loss, prog_bar=True, logger=True)
    return loss

  def training_epoch_end(self, outputs):
    
    labels = []
    predictions = []
    for output in outputs:
      for out_labels in output[&quot;labels&quot;].detach().cpu():
        labels.append(out_labels)
      for out_predictions in output[&quot;predictions&quot;].detach().cpu():
        predictions.append(out_predictions)

    labels = torch.stack(labels).int()
    predictions = torch.stack(predictions)

    for i, name in enumerate(LABEL_COLUMNS):
      class_roc_auc = auroc(predictions[:, i], labels[:, i])
      self.logger.experiment.add_scalar(f&quot;{name}_roc_auc/Train&quot;, class_roc_auc, self.current_epoch)

  def configure_optimizers(self):

    optimizer = AdamW(self.parameters(), lr=2e-5)

    scheduler = get_linear_schedule_with_warmup(
      optimizer,
      num_warmup_steps=self.n_warmup_steps,
      num_training_steps=self.n_training_steps
    )

    return dict(
      optimizer=optimizer,
      lr_scheduler=dict(
        scheduler=scheduler,
        interval='step'
      )
    )
</code></pre>
<p>Can anyone point me in a direction to fix the error?</p>
<p>EXAMPLE OF DATA STRUCTURE (in CSV):</p>
<pre><code>sentence                      label_1          label_2          label_3
Lorem ipsum dolor sit amet    1                0                1
consectetur adipiscing elit   0                0                0
sed do eiusmod tempor         0                1                1
incididunt ut labore et       1                0                0
Lorem ipsum dolor sit amet    1                0                1
</code></pre>
"
71915952,Why does huggingface hang on list input for pipeline sentiment-analysis?,"<p>With python 3.10 and latest version of huggingface.</p>
<p>for simple code likes this</p>
<pre><code>from transformers import pipeline

input_list = ['How do I test my connection? (Windows)', 'how do I change my payment method?', 'How do I contact customer support?']

classifier = pipeline('sentiment-analysis')
results = classifier(input_list)
</code></pre>
<p>the program hangs and returns error messages:</p>
<pre><code>File &quot;.......env/lib/python3.10/multiprocessing/spawn.py&quot;, line 134, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

</code></pre>
<p>but replace the list input with a string, it works</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis')
result = classifier('How do I test my connection? (Windows)')
</code></pre>
"
71922261,TypeError: setup() got an unexpected keyword argument 'stage',"<p>I am trying to train my q&amp;a model through pytorch_lightning. However while running the command <code>trainer.fit(model,data_module)</code> I am getting the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-72-b9cdaa88efa7&gt; in &lt;module&gt;()
----&gt; 1 trainer.fit(model,data_module)

4 frames
/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _call_setup_hook(self)
   1488 
   1489         if self.datamodule is not None:
-&gt; 1490             self.datamodule.setup(stage=fn)
   1491         self._call_callback_hooks(&quot;setup&quot;, stage=fn)
   1492         self._call_lightning_module_hook(&quot;setup&quot;, stage=fn)

TypeError: setup() got an unexpected keyword argument 'stage'
</code></pre>
<p>I have installed and imported pytorch_lightning.</p>
<p>Also I have defined <code>data_module = BioQADataModule(train_df, val_df, tokenizer, batch_size = BATCH_SIZE)</code> where BATCH_SIZE = 2, N_EPOCHS = 6.</p>
<p>The model I have used is as follows:-</p>
<pre><code>model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)
</code></pre>
<p>Also, I have defined the class for the model as follows:-</p>
<pre><code>    class BioQAModel(pl.LightningModule):
    
      def __init__(self):
        super().__init__()
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)
    
      def forward(self, input_ids, attention_mask, labels=None):
        output = self.model(
            input_ids = encoding[&quot;input_ids&quot;],
            attention_mask = encoding[&quot;attention_mask&quot;],
            labels=labels
        )
    
        return output.loss, output.logits
    
      def training_step(self, batch, batch_idx):
        input_ids = batch[&quot;input_ids&quot;]
        attention_mask = batch[&quot;attention_mask&quot;]
        labels = batch[&quot;labels&quot;]
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log(&quot;train_loss&quot;, loss, prog_bar=True, logger=True)
        return loss
    
      def validation_step(self, batch, batch_idx):
        input_ids = batch[&quot;input_ids&quot;]
        attention_mask = batch[&quot;attention_mask&quot;]
        labels = batch[&quot;labels&quot;]
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log(&quot;val_loss&quot;, loss, prog_bar=True, logger=True)
        return loss 
    
      def test_step(self, batch, batch_idx):
        input_ids = batch[&quot;input_ids&quot;]
        attention_mask = batch[&quot;attention_mask&quot;]
        labels = batch[&quot;labels&quot;]
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log(&quot;test_loss&quot;, loss, prog_bar=True, logger=True)
        return loss
    
      def configure_optimizers(self):
        return AdamW(self.parameters(), lr=0.0001)
</code></pre>
<p>For any additional information required, please specify.</p>
<p>Edit 1: Adding BioQADataModule:</p>
<pre><code>class BioQADataModule(pl.LightningDataModule):

  def __init__(
      self,
      train_df: pd.DataFrame,
      test_df: pd.DataFrame,
      tokenizer: T5Tokenizer,
      batch_size: int = 8,
      source_max_token_len = 396,
      target_max_token_len = 32
    ):
      super().__init__()
      self.batch_size = batch_size
      self.train_df = train_df
      self.test_df = test_df
      self.tokenizer = tokenizer
      self.source_max_token_len = source_max_token_len
      self.target_max_token_len = target_max_token_len

  def setup(self):
    self.train_dataset = BioQADataset(
        self.train_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

    self.test_dataset = BioQADataset(
        self.test_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

  def train_dataloader(self):
    return DataLoader(
        self.train_dataset,
        batch_size = self.batch_size,
        shuffle = True,
        num_workers = 4
    )

  def val_dataloader(self):
    return DataLoader(
        self.train_dataset,
        batch_size = 1,
        shuffle = True,
        num_workers = 4  
    )

  def test_dataloader(self):
    return DataLoader(
        self.train_dataset,
        batch_size = 1,
        shuffle = True,
        num_workers = 4  
    )
</code></pre>
"
71926953,Wandb website for Huggingface Trainer shows plots and logs only for the first model,"<p>I am finetuning multiple models using for loop as follows.</p>
<pre><code>for file in os.listdir(args.data_dir):
    finetune(args, file)
</code></pre>
<p>BUT <code>wandb</code> website shows plots and logs only for the first file i.e., <code>file1</code> in <code>data_dir</code> although it is training and saving models for other files. It feels very strange behavior.</p>
<pre><code>wandb: Synced bertweet-base-finetuned-file1: https://wandb.ai/***/huggingface/runs/***
</code></pre>
<p>This is a small snippet of <strong>finetuning</strong> code with Huggingface:</p>
<pre><code>def finetune(args, file):
    training_args = TrainingArguments(
        output_dir=f'{model_name}-finetuned-{file}',
        overwrite_output_dir=True,
        evaluation_strategy='no',
        num_train_epochs=args.epochs,
        learning_rate=args.lr,
        weight_decay=args.decay,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        fp16=True, # mixed-precision training to boost speed
        save_strategy='no',
        seed=args.seed,
        dataloader_num_workers=4,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset['train'],
        eval_dataset=None,
        data_collator=data_collator,
    )
    trainer.train()
    trainer.save_model()
</code></pre>
"
71944781,Error in trying to push model to huggingface,"<p>So, I am making a chatbot for discord using google Collaboratory. However whenever, i try to push it on huggingface.co. It does deploy and instead gives me an error. Also, while trying to clone the repo, it's giving me an error <strong>fatal: destination path 'DialoGPT-small-technoblade' already exists and is not an empty directory.</strong>
<a href=""https://i.stack.imgur.com/4Uaol.png"" rel=""nofollow noreferrer"">The error that is being shown</a></p>
<p>The code I'm using</p>
<pre><code>!pip install huggingface_hub
!sudo apt-get install git-lfs
!git config --global user.email &quot;email&quot;
!git config --global user.name &quot;username&quot;

!transformers-cli login
!git clone https://user:...token...@huggingface.co/user/repo
!ls -al

!git lfs install
!git add .
!git commit -m &quot;Initial commit&quot;

!git log
!git push

</code></pre>
"
72014025,"Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis',","<p>I am trying to use Transformers for the first time based on this model:</p>
<p><a href=""https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+like+you.+I+love+you"" rel=""nofollow noreferrer"">https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+like+you.+I+love+you</a></p>
<p>The sample code provided here its:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;text-classification&quot;,model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)
prediction = classifier(&quot;I love using transformers. The best part is wide range of support and its easy to use&quot;, )
print(prediction)
</code></pre>
<p>However I get this error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-6-74ca6189abbe&gt; in &lt;module&gt;
      1 from transformers import pipeline
----&gt; 2 classifier = pipeline(&quot;text-classification&quot;,model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)
      3 prediction = classifier(&quot;I love using transformers. The best part is wide range of support and its easy to use&quot;, )
      4 print(prediction)

/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)
    340     &quot;&quot;&quot;
    341     # Retrieve the task
--&gt; 342     targeted_task, task_options = check_task(task)
    343 
    344     # Use default model/config/tokenizer for the task if no model is provided

/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py in check_task(task)
    234         raise KeyError(f&quot;Invalid translation task {task}, use 'translation_XX_to_YY' format&quot;)
    235 
--&gt; 236     raise KeyError(
    237         f&quot;Unknown task {task}, available tasks are {list(SUPPORTED_TASKS.keys()) + ['translation_XX_to_YY']}&quot;
    238     )

KeyError: &quot;Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'table-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'conversational', 'translation_XX_to_YY']&quot;
</code></pre>
<p>And Yes I Installed transfomers first</p>
"
72036646,Converting h5 to tflite,"<p>I have been trying to get this zero-shot text classification <code>joeddav / xlm-roberta-large-xnli</code> to convert from h5 to tflite file (<a href=""https://huggingface.co/joeddav/xlm-roberta-large-xnli"" rel=""nofollow noreferrer"">https://huggingface.co/joeddav/xlm-roberta-large-xnli</a>), but this error pops up and I cant find it described online, how is it fixed? If it can't, is there another zero-shot text classifier I can use that would produce similar accuracy even after becoming tflite?</p>
<pre><code>AttributeError: 'T5ForConditionalGeneration' object has no attribute 'call'
</code></pre>
<p>I have been trying a few different tutorials and the current google colab file I have is an amalgam of a couple of them. <a href=""https://colab.research.google.com/drive/1sYQJqvhM_KEvMt2IP15d8Ud9L-ApiYv6?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1sYQJqvhM_KEvMt2IP15d8Ud9L-ApiYv6?usp=sharing</a></p>
"
72108945,saving finetuned model locally,"<p>I'm trying to understand how to save a fine-tuned model locally, instead of pushing it to the hub.</p>
<p>I've done some tutorials and at the last step of fine-tuning a model is running <code>trainer.train()</code> .  And then the instruction is usually: <code>trainer.push_to_hub</code></p>
<p>But what if I don't want to push to the hub?  I want to save the model locally, and then later be able to load it from my own computer into future task so I can do inference without re-tuning.</p>
<p>How can I do that?</p>
<p>eg: Initially load a model from hugging face:</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=5)
</code></pre>
<pre><code>trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

</code></pre>
<p>Somehow save the new trained model locally, so that next time I can pass</p>
<pre><code>model = 'some local directory where model and configs (?) got saved'
</code></pre>
"
72147225,Pytorch model object has no attribute 'predict' BERT,"<p>I had train a BertClassifier model using pytorch. After creating my best.pt I would like to make in production my model and using it to predict and classifier starting from a sample, so I resume them from the checkpoint. Otherwise after put it in evaluation and freeze model, I use .predict to make in work on my sample but I'm encountering this Attribute Error. I had also inizialize it before calling the checkpoint. When I am wrong? Thank you for your help!</p>
<pre><code>def save_ckp(state, is_best, checkpoint_path, best_model_path):
    &quot;&quot;&quot;
    function created to save checkpoint, the latest one and the best one. 
    This creates flexibility: either you are interested in the state of the latest checkpoint or the best checkpoint.
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    &quot;&quot;&quot;
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    &quot;&quot;&quot;
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into       
    optimizer: optimizer we defined in previous training
    &quot;&quot;&quot;
    # load check point
    checkpoint = torch.load(checkpoint_fpath)
    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])
    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])
    # initialize valid_loss_min from checkpoint to valid_loss_min
    valid_loss_min = checkpoint['valid_loss_min']
    # return model, optimizer, epoch value, min validation loss 
    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()

#Create the BertClassfier class
class BertClassifier(nn.Module):
    &quot;&quot;&quot;Bert Model for Classification Tasks.&quot;&quot;&quot;
    def __init__(self, freeze_bert=True):
        &quot;&quot;&quot;
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        &quot;&quot;&quot;
        super(BertClassifier, self).__init__()
        
        .......
        
    def forward(self, input_ids, attention_mask):
        ''' Feed input to BERT and the classifier to compute logits.
         @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,
                       max_length)
         @param    attention_mask (torch.Tensor): a tensor that hold attention mask
                       information with shape (batch_size, max_length)
         @return   logits (torch.Tensor): an output tensor with shape (batch_size,
                       num_labels) '''
         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

def initialize_model(epochs):
    &quot;&quot;&quot; Initialize the Bert Classifier, the optimizer and the learning rate scheduler.&quot;&quot;&quot;
    # Instantiate Bert Classifier
    bert_classifier = BertClassifier(freeze_bert=False)

    # Tell PyTorch to run the model on GPU
    bert_classifier = bert_classifier.to(device)

    # Create the optimizer
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=lr,    # Default learning rate
                      eps=1e-8    # Default epsilon value
                      )

    # Total number of training steps
    total_steps = len(train_dataloader) * epochs

    # Set up the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0, # Default value
                                                num_training_steps=total_steps)
    return bert_classifier, optimizer, scheduler
    

def train(model, train_dataloader, val_dataloader, valid_loss_min_input, checkpoint_path, best_model_path, start_epochs, epochs, evaluation=True):

    &quot;&quot;&quot;Train the BertClassifier model.&quot;&quot;&quot;
    # Start training loop
    logging.info(&quot;--Start training...\n&quot;)

    # Initialize tracker for minimum validation loss
    valid_loss_min = valid_loss_min_input 


    for epoch_i in range(start_epochs, epochs):
        # =======================================
        #               Training
        # =======================================
        # Print the header of the result table
        logging.info((f&quot;{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}&quot;))

        # Measure the elapsed time of each epoch
        t0_epoch, t0_batch = time.time(), time.time()

        # Reset tracking variables at the beginning of each epoch
        total_loss, batch_loss, batch_counts = 0, 0, 0

        # Put the model into the training mode
        model.train()

        # For each batch of training data...
        for step, batch in enumerate(train_dataloader):
            batch_counts +=1
            # Load batch to GPU
            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

            # Zero out any previously calculated gradients
            model.zero_grad()

            # Perform a forward pass. This will return logits.
            logits = model(b_input_ids, b_attn_mask)

            # Compute loss and accumulate the loss values
            loss = loss_fn(logits, b_labels)
            batch_loss += loss.item()
            total_loss += loss.item()

            # Perform a backward pass to calculate gradients
            loss.backward()

            # Clip the norm of the gradients to 1.0 to prevent &quot;exploding gradients&quot;
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and the learning rate
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 500 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 20 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                logging.info(f&quot;{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}&quot;)

                # Reset batch tracking variables
                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()

        # Calculate the average loss over the entire training data
        avg_train_loss = total_loss / len(train_dataloader)

        logging.info(&quot;-&quot;*70)
        # =======================================
        #               Evaluation
        # =======================================
        if evaluation == True:
            # After the completion of each training epoch, measure the model's performance
            # on our validation set.
            val_loss, val_accuracy = evaluate(model, val_dataloader)

            # Print performance over the entire training data
            time_elapsed = time.time() - t0_epoch
            
            logging.info(f&quot;{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^10.6f} | {time_elapsed:^9.2f}&quot;)

            logging.info(&quot;-&quot;*70)
        logging.info(&quot;\n&quot;)


         # create checkpoint variable and add important data
        checkpoint = {
            'epoch': epoch_i + 1,
            'valid_loss_min': val_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
        
        # save checkpoint
        save_ckp(checkpoint, False, checkpoint_path, best_model_path)
        
        ## TODO: save the model if validation loss has decreased
        if val_loss &lt;= valid_loss_min:
            print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))
            # save checkpoint as best model
            save_ckp(checkpoint, True, checkpoint_path, best_model_path)
            valid_loss_min = val_loss
    
    logging.info(&quot;-----------------Training complete--------------------------&quot;)

def evaluate(model, val_dataloader):
    &quot;&quot;&quot;After the completion of each training epoch, measure the model's performance on our validation set.&quot;&quot;&quot;
    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []

    # For each batch in our validation set...
    for batch in val_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        # Compute loss
        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds = torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)

    # Compute the average accuracy and loss over the validation set.
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)

    return val_loss, val_accuracy

bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
train(model = bert_classifier ......)



bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
model, optimizer, start_epoch, valid_loss_min = load_ckp(r&quot;./best_model/best_model.pt&quot;, bert_classifier, optimizer)

model.eval()
model.freeze()

sample = {
  &quot;seq&quot;: &quot;ABCDE&quot;,}

predictions = model.predict(sample)
</code></pre>
<pre><code>AttributeError: 'BertClassifier' object has no attribute 'predict'
</code></pre>
"
72214408,why does huggingface t5 tokenizer ignore some of the whitespaces?,"<p>I am using T5 model and tokenizer for a downstream task. I want to add certain whitesapces to the tokenizer like line ending <code>(\t)</code> and tab <code>(\t)</code>. Adding these tokens work but somehow the tokenizer always ignores the second whitespace. So, it tokenizes the sequence <code>â€œ\n\nâ€</code> as a single line ending and the sequence <code>&quot;\n\n\n\n&quot;</code> is tokenized as two line endings and so on. See below to reproduce.</p>
<pre><code>from transformers import T5Tokenizer
tokenizer = T5Tokenizer.from_pretrained(&quot;t5-large&quot;)
tokenizer.add_tokens([&quot;\n&quot;])

tokenizer.encode(&quot;\n&quot;) # returns [32100, 1] as expected
tokenizer.encode(&quot;\n\n&quot;) # returns [32100, 1] but expected would be [32100, 32100, 1]
tokenizer.encode(&quot;\n\n\n\n&quot;) # returns [32100, 32100, 1] but expected would be [32100, 32100, 32100, 32100, 1]
</code></pre>
<p>what is the reasoning behind this behaviour? Is it a bug or something related to how tokenizer works? I noticed that this only happens for added whitespaces but not for other characters.</p>
<p>Is there way to prevent tokenizer from ignoring the repeated whitespaces?</p>
"
72261504,Hugginface Transformers Bert Tokenizer - Find out which documents get truncated,"<p>I am using the Transforms library from Huggingface to create a text classification model based on Bert. For this I tokenise my documents and I set truncation to be true as my documents are longer than allowed (512).</p>
<p>How can I find out how many documents are actually getting truncated? I don't think the length (512) is character or word count of the document, as the Tokenizer prepares the document as input for the model. What happens to the document and is there a straight forward way to check whether or not it gets truncated?</p>
<p>This is the code I use to tokenise the documents.</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;) 
model = BertForSequenceClassification.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;, num_labels=7)
train_encoded =  tokenizer(X_train, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<p>In case you have any more questions about my code or problem, feel free to ask.</p>
"
72280030,how to resolve Transformer model DistilBert error got an unexpected keyword argument 'special_tokens_mask',"<p>I am using</p>
<p>Apple Mac M1</p>
<p>OS: MacOS Monterey</p>
<p>Python 3.10.4</p>
<p>I am trying to implement a vector search with DistilBERT and Weaviate by following this <a href=""https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154"" rel=""nofollow noreferrer"">tutorial</a></p>
<p>below is the code setup</p>
<pre><code>import nltk
import os
import random
import time
import torch
import weaviate
from transformers import AutoModel, AutoTokenizer
from nltk.tokenize import sent_tokenize

torch.set_grad_enabled(False)

# udpated to use different model if desired
MODEL_NAME = &quot;distilbert-base-uncased&quot;
model = AutoModel.from_pretrained(MODEL_NAME)
model.to('cuda') # remove if working without GPUs
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# initialize nltk (for tokenizing sentences)
nltk.download('punkt')

# initialize weaviate client for importing and searching
client = weaviate.Client(&quot;http://localhost:8080&quot;)

def get_post_filenames(limit_objects=100):
    file_names = []
    i=0
    for root, dirs, files in os.walk(&quot;./data/20news-bydate-test&quot;):
        for filename in files:
            path = os.path.join(root, filename)
            file_names += [path]
        
    random.shuffle(file_names)
    limit_objects = min(len(file_names), limit_objects)
      
    file_names = file_names[:limit_objects]

    return file_names

def read_posts(filenames=[]):
    posts = []
    for filename in filenames:
        f = open(filename, encoding=&quot;utf-8&quot;, errors='ignore')
        post = f.read()
        
        # strip the headers (the first occurrence of two newlines)
        post = post[post.find('\n\n'):]
        
        # remove posts with less than 10 words to remove some of the noise
        if len(post.split(' ')) &lt; 10:
               continue
        
        post = post.replace('\n', ' ').replace('\t', ' ').strip()
        if len(post) &gt; 1000:
            post = post[:1000]
        posts += [post]

    return posts       


def text2vec(text):
    tokens_pt = tokenizer(text, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=&quot;pt&quot;)
    tokens_pt.to('cuda') # remove if working without GPUs
    outputs = model(**tokens_pt)
    return outputs[0].mean(0).mean(0).detach()

def vectorize_posts(posts=[]):
    post_vectors=[]
    before=time.time()
    for i, post in enumerate(posts):
        vec=text2vec(sent_tokenize(post))
        post_vectors += [vec]
        if i % 100 == 0 and i != 0:
            print(&quot;So far {} objects vectorized in {}s&quot;.format(i, time.time()-before))
    after=time.time()
    
    print(&quot;Vectorized {} items in {}s&quot;.format(len(posts), after-before))
    
    return post_vectors

def init_weaviate_schema():
    # a simple schema containing just a single class for our posts
    schema = {
        &quot;classes&quot;: [{
                &quot;class&quot;: &quot;Post&quot;,
                &quot;vectorizer&quot;: &quot;none&quot;, # explicitly tell Weaviate not to vectorize anything, we are providing the vectors ourselves through our BERT model
                &quot;properties&quot;: [{
                    &quot;name&quot;: &quot;content&quot;,
                    &quot;dataType&quot;: [&quot;text&quot;],
                }]
        }]
    }

    # cleanup from previous runs
    client.schema.delete_all()

    client.schema.create(schema)

def import_posts_with_vectors(posts, vectors, batchsize=256):
    batch = weaviate.ObjectsBatchRequest()

    for i, post in enumerate(posts):
        props = {
            &quot;content&quot;: post,
        }
        batch.add(props, &quot;Post&quot;, vector=vectors[i])
        
        # when either batch size is reached or we are at the last object
        if (i !=0 and i % batchsize == 0) or i == len(posts) - 1:
            # send off the batch
            client.batch.create(batch)
            
            # and reset for the next batch
            batch = weaviate.ObjectsBatchRequest() 
    

def search(query=&quot;&quot;, limit=3):
    before = time.time()
    vec = text2vec(query)
    vec_took = time.time() - before

    before = time.time()
    near_vec = {&quot;vector&quot;: vec.tolist()}
    res = client \
        .query.get(&quot;Post&quot;, [&quot;content&quot;, &quot;_additional {certainty}&quot;]) \
        .with_near_vector(near_vec) \
        .with_limit(limit) \
        .do()
    search_took = time.time() - before

    print(&quot;\nQuery \&quot;{}\&quot; with {} results took {:.3f}s ({:.3f}s to vectorize and {:.3f}s to search)&quot; \
          .format(query, limit, vec_took+search_took, vec_took, search_took))
    for post in res[&quot;data&quot;][&quot;Get&quot;][&quot;Post&quot;]:
        print(&quot;{:.4f}: {}&quot;.format(post[&quot;_additional&quot;][&quot;certainty&quot;], post[&quot;content&quot;]))
        print('---')

# run everything
init_weaviate_schema()
posts = read_posts(get_post_filenames(4000))
vectors = vectorize_posts(posts)
import_posts_with_vectors(posts, vectors)

search(&quot;the best camera lens&quot;, 1)
search(&quot;which software do i need to view jpeg files&quot;, 1)
search(&quot;windows vs mac&quot;, 1)

</code></pre>
<p>the fuction below trigger errors</p>
<pre><code>

def text2vec(text):
    # tokens_pt = tokenizer(text, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=&quot;pt&quot;)
    tokens_pt = tokenizer.encode_plus(text, add_special_tokens = True,    truncation = True, padding = &quot;max_length&quot;, return_attention_mask = True, return_tensors = &quot;pt&quot;)

    tokens_pt.to('cuda') # remove if working without GPUs
    outputs = model(**tokens_pt)
    return outputs[0].mean(0).mean(0).detach()
</code></pre>
<blockquote>
<blockquote>
<p>error 1</p>
<blockquote>
<p>tokens_pt.to('cuda') # remove if working without GPUs
AttributeError: 'dict' object has no attribute 'to'</p>
</blockquote>
</blockquote>
</blockquote>
<p>when I comment out the GPU</p>
<pre><code>#tokens_pt.to('cuda')
</code></pre>
<p>and run the code. I get this error</p>
<blockquote>
<blockquote>
<p>error 2</p>
<blockquote>
<p>outputs = model(**tokens_pt)
File &quot;/opt/homebrew/Caskroom/miniforge/base/envs/py310a/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1110, in _call_impl
return forward_call(*input, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'special_tokens_mask'</p>
</blockquote>
</blockquote>
</blockquote>
<p>what is causing this errors and how can I fix it ?</p>
"
72355671,huggingface longformer case sensitive tokenizer,"<p>This <a href=""https://jesusleal.io/2020/11/24/Longformer-with-IMDB/"" rel=""nofollow noreferrer"">page</a> shows how to build a longformer based classification.</p>
<pre><code>import pandas as pd
import datasets
from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
import wandb
import os


# load model and tokenizer and define length of the text sequence
model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',
                                                           gradient_checkpointing=False,
                                                           attention_window = 512)
tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)
</code></pre>
<p>I noticed that the tokenizer is sensitive to case of the data. words <code>do</code> and <code>Do</code> get different tokens below. I dont need such behavior. I can always lowercase my data before feeding to longformer. But is there is any other better way to tell tokenizer to ignore case of the data?</p>
<pre><code>encoded_input = tokenizer(&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;)
print(encoded_input)
{'input_ids': [0, 8275, 45, 31510, 459, 11, 5, 5185, 9, 44859, 6, 13, 51, 32, 12405, 8, 2119, 7, 6378, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

encoded_input3 = tokenizer(&quot;do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;)
print(encoded_input3)
{'input_ids': [0, 5016, 45, 31510, 459, 11, 5, 5185, 9, 44859, 6, 13, 51, 32, 12405, 8, 2119, 7, 6378, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
"
72411360,Why does huggingface tokenizer return only 1 `input_ids` instead of 3?,"<p>I'm trying to tokenize the <code>squad</code> dataset following the <a href=""https://huggingface.co/course/chapter1/1"" rel=""nofollow noreferrer"">huggingface tutorial</a>:</p>
<pre><code>from datasets import load_dataset
from transformers import RobertaTokenizer
from transformers import logging
logging.set_verbosity_error()

dataset = load_dataset('squad')
checkpoint = 'roberta-base'
tokenizer = RobertaTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example['question'], example['context'], [d['text'][0] for d in example['answers']], truncation=True) 

tokenized_datasets = dataset['train'].map(tokenize_function, batched=True)
</code></pre>
<p>But when I print</p>
<pre><code>tokenized_datasets
</code></pre>
<p>I get</p>
<pre><code>Dataset({
    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask'],
    num_rows: 87599
})
</code></pre>
<p>But shouldn't this return 3 <code>input_ids</code>, one for the question one for the context and one for the answer?</p>
"
72414634,how can we get the attention scores of multimodal models via hugging face library?,"<p>I was wondering if we could get the attention scores of any multimodal model using the api provided  by the hugging face library, as it's relatively easier to get such scores of normal language bert model, but what about lxmert? If anyone can answer this, it would help the understanding of such models.</p>
"
72489570,Getting random output every time on running Next Sentence Prediction code using BERT,"<p>Based on the code provided below, I am trying to run NSP (Next Sentence Prediction) on a custom dataset. The loss after training the model is different every time and the model give different accuracies every time. What am I missing or doing wrong?</p>
<pre><code>pip install transformers[torch]
from transformers import BertTokenizer, BertForNextSentencePrediction 
import torch  
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') 
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
with open('clean.txt', 'r') as fp:
    text = fp.read().split('\n')
bag = [item for sentence in text for item in sentence.split('.') if item != '']
bag_size = len(bag)
import random
 
sentence_a = []
sentence_b = []
label = []
 
for paragraph in text:
    sentences = [
        sentence for sentence in paragraph.split('.') if sentence != ''
    ]
    num_sentences = len(sentences)
    if num_sentences &gt; 1:
        start = random.randint(0, num_sentences-2)
        # 50/50 whether is IsNextSentence or NotNextSentence
        if random.random() &gt;= 0.5:
            # this is IsNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(sentences[start+1])
            label.append(0)
        else:
            index = random.randint(0, bag_size-1)
            # this is NotNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(bag[index])
            label.append(1)
 
inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
inputs['labels'] = torch.LongTensor([label]).T
class MeditationsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)
 
dataset = MeditationsDataset(inputs)
loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
from transformers import AdamW
 
# activate training mode
model.train()
# initialize optimizer
optim = AdamW(model.parameters(), lr=5e-6)
 
from tqdm import tqdm  # for our progress bar
 
epochs = 2
 
for epoch in range(epochs):
    # setup loop with TQDM and dataloader
    loop = tqdm(loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())
</code></pre>
<p>In the code below I am testing the model on unseen data:</p>
<pre><code>from torch.nn import functional as f
from torch.nn.functional import softmax
prompt = &quot;sentence 1 text&quot;
prompt2 = &quot;sentence 2 text&quot;
output = tokenizer.encode_plus(prompt,prompt2, return_tensors=&quot;pt&quot;)
result = model(**output)[0]
prob = softmax(result, dim=1)
print(prob)
</code></pre>
<p>So, the value of prob and loss is different every single time for the same unseen data which to the best of my knowledge should be same.</p>
"
72503309,Save a Bert model with custom forward function and heads on Hugginface,"<p>I have created my own BertClassifier model, starting from a pretrained and then added my own classification heads composed by different layers. After the fine-tuning, I want to save the model using model.save_pretrained() but when I print it upload it from pretrained i don't see my classifier head.
The code is the following. How can I save the all structure on my model and make it full accessible with <code> AutoModel.from_preatrained('folder_path')</code> ?
. Thanks!</p>
<pre><code>class BertClassifier(PreTrainedModel):
    &quot;&quot;&quot;Bert Model for Classification Tasks.&quot;&quot;&quot;
    config_class = AutoConfig
    def __init__(self,config, freeze_bert=True): #tuning only the head
        &quot;&quot;&quot;
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        &quot;&quot;&quot;
        #super(BertClassifier, self).__init__()
        super().__init__(config)

        # Instantiate BERT model
        # Specify hidden size of BERT, hidden size of our classifier, and number of labels
        self.D_in = 1024 #hidden size of Bert
        self.H = 512
        self.D_out = 2
 
        # Instantiate the classifier head with some one-layer feed-forward classifier
        self.classifier = nn.Sequential(
            nn.Linear(self.D_in, 512),
            nn.Tanh(),
            nn.Linear(512, self.D_out),
            nn.Tanh()
        )
 


    def forward(self, input_ids, attention_mask):


         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

</code></pre>
<pre><code>configuration=AutoConfig.from_pretrained('Rostlab/prot_bert_bfd')
model = BertClassifier(config=configuration,freeze_bert=False)
</code></pre>
<p>Saving the model after fine-tuning</p>
<pre><code>model.save_pretrained('path')
</code></pre>
<p>Loading the fine-tuned model</p>
<pre><code>model = AutoModel.from_pretrained('path') 
</code></pre>
<p>Printing the model after loading shows I have as the last layer the following and missing my 2 linear layer:</p>
<pre><code> (output): BertOutput(
          (dense): Linear(in_features=4096, out_features=1024, bias=True)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (adapters): ModuleDict()
          (adapter_fusion_layer): ModuleDict()
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (activation): Tanh()
  )
  (prefix_tuning): PrefixTuningPool(
    (prefix_tunings): ModuleDict()
  )
)
</code></pre>
"
72605644,"Mobilevit Binary classification ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1))","<p>I am using the colab notebook(<a href=""https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/mobilevit.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/mobilevit.ipynb</a>) for <strong>mobilevit</strong> to train on a dataset I have of <strong>25k pictures for 2 classes</strong>. Since it's a <strong>binary classification</strong>, I have used <strong>keras.losses.BinaryCrossentropy</strong> and <strong>Sigmoid as activation function</strong> at the last layer:-</p>
<pre><code>def create_mobilevit(num_classes=2):
inputs = keras.Input((image_size, image_size, 3))
x = layers.Rescaling(scale=1.0 / 255)(inputs)

# Initial conv-stem -&gt; MV2 block.
x = conv_block(x, filters=16)
x = inverted_residual_block(
    x, expanded_channels=16 * expansion_factor, output_channels=16
)

# Downsampling with MV2 block.
x = inverted_residual_block(
    x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2
)
x = inverted_residual_block(
    x, expanded_channels=24 * expansion_factor, output_channels=24
)
x = inverted_residual_block(
    x, expanded_channels=24 * expansion_factor, output_channels=24
)

# First MV2 -&gt; MobileViT block.
x = inverted_residual_block(
    x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2
)
x = mobilevit_block(x, num_blocks=2, projection_dim=64)

# Second MV2 -&gt; MobileViT block.
x = inverted_residual_block(
    x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2
)
x = mobilevit_block(x, num_blocks=4, projection_dim=80)

# Third MV2 -&gt; MobileViT block.
x = inverted_residual_block(
    x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2
)
x = mobilevit_block(x, num_blocks=3, projection_dim=96)
x = conv_block(x, filters=320, kernel_size=1, strides=1)

# Classification head.
x = layers.GlobalAvgPool2D()(x)
outputs = layers.Dense(num_classes, activation=&quot;sigmoid&quot;)(x)

return keras.Model(inputs, outputs)
</code></pre>
<p>And here's my dataset preparation cell:-</p>
<pre><code>batch_size = 64
auto = tf.data.AUTOTUNE
resize_bigger = 512
num_classes = 2


def preprocess_dataset(is_training=True):
    def _pp(image, label):
        if is_training:
            # Resize to a bigger spatial resolution and take the random
            # crops.
            image = tf.image.resize(image, (resize_bigger, resize_bigger))
            image = tf.image.random_crop(image, (image_size, image_size, 3))
            image = tf.image.random_flip_left_right(image)
        else:
            image = tf.image.resize(image, (image_size, image_size))
        label = tf.one_hot(label, depth=num_classes)
        return image, label

    return _pp


def prepare_dataset(dataset, is_training=True):
    if is_training:
        dataset = dataset.shuffle(batch_size * 10)
    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=auto)
    return dataset.batch(batch_size).prefetch(auto)
</code></pre>
<p>And this is the cell for training the model:-</p>
<pre><code>learning_rate = 0.002
label_smoothing_factor = 0.1
epochs = 30

optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
loss_fn = keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing_factor)


def run_experiment(epochs=epochs):
    mobilevit_xxs = create_mobilevit(num_classes=num_classes)
    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[&quot;accuracy&quot;])

    checkpoint_filepath = &quot;/tmp/checkpoint&quot;
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor=&quot;val_accuracy&quot;,
        save_best_only=True,
        save_weights_only=True,
    )

    mobilevit_xxs.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        callbacks=[checkpoint_callback],
    )
    mobilevit_xxs.load_weights(checkpoint_filepath)
    _, accuracy = mobilevit_xxs.evaluate(val_ds)
    print(f&quot;Validation accuracy: {round(accuracy * 100, 2)}%&quot;)
    return mobilevit_xxs


mobilevit_xxs = run_experiment()
</code></pre>
<p>Basically the code is identical to <a href=""https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/mobilevit.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/mobilevit.ipynb</a> except for the change in BinaryCrossEntropy loss and Sigmoid as actv. func. I don't understand why I am getting this even though I am explicitly ont-hot-coded my class labels -</p>
<pre><code>ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).
</code></pre>
"
72628556,'MaskedLMOutput' object has no attribute 'view',"<p>I wrote this:</p>
<pre><code>def forward(self, x):
    x = self.bert(x)
    
    x = x.view(x.shape[0], -1)
    x = self.fc(self.dropout(self.bn(x)))
    return x
</code></pre>
<p>but it doesn't work well, and the error is 'MaskedLMOutput' object has no attribute 'view'.
I'm considering the input might not be 'tensor' type, so I change it as below:</p>
<pre><code>def forward(self, x):
        x = torch.tensor(x)     # this part
        x = self.bert(x)
        
        x = x.view(x.shape[0], -1)
        x = self.fc(self.dropout(self.bn(x)))
        return x
</code></pre>
<p>but it still gets wrong, same error 'MaskedLMOutput' object has no attribute 'view'.</p>
<p>Could someone tell me how to fix this?  Much thanks.</p>
<p>Whole error information here:</p>
<pre><code> ------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    Input In [5], in &lt;cell line: 8&gt;()
          6 optimizer = optim.Adam(bert_punc.parameters(), lr=learning_rate_top)
          7 criterion = nn.CrossEntropyLoss()
    ----&gt; 8 bert_punc, optimizer, best_val_loss = train(bert_punc, optimizer, criterion, epochs_top, 
          9     data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations_top, best_val_loss=1e9)
    
    Input In [3], in train(model, optimizer, criterion, epochs, data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations, best_val_loss)
         17 inputs.requires_grad = False
         18 labels.requires_grad = False
    ---&gt; 19 output = model(inputs)
         20 loss = criterion(output, labels)
         21 loss.backward()
    
    File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in Module._call_impl(self, *input, **kwargs)
       1106 # If we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -&gt; 1110     return forward_call(*input, **kwargs)
       1111 # Do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    File ~\anaconda3\lib\site-packages\torch\nn\parallel\data_parallel.py:166, in DataParallel.forward(self, *inputs, **kwargs)
        163     kwargs = ({},)
        165 if len(self.device_ids) == 1:
    --&gt; 166     return self.module(*inputs[0], **kwargs[0])
        167 replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
        168 outputs = self.parallel_apply(replicas, inputs, kwargs)
    
    File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in Module._call_impl(self, *input, **kwargs)
       1106 # If we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -&gt; 1110     return forward_call(*input, **kwargs)
       1111 # Do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    File D:\BertPunc-original\model.py:21, in BertPunc.forward(self, x)
         18 x = torch.tensor(x)
         19 x = self.bert(x)
    ---&gt; 21 x = x.view(x.shape[0], -1)
         22 x = self.fc(self.dropout(self.bn(x)))
         23 return x
    
    AttributeError: 'MaskedLMOutput' object has no attribute 'view'
</code></pre>
"
72680734,Huggingface Trainer only doing 3 epochs no matter the TrainingArguments,"<p>Im new at machine learning and I'm facing an issue where I want to increase the epochs for training but .train() will only do 3 epochs. What am I doing wrong?</p>
<p><strong>This is my dataset:</strong></p>
<blockquote>
<pre><code>&gt; DatasetDict({ train: Dataset({ features: [â€˜textâ€™, â€˜labelâ€™], num_rows:
&gt; 85021 }) test: Dataset({ features: [â€˜textâ€™, â€˜labelâ€™], num_rows: 15004
&gt; }) })
</code></pre>
</blockquote>
<p><strong>and its features:</strong></p>
<pre><code>&gt; {â€˜labelâ€™: ClassLabel(num_classes=20, names=[â€˜01. AGRIâ€™, â€˜02. ALIMâ€™,
&gt; â€˜03. CHEMFERâ€™, â€˜04. ATEXâ€™, â€˜05. MACHâ€™, â€˜06. MARNAVâ€™, â€˜07. CONSTâ€™, â€˜08.
&gt; MINESâ€™, â€œ09. DOMâ€, â€˜10. TRANâ€™, â€˜11. ARARTILLâ€™, â€˜12. PREELECâ€™, â€˜13.
&gt; CERâ€™, â€˜14. ACHIMIâ€™, â€˜15. ECLAâ€™, â€˜16. HABIâ€™, â€˜17. ANDUSâ€™, â€˜18. ARBUâ€™,
&gt; â€˜19. CHIRURâ€™, â€˜20. ARPAâ€™], id=None), â€˜textâ€™: Value(dtype=â€˜stringâ€™,
&gt; id=None)}
</code></pre>
<p><strong>My Trainer:</strong></p>
<pre><code>trainer = Trainer(
model=model,
args=training_args,
train_dataset=tokenized_datasets[â€œtrainâ€],
eval_dataset=tokenized_datasets[â€œtestâ€],
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_metrics,
)
</code></pre>
<p><strong>what my .train() is showing:</strong></p>
<blockquote>
<p>***** Running training ***** Num examples = 85021 Num Epochs = 3 Instantaneous batch size per device = 8 Total train batch size (w.
parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps
= 1 Total optimization steps = 31884</p>
<p>|Epoch|Training Loss|Validation Loss|Accuracy|
|1|0.994300|0.972638|0.711610|
|2|0.825400|0.879027|0.736337|
|3|0.660800|0.893457|0.744401|</p>
</blockquote>
<p>I would like to continue training beyond the 3 epochs to increase my accuracy and continue to decrease training and validation loss. I tried changing the <code>num_train_epochs=10</code> as you can see but nothing changes.</p>
<p>This is largely my code:</p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=10,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
  logging_steps=10,
)

### Metrics
from datasets import load_metric
metric = load_metric(&quot;accuracy&quot;)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

### Trainer
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)
</code></pre>
"
72687276,Evaluate BERT Model param.requires_grad,"<p>I have a doubt regarding the evaluation on the test set of my bert model. During the eval part param.requires_grad is suppose to be True or False? indipendently if I did a full fine tuning during training or not. My model is in model.eval() mode but I want to be sure to not force nothing wrong in the Model() class when i call it for evaluation. Thanks !</p>
<pre><code>  if freeze_bert == 'True':
        for param in self.bert.parameters():
            param.requires_grad = False
            #logging.info('freeze_bert: {}'.format(freeze_bert)) 
            #logging.info('param.requires_grad: {}'.format(param.requires_grad))
    if freeze_bert == 'False':
        for param in self.bert.parameters():
            param.requires_grad = True
</code></pre>
"
72690203,Getting KeyErrors when training Hugging Face Transformer,"<p>I am generally following this tutorial (<a href=""https://huggingface.co/docs/transformers/training#:%7E:text=%F0%9F%A4%97%20Transformers%20provides%20access%20to,an%20incredibly%20powerful%20training%20technique."" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/training#:~:text=%F0%9F%A4%97%20Transformers%20provides%20access%20to,an%20incredibly%20powerful%20training%20technique.</a>) to implement fine-tuning on a pretrained transformer. The main difference is I am using my own custom dataset that is being sourced from a JSON file that has a document's text and the label it should belong to. To be able to do this I needed to create my own class which is based off of the Dataset class from pytorch. This is what that class looks like:</p>
<pre><code>class PDFsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        
        print(&quot;\n\n\n\nindex&quot;,idx)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

</code></pre>
<p>I am getting an error when training the transformer that says</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\indexes\base.py&quot;, line 3621, in get_loc
    return self._engine.get_loc(casted_key)
  File &quot;pandas\_libs\index.pyx&quot;, line 136, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas\_libs\index.pyx&quot;, line 163, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas\_libs\hashtable_class_helper.pxi&quot;, line 2131, in pandas._libs.hashtable.Int64HashTable.get_item
  File &quot;pandas\_libs\hashtable_class_helper.pxi&quot;, line 2140, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 19

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;c:\Users\e417922\Downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\HF_Transformer.py&quot;, line 147, in &lt;module&gt;
    transformer.train_transformer()
  File &quot;c:\Users\e417922\Downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\HF_Transformer.py&quot;, line 135, in train_transformer
    trainer.train()
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\transformers\trainer.py&quot;, line 1409, in train
    return inner_training_loop(
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\transformers\trainer.py&quot;, line 1625, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\dataloader.py&quot;, line 530, in __next__
    data = self._next_data()
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\dataloader.py&quot;, line 570, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 49, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;c:\Users\e417922\Downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\HF_Transformer.py&quot;, line 42, in __getitem__
    for key in self.encodings[idx]:
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\series.py&quot;, line 958, in __getitem__
    return self._get_value(key)
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\series.py&quot;, line 1069, in _get_value
    loc = self.index.get_loc(label)
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\indexes\base.py&quot;, line 3623, in get_loc
    raise KeyError(key) from err
KeyError: 19
</code></pre>
<p>The KeyError that it fails on changes each time I run it. I'm a beginner with Transformers and HuggingFace so I have no clue what's causing this problem.</p>
<p>Edit:
Sample Input is a JSON File where elements would look like this:
{
&quot;text_clean&quot;: [
&quot;article with a few hundred words&quot;,
another article with a lot of words&quot;,
&quot;yet another article&quot;
],
&quot;most_similar_label&quot;:[
&quot;Quantum&quot;
&quot;Artificial intelligence&quot;
&quot;Materials&quot;
]
}</p>
<p>Full Code:</p>
<pre><code>import tkinter as tk
from tkinter import filedialog
import json
import pandas as pd
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
from transformers import TrainingArguments, Trainer
import numpy as np
from datasets import load_metric
from sklearn.model_selection import train_test_split
import torch

class PDFsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        
        print(&quot;\n\n\n\nindex&quot;,idx)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

class HFTransformer:
    def __init__ (self):
        pass

    def import_from_json(self):
        #Prompts user to select json file
        root = tk.Tk()
        root.withdraw()
        self.json_file_path = filedialog.askopenfile().name
        #opens json file and loads data
        with open(self.json_file_path, &quot;r&quot;) as json_file:
                try:
                    json_load = json.load(json_file)
                except:
                    raise ValueError(&quot;No PDFs to convert to JSON&quot;)
        self.pdfs = json_load
        #converts json file data to dataframe for easier manipulation
        self.pdfs = pd.DataFrame.from_dict(self.pdfs)

        for index in range(len(self.pdfs[&quot;new_tags&quot;])):
            if self.pdfs[&quot;new_tags&quot;][index] == &quot;&quot;:
                self.pdfs[&quot;new_tags&quot;][index] = self.pdfs[&quot;most_similar_label&quot;][index]

        self.pdfs[&quot;labels&quot;] = self.pdfs[&quot;new_tags&quot;].apply(lambda val: self.change_tag_to_num(val))
        # for label in self.data[&quot;labels&quot;]:
     
    def change_tag_to_num(self, value):
        if value == &quot;Quantum&quot;:
            return 0
        elif value == &quot;Artificial intelligence&quot;:
            return 1
        elif value == &quot;Materials&quot;:
            return 2
        elif value == &quot;Energy&quot;:
            return 3
        elif value == &quot;Defense&quot;:
            return 4
        elif value == &quot;Satellite&quot;:
            return 5
        elif value == &quot;Other&quot;:
            return 6

    def tokenize_dataset(self):
        tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

        X_train, X_test, y_train, y_test = train_test_split(self.pdfs[&quot;text_clean&quot;], self.pdfs[&quot;labels&quot;],test_size=0.2)

        train_encodings = X_train.apply(lambda string: tokenizer(string, truncation=True, padding=True,max_length=10))
        test_encodings = X_test.apply(lambda string: tokenizer(string, truncation=True, padding=True,max_length=10))
    
        
        self.train_dataset = PDFsDataset(train_encodings, y_train)
    
        data_to_add = {&quot;input_ids&quot;: [], &quot;token_type_ids&quot;: [], &quot;attention_mask&quot;: []}

        for i in self.train_dataset.encodings:
            data_to_add[&quot;input_ids&quot;].append(i[&quot;input_ids&quot;])
            data_to_add[&quot;token_type_ids&quot;].append(i[&quot;token_type_ids&quot;])
            data_to_add[&quot;attention_mask&quot;].append(i[&quot;attention_mask&quot;])

        self.train_dataset.encodings = data_to_add

        self.eval_dataset = PDFsDataset(test_encodings,y_test)
        data_to_add = {&quot;input_ids&quot;: [], &quot;token_type_ids&quot;: [], &quot;attention_mask&quot;: []}

        for i in self.eval_dataset.encodings:
            data_to_add[&quot;input_ids&quot;].append(i[&quot;input_ids&quot;])
            data_to_add[&quot;token_type_ids&quot;].append(i[&quot;token_type_ids&quot;])
            data_to_add[&quot;attention_mask&quot;].append(i[&quot;attention_mask&quot;])
        
        self.eval_dataset.encodings = data_to_add

    def train_transformer(self):
        model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=7)
        training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)
        self.metric = load_metric(&quot;accuracy&quot;)
        training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)
    

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            compute_metrics=self.compute_metrics
        )
        trainer.train()
    def compute_metrics(self, eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return self.metric.compute(predictions=predictions, references=labels)


if __name__  == &quot;__main__&quot;:
    transformer = HFTransformer()
    transformer.import_from_json()
    transformer.tokenize_dataset()
    transformer.train_transformer()
</code></pre>
"
72695297,Difference between from_config and from_pretrained in HuggingFace,"<pre class=""lang-py prettyprint-override""><code>num_labels = 3 if task.startswith(&quot;mnli&quot;) else 1 if task==&quot;stsb&quot; else 2
preconfig = DistilBertConfig(n_layers=6)
    
model1 = AutoModelForSequenceClassification.from_config(preconfig)
model2 = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)
</code></pre>
<p>I am modifying <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=545PP3o8IrJV"" rel=""nofollow noreferrer"">this code</a> (modified code is provided above) to test DistilBERT transformer layer depth size via <code>from_config</code> since from my knowledge <code>from_pretrained</code> uses 6 layers because in the paper section 3 they said:</p>
<blockquote>
<p>we initialize the student from the teacher by taking one layer out of two</p>
</blockquote>
<p>While what I want to test is various sizes of layers. To test whether both are the same, I tried running the <code>from_config</code>
with <code>n_layers=6</code> because based on the documentation <a href=""https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertConfig"" rel=""nofollow noreferrer"">DistilBertConfig</a> the <code>n_layers</code> is used to determine the transformer block depth. However as I run <code>model1</code> and <code>model2</code> I found that with  SST-2 dataset, in accuracy:</p>
<ul>
<li><code>model1</code> achieved only <code>0.8073</code></li>
<li><code>model2</code> achieved <code>0.901</code></li>
</ul>
<p>If they both behave the same I expect the result to be somewhat similar but 10% drop is a significant drop, therefore I believe there ha to be a difference between the functions. Is there a reason behind the difference of the approach (for example <code>model1</code> has not yet applied hyperparameter search) and is there a way to make both functions behave the same? Thank you!</p>
"
72724748,Huggingface transformers padding vs pad_to_max_length,"<p>I'm running a code by using <code>pad_to_max_length = True</code> and everything works fine. Only I get a warning as follow:</p>
<blockquote>
<p>FutureWarning: The <code>pad_to_max_length</code> argument is deprecated and
will be removed in a future version, use <code>padding=True</code> or
<code>padding='longest'</code> to pad to the longest sequence in the batch, or
use <code>padding='max_length'</code> to pad to a max length. In this case, you
can give a specific length with <code>max_length</code> (e.g. <code>max_length=45</code>) or
leave max_length to None to pad to the maximal input size of the model
(e.g. 512 for Bert).</p>
</blockquote>
<p>But when I change <code>pad_to_max_length = True</code> to <code>padding='max_length'</code> I get this error:</p>
<pre><code>RuntimeError: stack expects each tensor to be equal size, but got [60] at entry 0 and [64] at entry 6
</code></pre>
<p>How can I change the code to the new version? Is there anything I got wrong with the warning documentation?</p>
<p>This is my encoder:</p>
<pre><code>encoding = self.tokenizer.encode_plus(
    poem,
    add_special_tokens=True,
    max_length= 60,
    return_token_type_ids=False,
    pad_to_max_length = True,
    return_attention_mask=True,
    return_tensors='pt',
)
</code></pre>
"
72747399,Finetuning LayoutLM on FUNSD-like dataset - index out of range in self,"<p>I'm experimenting with <code>huggingface</code> transformers to finetune <code>microsoft/layoutlmv2-base-uncased</code> through <code>AutoModelForTokenClassification</code> on my custom dataset that is similar to FUNSD (pre-processed and normalized). After a few iterations of training I get this error :</p>
<pre><code> Traceback (most recent call last):
  File &quot;layoutlmV2/train.py&quot;, line 137, in &lt;module&gt;
    trainer.train()
  File &quot;..../lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1409, in train
    return inner_training_loop(
  File &quot;..../lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1651, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File &quot;..../lib/python3.8/site-packages/transformers/trainer.py&quot;, line 2345, in training_step
    loss = self.compute_loss(model, inputs)
  File &quot;..../lib/python3.8/site-packages/transformers/trainer.py&quot;, line 2377, in compute_loss
    outputs = model(**inputs)
  File &quot;..../lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py&quot;, line 1228, in forward
    outputs = self.layoutlmv2(
  File &quot;..../lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py&quot;, line 902, in forward
    text_layout_emb = self._calc_text_embeddings(
  File &quot;..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py&quot;, line 753, in _calc_text_embeddings
    spatial_position_embeddings = self.embeddings._calc_spatial_position_embeddings(bbox)
  File &quot;..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py&quot;, line 93, in _calc_spatial_position_embeddings
    h_position_embeddings = self.h_position_embeddings(bbox[:, :, 3] - bbox[:, :, 1])
  File &quot;..../lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;..../lib/python3.8/site-packages/torch/nn/modules/sparse.py&quot;, line 158, in forward
    return F.embedding(
  File &quot;..../lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 2203, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>After further inspection (vocab size, bboxes, dimensions, classes...) I noticed that there's negative values inside the input tensor causing the error. While input tensors of successful previous iterations have unsigned integers only. These negative numbers are returned by <code>_calc_spatial_position_embeddings(self, bbox)</code> in <code>modeling_layoutlmv2.py</code></p>
<p>line 92 :</p>
<pre><code>h_position_embeddings = self.h_position_embeddings(bbox[:, :, 3] - bbox[:, :, 1])
</code></pre>
<ul>
<li>What may cause the returned input values to be negative?</li>
<li>What could I do to prevent this error from happening?</li>
</ul>
<hr />
<p><em>Example of the input tensor that triggers the error in <code>torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</code> :</em></p>
<pre><code>tensor([[ 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11,  9,  9,  9,  9,  9,  9,  9,  9,  9,
          9,  9,  9,  9,  9,  9,  9, 10, 10, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12,
         12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,
          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,
          8,  5,  5,  5,  5,  5,  5, -6, -6, -6, -6, -6, -6,  1,  1,  1,  1,  1,
          5,  5,  5,  5,  5,  5,  7,  5,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0]])
</code></pre>
"
72775559,resize_token_embeddings on the a pertrained model with different embedding size,"<p>I would like to ask about the way to change the embedding size of the trained model.</p>
<p>I have a trained model <code>models/BERT-pretrain-1-step-5000.pkl</code>.
Now I am adding a new token <code>[TRA]</code>to the tokeniser and try to use the <code>resize_token_embeddings</code> to the pertained one.</p>
<pre class=""lang-py prettyprint-override""><code>from pytorch_pretrained_bert_inset import BertModel #BertTokenizer 
from transformers import AutoTokenizer
from torch.nn.utils.rnn import pad_sequence
import tqdm

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model_bert = BertModel.from_pretrained('bert-base-uncased', state_dict=torch.load('models/BERT-pretrain-1-step-5000.pkl', map_location=torch.device('cpu')))

#print(tokenizer.all_special_tokens) #--&gt; ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
#print(tokenizer.all_special_ids)    #--&gt; [100, 102, 0, 101, 103]

num_added_toks = tokenizer.add_tokens(['[TRA]'], special_tokens=True)
model_bert.resize_token_embeddings(len(tokenizer))  # --&gt; Embedding(30523, 768)
print('[TRA] token id: ', tokenizer.convert_tokens_to_ids('[TRA]'))  # --&gt; 30522
</code></pre>
<p>But I encountered the error:</p>
<pre><code>AttributeError: 'BertModel' object has no attribute 'resize_token_embeddings'
</code></pre>
<p>I assume that it is because the <code>model_bert(BERT-pretrain-1-step-5000.pkl)</code> I had has the different embedding size.
I would like to know if there is any way to fit the embedding size of my modified tokeniser and the model I would like to use as the initial weights.</p>
<p>Thanks a lot!!</p>
"
72777174,HuggingFace TFRobertaModel detailed summary,"<pre><code>from transformers import RobertaTokenizer, TFRobertaModel
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)
model = TFRobertaModel.from_pretrained(&quot;roberta-base&quot;)
</code></pre>
<p>I want a detailed layer summary of this HuggingFace <code>TFRobertaModel()</code> so that I can visualize shapes, layers and customize if needed. However, when I did:
<code>model.summary()</code>, it just shows everything in a single layer. I tried digging into it's different attributes, but not able to get a detailed layer summary. Is it possible to do so?</p>
<pre><code>Model: &quot;tf_roberta_model_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
roberta (TFRobertaMainLayer) multiple                  124645632 
=================================================================
Total params: 124,645,632
Trainable params: 124,645,632
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>Also, there is a related <a href=""https://discuss.huggingface.co/t/tensorflow-model-summary-doesnt-show-detail-of-tfbertmodel/799"" rel=""nofollow noreferrer"">question</a> in HuggingFace forum which hasn't been answered yet.</p>
"
72815200,Does HuggingFace's Trainer automatically ignore features not required by the model?,"<p>I am a new user of Transformers and I have successfully fine-tuned a BERT model following the tutorial.</p>
<p>However, I have one question about the features I send to the Trainer and those accepted by the BERT model.</p>
<p>Specifically, my original dataset contains two columns named â€œtextâ€ and â€œlabelâ€. After tokenizing the â€œtextâ€, the dataset object now has three more columns named â€œinput_idsâ€, â€œtoken_type_idsâ€, and â€œattention_maskâ€. I understand that these three columns are required by the BERT model, but I didnâ€™t drop the original â€œtextâ€ column when I fed the dataset to the Trainer API.</p>
<p>So my question is, does BERT automatically ignore non-relevant features? (maybe this is achieved quietly by the Trainer API) Or should I remove these columns, leaving only â€œinput_idsâ€, â€œtoken_type_idsâ€, and â€œattention_maskâ€?</p>
<p>For example, below is my dataset object:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 6851
    })
    test: Dataset({
        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 762
    })
})
</code></pre>
<p>And I fed it and my model to the Trainer:</p>
<pre><code>trainer = Trainer(
    model,
    training_args,
    train_dataset=data[&quot;train&quot;],
    eval_dataset=data[&quot;test&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
</code></pre>
<p>What happened to the &quot;text&quot; feature?</p>
"
72845812,"bert-base-uncased: TypeError: tuple indices must be integers or slices, not tuple","<p>I want to see embeddings for the input text I give to the model, and then feed it to the rest of the BERT. To do so, I partitioned the model into two sequential models, but I must have done it wrong because rest_of_bert model raises TypeError. Original model does not raise any error with the input_ids as input processed with text_to_input function.</p>
<p>Input[0]:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
cls_token_id = tokenizer.cls_token_id
sep_token_id = tokenizer.sep_token_id
pad_token_id = tokenizer.pad_token_id

model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
model.eval()
</code></pre>
<p>Output[0]:</p>
<pre class=""lang-py prettyprint-override""><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
</code></pre>
<p>Input[1]:</p>
<pre class=""lang-py prettyprint-override""><code>def text_to_input(text):
  x = tokenizer.encode(text, add_special_tokens=False) # returns python list
  x = [cls_token_id] + x + [sep_token_id]
  token_count = len(x)
  pad_count = 512 - token_count
  x = x + [pad_token_id for i in range(pad_count)]
  return torch.tensor([x])

extract_embeddings = torch.nn.Sequential(list(model.children())[0])
rest_of_bert = torch.nn.Sequential(*list(model.children())[1:])

input_ids = text_to_input('A sentence.')
x_embedding = extract_embeddings(input_ids)
output = rest_of_bert(x_embedding)
</code></pre>
<p>Output[1]:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-5-d371d8a2fb3c&gt; in &lt;module&gt;()
     12 input_ids = text_to_input('A sentence.')
     13 x_embedding = extract_embeddings(input_ids)
---&gt; 14 output = rest_of_bert(x_embedding)

4 frames
/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py in __getitem__(self, k)
    220             return inner_dict[k]
    221         else:
--&gt; 222             return self.to_tuple()[k]
    223 
    224     def __setattr__(self, name, value):

TypeError: tuple indices must be integers or slices, not tuple
</code></pre>
"
72854302,Are the pre-trained layers of the Huggingface BERT models frozen?,"<p>I use the following classification model from Huggingface:</p>
<pre class=""lang-py prettyprint-override""><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;dbmdz/bert-base-german-cased&quot;, num_labels=2).to(device)
</code></pre>
<p>As I understand, this adds a dense layer at the end of the pre-trained model which has 2 output nodes. But are all the pre-trained layers before that frozen? Or are they also updated when fine-tuning? I can't find information about that in the docs...</p>
<p>So do I still have to do something like this?:</p>
<pre class=""lang-py prettyprint-override""><code>for param in model.bert.parameters():
    param.requires_grad = False
</code></pre>
"
72885929,REBEL: Relation Extraction By End-to-end Language generation,"<p>I am trying to run the following pieces of code:</p>
<pre><code>from transformers import pipeline

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
# We need to use the tokenizer manually since we need special tokens.
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(&quot;Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic&quot;, return_tensors=True, return_text=False)[0][&quot;generated_token_ids&quot;]])
print(extracted_text[0])
# Function to parse the generated text and extract the triplets
def extract_triplets(text):
    triplets = []
    relation, subject, relation, object_ = '', '', '', ''
    text = text.strip()
    current = 'x'
    for token in text.replace(&quot;&lt;s&gt;&quot;, &quot;&quot;).replace(&quot;&lt;pad&gt;&quot;, &quot;&quot;).replace(&quot;&lt;/s&gt;&quot;, &quot;&quot;).split():
        if token == &quot;&lt;triplet&gt;&quot;:
            current = 't'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
                relation = ''
            subject = ''
        elif token == &quot;&lt;subj&gt;&quot;:
            current = 's'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
            object_ = ''
        elif token == &quot;&lt;obj&gt;&quot;:
            current = 'o'
            relation = ''
        else:
            if current == 't':
                subject += ' ' + token
            elif current == 's':
                object_ += ' ' + token
            elif current == 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object_ != '':
        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
    return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)
</code></pre>
<p>Unfortunately I get the following error.</p>
<blockquote>
<p>TypeError: Can't convert {'output_ids': [[0, 50267, 221, 20339, 2615, 102, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 2034, 11, 5, 6833, 15752, 10014, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 19664, 1780, 219, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 2034, 11, 5, 6833, 15752, 10014, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 18978, 3497, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 6308, 6833, 15752, 10014, 2]]} to Sequence</p>
</blockquote>
<p>Can anyone provide a solution for this error?</p>
"
72912929,Huggingface - Finetuning in Tensorflow with custom datasets,"<p>I have been battling with my own implementation on my dataset with a different transformer model than the tutorial, and I have been getting this error <code>AttributeError: 'NoneType' object has no attribute 'dtype'</code>, when i was starting to train my model. I have been trying to debug for hours, and then I have tried the tutorial from hugging face as it can be found here <a href=""https://huggingface.co/transformers/v3.2.0/custom_datasets.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/v3.2.0/custom_datasets.html</a>. Running this exact code, so I could identify my mistake, also leads to the same error.</p>
<pre><code>!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz

from pathlib import Path
def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is &quot;neg&quot; else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

from transformers import TFDistilBertForSequenceClassification

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)
</code></pre>
<p>My goal will be to perform multi-label text classification on my own custom dataset, which unfortunately I cannot share for privacy reasons. If anyone could point out what is wrong with this implementation, will be highly appreciated.</p>
"
72936945,Pass elements of a list in a function,"<p>I have a function that is able to create triples and relationships from text. However, when I create a list of a column that contains text and pass it through the function, it only processes the first row, or item of the list. Therefore, I am wondering how the whole list can be processed within this function. Maybe a for loop would work?</p>
<p>The following line contains the list</p>
<pre><code>rez_dictionary = {'Decent Little Reader, Poor Tablet',
 'Ok For What It Is',
 'Too Heavy and Poor weld quality,',
 'difficult mount',
 'just got it installed'}
</code></pre>
<pre><code>from transformers import pipeline

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')

# We need to use the tokenizer manually since we need special tokens.
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(rez_dictionary, return_tensors=True, return_text=False)[0][&quot;generated_token_ids&quot;]])

print(extracted_text[0])
</code></pre>
<p>If anyone has a suggestion, I am looking forward for it.</p>
<p>Would it also be possible to get the output adjusted to the following format:</p>
<pre><code># Function to parse the generated text and extract the triplets
def extract_triplets(text):
    triplets = []
    relation, subject, relation, object_ = '', '', '', ''
    text = text.strip()
    current = 'x'
    for token in text.replace(&quot;&lt;s&gt;&quot;, &quot;&quot;).replace(&quot;&lt;pad&gt;&quot;, &quot;&quot;).replace(&quot;&lt;/s&gt;&quot;, &quot;&quot;).split():
        if token == &quot;&lt;triplet&gt;&quot;:
            current = 't'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
                relation = ''
            subject = ''
        elif token == &quot;&lt;subj&gt;&quot;:
            current = 's'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
            object_ = ''
        elif token == &quot;&lt;obj&gt;&quot;:
            current = 'o'
            relation = ''
        else:
            if current == 't':
                subject += ' ' + token
            elif current == 's':
                object_ += ' ' + token
            elif current == 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object_ != '':
        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
    return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)
</code></pre>
"
72955752,Finding the scores for each tweet with a BERT-based sentiment analysis model,"<p>I am doing a sentiment analysis of twitter posts and I have a question regarding â€œGerman Sentiment Classification with Bertâ€:</p>
<p>I would like to display the sentiment score (positive, negative, neutral) for each tweet like it is shown on the models card on <a href=""https://huggingface.co/oliverguhr/german-sentiment-bert"" rel=""nofollow noreferrer"">huggingface</a>(screenshot)
I tried to go through the implementation of the mode by stepping into every line of code but could not figure out how to find the scores.</p>
<p><a href=""https://i.stack.imgur.com/PPa99.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PPa99.png"" alt=""enter image description here"" /></a></p>
<p>My code is based on the following code:</p>
<pre><code>
model = SentimentModel()

texts = [
    &quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    &quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    &quot;Der Test verlief positiv.&quot;,&quot;Sie fÃ¤hrt ein grÃ¼nes Auto.&quot;]
       
result = model.predict_sentiment(texts)
print(result)
</code></pre>
"
73082185,prediction logits using lxmert with hugging face library,"<p>how can we get the prediction logits in the lxmert model using hugging face library? It's fairly easy to get in visualbert, but I'm not able to get it with the lxmert model. In case of visualbert model, the keys I'm getting are :</p>
<pre class=""lang-py prettyprint-override""><code>['prediction_logits', 'seq_relationship_logits', 'attentions']
</code></pre>
<p>and with the help of lxmert mode, the keys are :</p>
<pre class=""lang-py prettyprint-override""><code>['language_output', 'vision_output', 'pooled_output', 'language_attentions', 'vision_attentions', 'cross_encoder_attentions']
</code></pre>
<p>Even though there's a mention of prediction logits in the documentation I am not able to get them, if someone can help that would be great.</p>
<p>EDIT : <a href=""https://colab.research.google.com/drive/103TRUWj6mXPIERuvdKgLBbSF8N_SfEe1?usp=sharing"" rel=""nofollow noreferrer"">Link</a> to colab notebook for lxmert.</p>
"
73094001,huggingface model: Kernel Restarting The kernel for .ipynb appears to have died. It will restart automatically,"<p>I am using a pre-trained <a href=""https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-spanish"" rel=""nofollow noreferrer"">Huggingface</a> model for Speech Recognition in Spanish to transcribe text from 922 .mp3 files. Nevertheless, after transcribing less than 10 files, it breaks, showing the following message:</p>
<blockquote>
<p>Kernel Restarting: The kernel for .ipynb appears to have died. It will restart automatically</p>
</blockquote>
<p>I have tried other alternatives mentioned in other questions, like reinstalling <code>conda</code> or <code>mkl</code>, or trying cloud servers like Saturn Cloud or Colab. But the same happens in Saturn Cloud, and in plain Colab the runtime is capped.</p>
<p>The code is the following:</p>
<pre><code>from huggingsound import SpeechRecognitionModel
model = SpeechRecognitionModel(&quot;jonatasgrosman/wav2vec2-large-xlsr-53-spanish&quot;)

# Get file paths to each .mp3 file
root_dir = os.path.join(&quot;..&quot;, &quot;data&quot;, &quot;raw&quot;, &quot;audio_files&quot;)

# Filter .mp3 files only
file_paths = [os.path.join(root_dir, file) for file in os.listdir(root_dir)]
file_paths = [file for file in file_paths if (os.path.isfile(file) and 
                                              file[-4:] == &quot;.mp3&quot;)]

# Transcribe all audios:
transcriptions = []
for file_id in range(len(file_paths)):
    transcript = model.transcribe([file_paths[file_id]])
    transcriptions.append(transcript)
</code></pre>
<p>Python: 3.8.5</p>
<p><strong>Files info:</strong>
.mp3 files of political speeches sampled at 44kHz, even though the package recommends to sample them at 16kHz</p>
<p><strong>Packages info:</strong></p>
<p>NumPy: 1.23.1</p>
<p>SciPy: 1.8.1</p>
<p>Model: jonatasgrosman/wav2vec2-large-xlsr-53-spanish (22nd July 2022 version)</p>
<p><strong>Jupyter notebook packages info:</strong></p>
<p>IPython          : 7.29.0</p>
<p>ipykernel        : 6.4.1</p>
<p>ipywidgets       : 7.6.5</p>
<p>jupyter_client   : 7.0.6</p>
<p>jupyter_core     : 4.9.1</p>
<p>jupyter_server   : 1.4.1</p>
<p>jupyterlab       : 3.2.1</p>
<p>nbclient         : 0.5.3</p>
<p>nbconvert        : 6.1.0</p>
<p>nbformat         : 5.1.3</p>
<p>notebook         : 6.4.6</p>
<p>qtconsole        : 5.2.2</p>
<p>traitlets        : 5.1.1</p>
<p>Any help into how to solve this would be much appreciated.</p>
"
73107703,issue when importing BloomTokenizer from transformers in python,"<p>I am trying to import BloomTokenizer from transformers</p>
<pre><code>from transformers import BloomTokenizer
</code></pre>
<p>and I receive the following error</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ImportError: cannot import name 'BloomTokenizer' from 'transformers' 
(/root/miniforge3/envs/pytorch/lib/python3.8/site-packages/transformers/__init__.py)
</code></pre>
<p>my version of transformers:</p>
<pre><code>transformers                 4.20.1
</code></pre>
<p>what could I do to be able to import BloomTokenizer?</p>
"
73117866,BART loading from HuggingFace requires logging in,"<p>I'm trying to use pretrained model from HuggingFace. However, I get the following error,</p>
<pre><code>OSError: bart-large is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.
</code></pre>
<p>The code I'm using is a little dated, and I have not found definite solution, and I'm not sure if it is a bug, or I really need to somehow log in to use this model.</p>
"
73126202,The output of the PyTourch DL network doesn't match the last layer provided in the network,"<p>I am trying to build a PyTorch model that can predict the rank of a text where the output is a float number between 0 and 1.</p>
<p>My input details are</p>
<ol>
<li>My batch size is 32.</li>
<li>Max length for the tokenizer is 116</li>
<li>In addition to the masks and Ids generated from the tokenizer, I am adding 11 values that were generated through preprocessing to the input text.</li>
</ol>
<p>S the entire input shape would be 32 for batch and array with 127 item for each sample text provided</p>
<p>My layers are as follows:</p>
<ol>
<li>a DistilBERT uncased transformer. and I am using the DistilBERT tokenizer over the text.</li>
<li>The following layer is a CNN that takes the output of the DistilBERT (127 channel) as input and provide 64 channels as output, with kernel=1</li>
<li>After this, 6 CNN layers each input is 64 and output is 64 with a kernel size of 3 and dilation increasing from 2 to 32. On top of each CNN, there is a relu and a maxpooling with 2 as kernal size.</li>
<li>My last CNN layer (and where the issue is happening) have 64 input channels and 32 output channels with a kernel size of 1 and a relu with AdaptiveMaxPool1d with size of 32 on top of it</li>
<li>Linear layer takes 32 and output 16</li>
<li>Linear layer takes 16 and output 1</li>
</ol>
<p>below is my code</p>
<pre><code>class Dataset(Dataset):
    def __init__(self, df, max_len, bert_model_name, multi=1):
        super().__init__()
        self.df = df.reset_index(drop=True)
        self.max_len = max_len
        
        self.tokenizer = DistilBertTokenizer.from_pretrained(
            bert_model_name, 
            do_lower_case=True,
            strip_accents=True,
            wordpieces_prefix=None,
            use_fast=True
        )
        self.multiplier = multi

    def __getitem__(self, index):
        row = self.df.iloc[index]
        
        inputs = self.tokenizer.encode_plus(
            row.source,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding=&quot;max_length&quot;,
            return_token_type_ids=True,
            truncation=True
        )
        
        return (
            t.LongTensor(t.cat([
                t.LongTensor([
                    row.n_total_cells * self.multiplier, 
                    row.n_code_cells * self.multiplier,
                    row.n_markdown_cells * self.multiplier,
                    row.word_counts * self.multiplier,
                    row.line_counts * self.multiplier,
                    row.empty_line_counts * self.multiplier,
                    row.full_lines_count * self.multiplier,
                    row.text_lines_count * self.multiplier,
                    row.tag_lines_count * self.multiplier,
                    row.weight * self.multiplier,
                    row.weight_counts * self.multiplier,
                ]), 
                t.LongTensor(inputs['input_ids']),
            ], 0)), 
            
            t.LongTensor(t.cat([
                t.ones(11, dtype=t.long),
                t.LongTensor(inputs['attention_mask']),
            ], 0)),
        )

class BModel(nn.Module):
    def __init__(self, bert_model_name):
        super(BModel, self).__init__()
        self.distill_bert = DistilBertModel.from_pretrained(bert_model_name)       

        self.hidden_size = self.distill_bert.config.hidden_size
        print(self.hidden_size) # 768
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        
        self.cnn_layers()

    def forward(self, inputs):
        dbert = self.cnn_forward(inputs[0], inputs[1])
        return dbert

 def cnn_layers(self):
        self.layers = 4
        kernel_size = 3
        inp = 127
        out = 32
        grades = [2, 4, 8, 16, 32, 64, ]
        
        self.convs = nn.ModuleList()
        self.relus = nn.ModuleList()
        self.maxs = nn.ModuleList()
        self.norms = nn.ModuleList()
        
        self.start_conv = nn.Conv1d(
            in_channels=inp,
            out_channels=64,
            kernel_size=1,
            bias=True
        )
        
        for i in range(self.layers):
            # dilated convolutions
            self.convs.append(nn.Conv1d(
                in_channels=64,
                out_channels=64,
                kernel_size = kernel_size,
                bias=False,
                dilation=grades[i]
            ))

            self.relus.append(nn.ReLU())

            self.maxs.append(nn.MaxPool1d(
                kernel_size=kernel_size-1,
            ))

            self.norms.append(nn.BatchNorm1d(
                num_features=64,
            ))


        self.end_conv = nn.Conv1d(
            in_channels=64,
            out_channels=out,
            kernel_size=1,
            bias=True
        )
        
        self.max_pool = nn.AdaptiveMaxPool1d(out)
        
        self.top1 = nn.Linear(out, 16) 
        self.top2 = nn.Linear(16, 1)
        
    def cnn_forward(self, ids, masks):
        x = self.distill_bert(ids, masks)[0]
        x = self.relu(x)
        x = self.dropout(x)
        print(f&quot;X size after BERT:&quot;, x.size())
        
        x = self.start_conv(x)
        print(f&quot;X size after First Conv:&quot;, x.size())
        for i in range(self.layers):
            x = self.norms[i](self.maxs[i](self.relus[i](self.convs[i](x))))
            print(f&quot;X size after {i} CNN dilation:&quot;, x.size())
            
        x = self.max_pool(t.abs(self.end_conv(x)))
        print(&quot;X size after AdaptiveMaxPool1d:&quot;, x.size())
        
        x = self.top1(x)
        print(&quot;X size after before-last linear:&quot;, x.size())
        
        x = self.top2(x)
        print(&quot;X size after last linear:&quot;, x.size())
        return x

</code></pre>
<p>Printing the output size after each layer would be as below</p>
<pre><code>X size after First Conv: torch.Size([32, 64, 768])
X size after 0 CNN dilation: torch.Size([32, 64, 382])
X size after 1 CNN dilation: torch.Size([32, 64, 187])
X size after 2 CNN dilation: torch.Size([32, 64, 85])
X size after 3 CNN dilation: torch.Size([32, 64, 26])
X size after AdaptiveMaxPool1d: torch.Size([32, 32, 32])
X size after before-last linear: torch.Size([32, 32, 16])
X size after last linear: torch.Size([32, 32, 1]
</code></pre>
<p>The issue I am facing is after the AdaptiveMaxPool1d, the output of this layer suppose to be 2 dimensions instead of 3 <code>[32, 32]</code> instead of <code>[32, 32, 32]</code></p>
<p>The output of AdaptiveMaxPool1d fits into the linear layer but is with one extra dimension causing the output pred to differ from the true input</p>
<p>when I check the pred size vs the true size it would be</p>
<pre><code>y_pred shape (12480,)
y_val shape (390,)
</code></pre>
<p>and the code blow with this error</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [13], in &lt;cell line: 21&gt;()
     17 # print(mkdn_train_loader, mkdn_val_loader)
     18 
     19 ########################################################################################################################

File E:\KAGGLE_COMP\pt_model.py:796, in train(model, train_loader, val_loader, epochs, patience, path)
    793 print('y_val shape', y_val.shape)
    794 print(y_pred[:10])
--&gt; 796 print(&quot;Validation MSE:&quot;, np.round(mean_squared_error(y_val, y_pred), 4))
    797 print()
    799 early_stopping(np.round(mean_squared_error(y_val, y_pred), 4), model)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:438, in mean_squared_error(y_true, y_pred, sample_weight, multioutput, squared)
    378 def mean_squared_error(
    379     y_true, y_pred, *, sample_weight=None, multioutput=&quot;uniform_average&quot;, squared=True
    380 ):
    381     &quot;&quot;&quot;Mean squared error regression loss.
    382 
    383     Read more in the :ref:`User Guide &lt;mean_squared_error&gt;`.
   (...)
    436     0.825...
    437     &quot;&quot;&quot;
--&gt; 438     y_type, y_true, y_pred, multioutput = _check_reg_targets(
    439         y_true, y_pred, multioutput
    440     )
    441     check_consistent_length(y_true, y_pred, sample_weight)
    442     output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:94, in _check_reg_targets(y_true, y_pred, multioutput, dtype)
     60 def _check_reg_targets(y_true, y_pred, multioutput, dtype=&quot;numeric&quot;):
     61     &quot;&quot;&quot;Check that y_true and y_pred belong to the same regression task.
     62 
     63     Parameters
   (...)
     92         the dtype argument passed to check_array.
     93     &quot;&quot;&quot;
---&gt; 94     check_consistent_length(y_true, y_pred)
     95     y_true = check_array(y_true, ensure_2d=False, dtype=dtype)
     96     y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\validation.py:332, in check_consistent_length(*arrays)
    330 uniques = np.unique(lengths)
    331 if len(uniques) &gt; 1:
--&gt; 332     raise ValueError(
    333         &quot;Found input variables with inconsistent numbers of samples: %r&quot;
    334         % [int(l) for l in lengths]
    335     )

ValueError: Found input variables with inconsistent numbers of samples: [390, 12480]

</code></pre>
<p>I need to know what I must change to make this run and the size is passed with correct shape.</p>
"
73127139,Equivalent to tokenizer() in Transformers 2.5.0?,"<p>I am trying to convert the following code to work with Transformers 2.5.0. As written, it works in version 4.18.0, but not 2.5.0.</p>
<pre><code># Converting pretrained BERT classification model to regression model
# i.e. extracting base model and swapping out heads

from transformers import BertTokenizer, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification, AutoConfig, AutoModelForTokenClassification
import torch
import numpy as np

old_model = BertForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-yelp-polarity&quot;)
model = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, num_labels=1) 
model.bert = old_model.bert

# Ensure that model parameters are equivalent except for classifier head layer
for param_name in model.state_dict():
    if 'classifier' not in param_name:
        sub_param, full_param = model.state_dict()[param_name], old_model.state_dict()[param_name] # type: torch.Tensor, torch.Tensor
        assert (sub_param.cpu().numpy() == full_param.cpu().numpy()).all(), param_name


tokenizer = BertTokenizer.from_pretrained(&quot;textattack/bert-base-uncased-yelp-polarity&quot;)
inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    logits = model(**inputs).logits

output_value = np.array(logits)[0][0]
print(output_value)
</code></pre>
<p>tokenizer is not callable with transformers 2.5.0, resulting the following:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-d83f0d613f4b&gt; in &lt;module&gt;
     19 
     20 
---&gt; 21 inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
     22 
     23 with torch.no_grad():

TypeError: 'BertTokenizer' object is not callable
</code></pre>
<p>However, attempting to replace tokenizer() with tokenizer.tokenize() results in the following:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-2-1d431131eb87&gt; in &lt;module&gt;
     21 
     22 with torch.no_grad():
---&gt; 23     logits = model(**inputs).logits
     24 
     25 output_value = np.array(logits)[0][0]

TypeError: BertForSequenceClassification object argument after ** must be a mapping, not list
</code></pre>
<p>Any help would be greatly appreciated.</p>
<hr />
<h2>Solution</h2>
<p>Using tokenizer.encode_plus() as suggested by @cronoik:</p>
<pre><code>tokenized = tokenizer.encode_plus(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    logits = model(**tokenized)

output_value = np.array(logits)[0]
print(output_value)
</code></pre>
"
73155719,Do weights of the [PAD] token have a function?,"<p>When looking at the weights of a transformer model, I noticed that the embedding weights for the padding token <code>[PAD]</code> are nonzero. I was wondering whether these weights have a function, since they are ignored in the multi-head attention layers.</p>
<p>Would it make sense to set these weights to zeros? The weights can be seen using <code>model.base_model.embeddings.word_embeddings.weight[PAD_ID]</code> where typically <code>PAD_ID=0</code>.</p>
"
73182816,Why there are no logs and which model is saved?,"<p>Iâ€™m using <code>Trainer</code> to train my model.</p>
<p>I have the following outputs on screen:</p>
<pre><code>Epoch   Training Loss   Validation Loss Accuracy
0   No log  1.114260    0.342667
1   No log  0.939480    0.545333
2   No log  0.816581    0.660000
3   No log  0.752204    0.710667
4   No log  0.741462    0.741333
5   No log  0.801005    0.754667
6   0.675800    0.892765    0.748000
7   0.675800    1.190328    0.752000
8   0.675800    1.272624    0.745333
</code></pre>
<ol>
<li>Why there are no logs for epochs 0-5 ? (Do I need to configure / enable them ?)</li>
<li><strong>Epoch #5</strong> got best accuracy.
When I will use predict, which model checkpoint will be used ?
(the model which trained after 5 epochs or the model which trained after 8 epochs) ?</li>
</ol>
"
73201858,"""No module named keras"" error in transformers","<p>I'm trying to load a pretrained BERT model in a sagemaker training job using the transformers library and I'm getting &quot;No modul named keras error&quot;. You can find the relevant code, imports and requirements.txt below</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras import applications
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import optimizers
from tensorflow.keras import metrics
from tensorflow.keras import Model
from transformers import TFAutoModel
from transformers import BertTokenizer
from tensorflow.keras.layers import LeakyReLU

bert1 = TFAutoModel.from_pretrained('path/to/BERT', from_pt=True)
</code></pre>
<p>requirements.txt (I haven't set any versions for these):</p>
<pre><code>transformers
torch
SentencePiece
</code></pre>
<p>Other env settings:</p>
<pre><code>Python= 3.7
tensorflow= 2.3
</code></pre>
<p>I had used these exact settings a few months back and faced no issues, so not sure why I'm getting this error now</p>
<p>Edit: based on the answers I received, I added Keras to my requirements and also added from TensorFlow import keras statement, and now I'm getting the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'keras.saving'
</code></pre>
"
73232595,Huggingface Trainer load_best_model f1 score vs. loss and overfitting,"<p>I have trained a roberta-large and specified <code>load_best_model_at_end=True</code> and <code>metric_for_best_model=f1</code>. During training, I can see overfitting after the 6th epoch, which is the sweetspot. In Epoch 8, which is the next one to evaluate due to gradient accumulation, we can see that train loss decreases and eval_loss increases. Thus, overfitting starts. The transformers trainer in the end loads the model from epoch 8, checkpoint <code>-14928</code>, as the f1 score is a bit highea. I was wondering, in theory, wouldn't be the model from epoch 6 be better suited, as it did not overfit? Or does one really go for the f1 metric here even though the model did overfit? (the eval loss decreased in epochs &lt;6 constantly).</p>
<p>The test_loss from the second checkpoint, which is then loaded as the &quot;best&quot;, is 0.128. Is it possible to lower that using the first checkpoint which should be the better model anyway?</p>
<pre><code>checkpoint-11196:
{'loss': 0.0638, 'learning_rate': 8.666799323450404e-06, 'epoch': 6.0}

{'eval_loss': 0.09599845856428146, 'eval_accuracy': 0.9749235986101227, 'eval_precision': 0.9648319293367138, 'eval_recall': 0.9858766505097777, 'eval_f1': 0.9752407721241682, 'eval_runtime': 282.2294, 'eval_samples_per_second': 84.637, 'eval_steps_per_second': 2.647, 'epoch': 6.0}

VS.

checkpoint-14928:
{'loss': 0.0312, 'learning_rate': 7.4291115311909265e-06, 'epoch': 8.0}

{'eval_loss': 0.12377820163965225, 'eval_accuracy': 0.976305103194206, 'eval_precision': 0.9719324391455539, 'eval_recall': 0.9810295838208257, 'eval_f1': 0.9764598236566295, 'eval_runtime': 276.7619, 'eval_samples_per_second': 86.309, 'eval_steps_per_second': 2.699, 'epoch': 8.0}
</code></pre>
"
73244442,HuggingFace Trainer() cannot report to wandb,"<p>I am trying to set trainer with arguments <code>report_to</code> to <code>wandb</code>, refer to <a href=""https://docs.wandb.ai/guides/integrations/huggingface#getting-started-track-experiments"" rel=""nofollow noreferrer"">this docs</a>
with config:</p>
<pre><code>training_args = TrainingArguments(
    output_dir=&quot;test_trainer&quot;,
    evaluation_strategy=&quot;steps&quot;,
    learning_rate=config.learning_rate,
    num_train_epochs=config.epochs,
    weight_decay=config.weight_decay,
    logging_dir=config.logging_dir,
    report_to=&quot;wandb&quot;,
    save_total_limit=1,
    per_device_train_batch_size=config.batch_size,
    per_device_eval_batch_size=config.batch_size,
    fp16=True,
    load_best_model_at_end=True,
    seed=42
)
</code></pre>
<p>yet when I set trainer with:</p>
<pre><code>trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)
</code></pre>
<p>it shows:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;
      4     train_dataset=train_dataset,
      5     eval_dataset=eval_dataset,
----&gt; 6     compute_metrics=compute_metrics
      7 )

~/.virtualenvs/transformers_lab/lib/python3.7/site-packages/transformers/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)
    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;
    287             )
--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)
    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks
    290         self.callback_handler = CallbackHandler(

~/.virtualenvs/transformers_lab/lib/python3.7/site-packages/transformers/integrations.py in get_reporting_integration_callbacks(report_to)
    794         if integration not in INTEGRATION_TO_CALLBACK:
    795             raise ValueError(
--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;
    797             )
    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]

ValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.
</code></pre>
<p>Have anyone got same error before?</p>
"
73247922,RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): initialization failed,"<p>Goal: Run a <strong>GPT-2</strong> model instance.</p>
<p>I am using the latest Tensorflow and Hugging Face ðŸ¤— Transformers.</p>
<ul>
<li>Tensorflow - 2.9.1</li>
<li>Transformers - 4.21.1</li>
</ul>
<p>Notebook:</p>
<pre><code>pip install tensorflow
</code></pre>
<pre><code>pip install transformers
</code></pre>
<pre><code>from transformers import pipeline, set_seed

generator = pipeline('text-generation', model='gpt2')
set_seed(42)
</code></pre>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
ImportError: numpy.core.multiarray failed to import

The above exception was the direct cause of the following exception:

SystemError                               Traceback (most recent call last)
SystemError: &lt;built-in method __contains__ of dict object at 0x7f5b58a64d00&gt; returned a result with an error set

The above exception was the direct cause of the following exception:

ImportError                               Traceback (most recent call last)
~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/utils/import_utils.py in _get_module(self, module_name)
   1001         try:
-&gt; 1002             return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
   1003         except Exception as e:

~/anaconda3/envs/python3/lib/python3.8/importlib/__init__.py in import_module(name, package)
    126             level += 1
--&gt; 127     return _bootstrap._gcd_import(name[level:], package, level)
    128 

~/anaconda3/envs/python3/lib/python3.8/importlib/_bootstrap.py in _gcd_import(name, package, level)

~/anaconda3/envs/python3/lib/python3.8/importlib/_bootstrap.py in _find_and_load(name, import_)

~/anaconda3/envs/python3/lib/python3.8/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

~/anaconda3/envs/python3/lib/python3.8/importlib/_bootstrap.py in _load_unlocked(spec)

~/anaconda3/envs/python3/lib/python3.8/importlib/_bootstrap_external.py in exec_module(self, module)

~/anaconda3/envs/python3/lib/python3.8/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/pipelines/__init__.py in &lt;module&gt;
     36 from ..utils import HUGGINGFACE_CO_RESOLVE_ENDPOINT, http_get, is_tf_available, is_torch_available, logging
---&gt; 37 from .audio_classification import AudioClassificationPipeline
     38 from .automatic_speech_recognition import AutomaticSpeechRecognitionPipeline

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/pipelines/audio_classification.py in &lt;module&gt;
     19 from ..utils import add_end_docstrings, is_torch_available, logging
---&gt; 20 from .base import PIPELINE_INIT_ARGS, Pipeline
     21 

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/pipelines/base.py in &lt;module&gt;
     33 from ..feature_extraction_utils import PreTrainedFeatureExtractor
---&gt; 34 from ..modelcard import ModelCard
     35 from ..models.auto.configuration_auto import AutoConfig

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/modelcard.py in &lt;module&gt;
     43 )
---&gt; 44 from .training_args import ParallelMode
     45 from .utils import (

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/training_args.py in &lt;module&gt;
     25 from .debug_utils import DebugOption
---&gt; 26 from .trainer_utils import (
     27     EvaluationStrategy,

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/trainer_utils.py in &lt;module&gt;
     46 if is_tf_available():
---&gt; 47     import tensorflow as tf
     48 

~/anaconda3/envs/python3/lib/python3.8/site-packages/tensorflow/__init__.py in &lt;module&gt;
     36 
---&gt; 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader

~/anaconda3/envs/python3/lib/python3.8/site-packages/tensorflow/python/__init__.py in &lt;module&gt;
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---&gt; 37 from tensorflow.python.eager import context
     38 

~/anaconda3/envs/python3/lib/python3.8/site-packages/tensorflow/python/eager/context.py in &lt;module&gt;
     34 from tensorflow.python import tf2
---&gt; 35 from tensorflow.python.client import pywrap_tf_session
     36 from tensorflow.python.eager import executor

~/anaconda3/envs/python3/lib/python3.8/site-packages/tensorflow/python/client/pywrap_tf_session.py in &lt;module&gt;
     18 from tensorflow.python import pywrap_tensorflow
---&gt; 19 from tensorflow.python.client._pywrap_tf_session import *
     20 from tensorflow.python.client._pywrap_tf_session import _TF_SetTarget

ImportError: initialization failed

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_4924/2487422996.py in &lt;cell line: 1&gt;()
----&gt; 1 from transformers import pipeline, set_seed
      2 
      3 generator = pipeline('text-generation', model='gpt2')
      4 set_seed(42)

~/anaconda3/envs/python3/lib/python3.8/importlib/_bootstrap.py in _handle_fromlist(module, fromlist, import_, recursive)

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/utils/import_utils.py in __getattr__(self, name)
    990             value = self._get_module(name)
    991         elif name in self._class_to_module.keys():
--&gt; 992             module = self._get_module(self._class_to_module[name])
    993             value = getattr(module, name)
    994         else:

~/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/utils/import_utils.py in _get_module(self, module_name)
   1002             return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
   1003         except Exception as e:
-&gt; 1004             raise RuntimeError(
   1005                 f&quot;Failed to import {self.__name__}.{module_name} because of the following error (look up to see its&quot;
   1006                 f&quot; traceback):\n{e}&quot;

RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):
initialization failed
</code></pre>
<pre><code>def query(payload, multiple, min_tokens, max_tokens):
    nlp_setup()
    list_dict = generator(payload, min_length=min_tokens, max_new_tokens=max_tokens, num_return_sequences=multiple)
    return [d['generated_text'].split(payload)[1].strip() for d in list_dict
</code></pre>
<pre><code>output = query(&quot;Banking customer's needs:&quot;, 3000, 50, 50)
</code></pre>
<p>RunTime, SystemError and ImportError all occur during import of transformers:</p>
<p>RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): initialization failed</p>
"
73257704,get contrastive_logits_per_image with flava model using huggingface library,"<p>I have used a code of Flava model from this link:</p>
<pre><code>https://huggingface.co/docs/transformers/model_doc/flava#transformers.FlavaModel.forward.example
</code></pre>
<p>But I am getting the following error:</p>
<pre><code>'FlavaModelOutput' object has no attribute 'contrastive_logits_per_image'
</code></pre>
<p>I tried using <code>FlavaForPreTraining</code> model instead, so updated code was :</p>
<pre class=""lang-py prettyprint-override""><code>from PIL import Image
import requests
from transformers import FlavaProcessor, FlavaForPreTraining

model = FlavaForPreTraining.from_pretrained(&quot;facebook/flava-full&quot;)
processor = FlavaProcessor.from_pretrained(&quot;facebook/flava-full&quot;)

url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=[&quot;a photo of a cat&quot;], images=image, return_tensors=&quot;pt&quot;, padding=True, return_codebook_pixels = True)

inputs.update(
    {
        &quot;input_ids_masked&quot;: inputs.input_ids,
    }
)

outputs = model(**inputs)

logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
</code></pre>
<p>but I'm still getting this as error:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:714: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  &quot;The `device` argument is deprecated and will be removed in v5 of Transformers.&quot;, FutureWarning

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-44-bdb428b8184a&gt; in &lt;module&gt;()
----&gt; 1 outputs = model(**inputs)

2 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1128         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130             return forward_call(*input, **kwargs)
   1131         # Do not call functions when jit is used
   1132         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in forward(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)
   1968             if mim_labels is not None:
   1969                 mim_labels = self._resize_to_2d(mim_labels)
-&gt; 1970                 bool_masked_pos = self._resize_to_2d(bool_masked_pos)
   1971                 mim_labels[bool_masked_pos.ne(True)] = self.ce_ignore_index
   1972 

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in _resize_to_2d(self, x)
   1765 
   1766     def _resize_to_2d(self, x: torch.Tensor):
-&gt; 1767         if x.dim() &gt; 2:
   1768             x = x.view(x.size(0), -1)
   1769         return x

AttributeError: 'NoneType' object has no attribute 'dim'
</code></pre>
<p>Can anyone provide suggestions with what's going wrong?</p>
"
73334654,What is better custom training the bert model or use the model with pretrained data?,"<p>I am coding my own models for a time but I saw huggingface and started using it. I wanted to know whether I should use the pretrained model or train model (the same hugging face model) with my own dataset. I am trying to make a question answering model.</p>
<p>I have dataset of 10k-20k questions.</p>
"
73358850,ValueError: No gradients provided for any variable: ['tf_deberta_v2_for_sequence_classification_1/deberta/embeddings/word_embeddings,"<p>I am trying to fine tune a transformer model for text classification but I am having trouble training the model. I have tried many things but none of them seem to work. I have also tried different solutions on other question but they didn't work. I am using 'microsoft/deberta-v3-base' model for fine tuning. Here's my code:</p>
<pre><code>train_dataset = Dataset.from_pandas(df_tr[['text', 'label']]).class_encode_column(&quot;label&quot;)
val_dataset = Dataset.from_pandas(df_tes[['text', 'label']]).class_encode_column(&quot;label&quot;)

train_tok_dataset = train_dataset.map(tokenizer_func, batched=True, remove_columns=('text'))
val_tok_dataset = val_dataset.map(tokenizer_func, batched=True, remove_columns=('text'))
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(config.model_name, num_labels=3)
transformer_model = TFAutoModelForSequenceClassification.from_pretrained(config.model_name, output_hidden_states=True)

input_ids = tf.keras.Input(shape=(config.max_len, ),dtype='int32')
attention_mask = tf.keras.Input(shape=(config.max_len, ), dtype='int32')

transformer = transformer_model([input_ids, attention_mask])    
hidden_states = transformer[1] # get output_hidden_states
#print(hidden_states)
hidden_states_size = 4 # count of the last states 
hiddes_states_ind = list(range(-hidden_states_size, 0, 1))

selected_hiddes_states = tf.keras.layers.concatenate(tuple([hidden_states[i] for i in hiddes_states_ind]))

# Now we can use selected_hiddes_states as we want
output = tf.keras.layers.Dense(128, activation='relu')(selected_hiddes_states)
output=tf.keras.layers.Flatten()(output)
output = tf.keras.layers.Dense(3, activation='softmax')(output)
model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = output)
from transformers import create_optimizer
import tensorflow as tf

batch_size = 8
num_epochs = config.epochs
#batches_per_epoch = len(tokenized_tweets[&quot;train&quot;]) // batch_size
total_train_steps = int(num_steps * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=num_steps/2)

model.compile(optimizer=optimizer)

with tf.device('GPU:0'):
    model.fit(x=[np.array(train_tok_dataset[&quot;input_ids&quot;]),np.array(train_tok_dataset[&quot;attention_mask&quot;])],
y=tf.keras.utils.to_categorical(y_train,num_classes=3),
validation_data=([np.array(val_tok_dataset[&quot;input_ids&quot;]),np.array(val_tok_dataset[&quot;attention_mask&quot;])],tf.keras.utils.to_categorical(y_test,num_classes=3)),
epochs=config.epochs,class_weight={0:0.57,1:0.18,2:0.39})
</code></pre>
<p>It seems like a small issue, but I am new to tensorflow and transformers so I couldn't sort it out myself.</p>
"
73405232,Transformer summariser pipeline giving different results on same model with fixed seed,"<p>I am using a HuggingFace summariser pipeline and I noticed that if I train a model for 3 epochs and then at the end run evaluation on all 3 epochs with fixed random seeds, I get a different results based on whether I restart the python console 3 times or whether I load the different model (one for every epoch) on the same summariser object in a loop, and I would like to understand why we have this strange behaviour.</p>
<p>While my results are based on ROUGE score on a large dataset, I have made this small reproducible example to show this issue. Instead of using the weights of the same model at different training epochs, I decided to demonstrate using two different summarization models, but the effect is the same. Grateful for any help.</p>
<p>Notice how in the first run I firstly use the <code>facebook/bart-large-cnn</code> model and then the <code>lidiya/bart-large-xsum-samsum</code> model without shutting the python terminal. In the second run I only use <code>lidiya/bart-large-xsum-samsum</code> model and get different output (which should not be the case).</p>
<p>NOTE: this reproducible example won't work on a CPU machine as it doesn't seem sensitive to <code>torch.use_deterministic_algorithms(True)</code> and it might give different results every time when run on a CPU, so should be reproduced on a GPU.</p>
<p>FIRST RUN</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, pipeline
import torch

# random text taken from UK news website
text = &quot;&quot;&quot;
The veteran retailer Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation, describing the lack of action as â€œhorrifyingâ€, with a prime minister â€œon shore leaveâ€ leaving a situation where â€œnobody is in chargeâ€.
Responding to Julyâ€™s 10.1% headline rate, the Conservative peer and Asda chair said: â€œWe have been very, very slow in recognising this train coming down the tunnel and itâ€™s run quite a lot of people over and we now have to deal with the aftermath.â€
Attacking a lack of leadership while Boris Johnson is away on holiday, he said: â€œWeâ€™ve got to have some action. The captain of the ship is on shore leave, right, nobodyâ€™s in charge at the moment.â€
Lord Rose, who is a former boss of Marks &amp; Spencer, said action was needed to kill â€œperniciousâ€ inflation, which he said â€œerodes wealth over timeâ€. He dismissed claims by the Tory leadership candidate Liz Trussâ€™s camp that it would be possible for the UK to grow its way out of the crisis.
&quot;&quot;&quot;

seed = 42
torch.cuda.manual_seed_all(seed)
torch.use_deterministic_algorithms(True)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/bart-large-cnn&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)

tokenizer = AutoTokenizer.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)
print(output)
</code></pre>
<p>output from <code>lidiya/bart-large-xsum-samsum</code> model should be</p>
<pre><code>[{'summary_text': 'The UK economy is in crisis because of inflation. The government has been slow to react to it. Boris Johnson is on holiday.'}]
</code></pre>
<p>SECOND RUN (you must restart python to conduct the experiment)</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, pipeline
import torch

text = &quot;&quot;&quot;
The veteran retailer Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation, describing the lack of action as â€œhorrifyingâ€, with a prime minister â€œon shore leaveâ€ leaving a situation where â€œnobody is in chargeâ€.
Responding to Julyâ€™s 10.1% headline rate, the Conservative peer and Asda chair said: â€œWe have been very, very slow in recognising this train coming down the tunnel and itâ€™s run quite a lot of people over and we now have to deal with the aftermath.â€
Attacking a lack of leadership while Boris Johnson is away on holiday, he said: â€œWeâ€™ve got to have some action. The captain of the ship is on shore leave, right, nobodyâ€™s in charge at the moment.â€
Lord Rose, who is a former boss of Marks &amp; Spencer, said action was needed to kill â€œperniciousâ€ inflation, which he said â€œerodes wealth over timeâ€. He dismissed claims by the Tory leadership candidate Liz Trussâ€™s camp that it would be possible for the UK to grow its way out of the crisis.
&quot;&quot;&quot;

seed = 42
torch.cuda.manual_seed_all(seed)
torch.use_deterministic_algorithms(True)

tokenizer = AutoTokenizer.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)
print(output)
</code></pre>
<p>output should be</p>
<pre><code>[{'summary_text': 'The government has been slow to deal with inflation. Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation.'}]
</code></pre>
<p>Why is the first output different from the second one?</p>
"
73409904,Is it possible to get the meaning of each word using BERT?,"<p>I'm a linguist, I'm new to AI and</p>
<p>I would like to know if BERT is able to get the meaning of each word based on context.</p>
<p>I've done some searches and found that BERT is able to do that and that if I'm not wrong, it recognizes them/ converts them into unique vectors, but that's not the output I want.</p>
<p>What I want is to get the meaning/ or the components that constitute the meaning of each word, written in plain English, is this possible?</p>
"
73415504,Error importing LayoutLMv2ForTokenClassification from HuggingFace,"<p>I am trying to run this <a href=""https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb"" rel=""nofollow noreferrer"">demo notebook</a> on colab and I am getting the following pytorch error when importing <code>LayoutLMv2ForTokenClassification</code>:</p>
<pre><code>from transformers import LayoutLMv2ForTokenClassification
</code></pre>
<p>Error:</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in _get_module(self, module_name)
   1029         try:
-&gt; 1030             return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
   1031         except Exception as e:

~\AppData\Local\Programs\Python\Python39\lib\importlib\__init__.py in import_module(name, package)
    126             level += 1
--&gt; 127     return _bootstrap._gcd_import(name[level:], package, level)
    128 

~\AppData\Local\Programs\Python\Python39\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

~\AppData\Local\Programs\Python\Python39\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

~\AppData\Local\Programs\Python\Python39\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

~\AppData\Local\Programs\Python\Python39\lib\importlib\_bootstrap.py in _load_unlocked(spec)

~\AppData\Local\Programs\Python\Python39\lib\importlib\_bootstrap_external.py in exec_module(self, module)

~\AppData\Local\Programs\Python\Python39\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\transformers\models\layoutlmv2\modeling_layoutlmv2.py in &lt;module&gt;
     48     import detectron2
---&gt; 49     from detectron2.modeling import META_ARCH_REGISTRY
     50 

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\__init__.py in &lt;module&gt;
     19 )
---&gt; 20 from .meta_arch import (
     21     META_ARCH_REGISTRY,

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\meta_arch\__init__.py in &lt;module&gt;
      5 
----&gt; 6 from .panoptic_fpn import PanopticFPN
      7 

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\meta_arch\panoptic_fpn.py in &lt;module&gt;
     13 from .build import META_ARCH_REGISTRY
---&gt; 14 from .rcnn import GeneralizedRCNN
     15 from .semantic_seg import build_sem_seg_head

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\meta_arch\rcnn.py in &lt;module&gt;
     17 from ..proposal_generator import build_proposal_generator
---&gt; 18 from ..roi_heads import build_roi_heads
     19 from .build import META_ARCH_REGISTRY

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\roi_heads\__init__.py in &lt;module&gt;
     14 )
---&gt; 15 from .roi_heads import (
     16     ROI_HEADS_REGISTRY,

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\roi_heads\roi_heads.py in &lt;module&gt;
     16 from ..matcher import Matcher
---&gt; 17 from ..poolers import ROIPooler
     18 from ..proposal_generator.proposal_utils import add_ground_truth_to_proposals

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\poolers.py in &lt;module&gt;
      9 from detectron2.structures import Boxes
---&gt; 10 from detectron2.utils.tracing import assert_fx_safe
     11 

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\detectron2\utils\tracing.py in &lt;module&gt;
      3 import torch
----&gt; 4 from torch.fx._symbolic_trace import _orig_module_call
      5 from torch.fx._symbolic_trace import is_fx_tracing as is_fx_tracing_current

ModuleNotFoundError: No module named 'torch.fx._symbolic_trace'

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-36-b5a73c6d310a&gt; in &lt;module&gt;
      4 device_ids = [0,1]
      5 
----&gt; 6 from transformers import LayoutLMv2ForTokenClassification, TrainingArguments, Trainer
      7 from datasets import load_metric
      8 import numpy as np

~\AppData\Local\Programs\Python\Python39\lib\importlib\_bootstrap.py in _handle_fromlist(module, fromlist, import_, recursive)

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in __getattr__(self, name)
   1019         elif name in self._class_to_module.keys():
   1020             module = self._get_module(self._class_to_module[name])
-&gt; 1021             value = getattr(module, name)
   1022         else:
   1023             raise AttributeError(f&quot;module {self.__name__} has no attribute {name}&quot;)

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in __getattr__(self, name)
   1018             value = self._get_module(name)
   1019         elif name in self._class_to_module.keys():
-&gt; 1020             module = self._get_module(self._class_to_module[name])
   1021             value = getattr(module, name)
   1022         else:

~\PycharmProjects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in _get_module(self, module_name)
   1030             return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
   1031         except Exception as e:
-&gt; 1032             raise RuntimeError(
   1033                 f&quot;Failed to import {self.__name__}.{module_name} because of the following error (look up to see its&quot;
   1034                 f&quot; traceback):\n{e}&quot;

RuntimeError: Failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):
No module named 'torch.fx._symbolic_trace'
</code></pre>
<p>Can anyone please help?</p>
<p>Thanks!</p>
"
73428120,RuntimeError: mse_cuda not implemented for Long when training a transformer.Trainer,"<p>I'm attempting to train a HuggingFace <code>Trainer</code> but seeing the following error:</p>
<pre><code>RuntimeError: &quot;mse_cuda&quot; not implemented for 'Long' when training a transformer.Trainer
</code></pre>
<p>I've tried this in multiple cloud environments (CPU &amp; GPU) with no luck. The dataset (<code>tok_dds</code>) is of the following shape and type, and I've ensured there are no NULL values.</p>
<pre><code>Dataset({
    features: ['label', 'title', 'text', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5000
})

{'label': int,
 'title': str,
 'text': str,
 'input': str,
 'input_ids': list,
 'token_type_ids': list,
 'attention_mask': list}
</code></pre>
<p>I have defined my loss functions as below:</p>
<pre><code>def corr(x,y): return np.corrcoef(x,y)[0][1]
def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}
</code></pre>
<p>However, when attempting to Train the <code>model_nm = 'microsoft/deberta-v3-small'</code> on the train/test split of my dataset. I see the following error:</p>
<pre><code>dds = tok_ds.train_test_split(0.25, seed=42)
tokz = AutoTokenizer.from_pretrained(model_nm)
model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)
trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],
                  tokenizer=tokz, compute_metrics=corr_d)
...
...
File /shared-libs/python3.9/py/lib/python3.9/site-packages/torch/nn/functional.py:3280, in mse_loss(input, target, size_average, reduce, reduction)
   3277     reduction = _Reduction.legacy_get_string(size_average, reduce)
   3279 expanded_input, expanded_target = torch.broadcast_tensors(input, target)
-&gt; 3280 return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: &quot;mse_cuda&quot; not implemented for 'Long' when training a transformer.Trainer
</code></pre>
<p>Here are the args passed into the <code>Trainer</code> if it's relevant:</p>
<pre><code>args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,
    evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,
    num_train_epochs=epochs, weight_decay=0.01, report_to='none')
</code></pre>
<p>Here's is what I think may be relevant environment information</p>
<pre><code>!python --version
Python 3.9.13

!pip list
Package                       Version
----------------------------- ------------
...
transformers                  4.21.1
huggingface-hub               0.8.1
pandas                        1.2.5
protobuf                      3.19.4
scikit-learn                  1.1.1
tensorflow                    2.9.1
torch                         1.12.0
</code></pre>
<p>Can anyone point me in the right direction to solve this problem?</p>
"
73433868,"""SystemError: google/protobuf/pyext/descriptor.cc:358: bad argument to internal function"" while using Audio Transformers in Hugging Face","<p>I am trying to do a task of &quot;Speech2Text&quot; using transformer model in Hugging Face.</p>
<p>I tried the code in <a href=""https://huggingface.co/docs/transformers/v4.21.1/en/model_doc/speech_to_text"" rel=""nofollow noreferrer"">this</a> documentation on hugging face</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset

model = Speech2TextForConditionalGeneration.from_pretrained(&quot;facebook/s2t-small-librispeech-asr&quot;)
processor = Speech2TextProcessor.from_pretrained(&quot;facebook/s2t-small-librispeech-asr&quot;)


ds = load_dataset(&quot;hf-internal-testing/librispeech_asr_demo&quot;, &quot;clean&quot;, split=&quot;validation&quot;)

inputs = processor(ds[0][&quot;audio&quot;][&quot;array&quot;], sampling_rate=ds[0][&quot;audio&quot;][&quot;sampling_rate&quot;], return_tensors=&quot;pt&quot;)
generated_ids = model.generate(inputs[&quot;input_features&quot;], attention_mask=inputs[&quot;attention_mask&quot;])

transcription = processor.batch_decode(generated_ids)
transcription
</code></pre>
<p>but when I tried to run this code in Google Colab I am receiving the following error :</p>
<p><code>SystemError: google/protobuf/pyext/descriptor.cc:358: bad argument to internal function</code></p>
<p>On checking the other error lines it seems that on calling <code>processor()</code>, <code>return_tesnors is None</code> even though it is specified as <code>pt</code>. Due to which code is importing <code>tensorflow</code> and that error is coming. (know <a href=""https://github.com/tensorflow/tensorflow/issues/48797"" rel=""nofollow noreferrer"">issue</a>)
<a href=""https://i.stack.imgur.com/3wuRs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3wuRs.png"" alt=""error message"" /></a></p>
<p>Full error message :</p>
<pre class=""lang-py prettyprint-override""><code>SystemError                               Traceback (most recent call last)
&lt;ipython-input-4-2a3231ef630c&gt; in &lt;module&gt;
      9 ds = load_dataset(&quot;hf-internal-testing/librispeech_asr_demo&quot;, &quot;clean&quot;, split=&quot;validation&quot;)
     10 
---&gt; 11 inputs = processor(ds[0][&quot;audio&quot;][&quot;array&quot;], sampling_rate=ds[0][&quot;audio&quot;][&quot;sampling_rate&quot;], return_tensors=&quot;pt&quot;)
     12 
     13 generated_ids = model.generate(inputs[&quot;input_features&quot;], attention_mask=inputs[&quot;attention_mask&quot;])

10 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/speech_to_text/processing_speech_to_text.py in __call__(self, *args, **kwargs)
     51         information.
     52         &quot;&quot;&quot;
---&gt; 53         return self.current_processor(*args, **kwargs)
     54 
     55     def batch_decode(self, *args, **kwargs):

/usr/local/lib/python3.7/dist-packages/transformers/models/speech_to_text/feature_extraction_speech_to_text.py in __call__(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_tensors, sampling_rate, return_attention_mask, **kwargs)
    230             pad_to_multiple_of=pad_to_multiple_of,
    231             return_attention_mask=return_attention_mask,
--&gt; 232             **kwargs,
    233         )
    234 

/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_sequence_utils.py in pad(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)
    161 
    162         if return_tensors is None:
--&gt; 163             if is_tf_available() and _is_tensorflow(first_element):
    164                 return_tensors = &quot;tf&quot;
    165             elif is_torch_available() and _is_torch(first_element):

/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py in _is_tensorflow(x)
     96 
     97 def _is_tensorflow(x):
---&gt; 98     import tensorflow as tf
     99 
    100     return isinstance(x, tf.Tensor)

/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py in &lt;module&gt;
     35 import typing as _typing
     36 
---&gt; 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     39 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py in &lt;module&gt;
     35 
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---&gt; 37 from tensorflow.python.eager import context
     38 
     39 # pylint: enable=wildcard-import

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py in &lt;module&gt;
     27 import six
     28 
---&gt; 29 from tensorflow.core.framework import function_pb2
     30 from tensorflow.core.protobuf import config_pb2
     31 from tensorflow.core.protobuf import coordination_config_pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py in &lt;module&gt;
     14 
     15 
---&gt; 16 from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
     17 from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
     18 from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py in &lt;module&gt;
     14 
     15 
---&gt; 16 from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py in &lt;module&gt;
     14 
     15 
---&gt; 16 from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py in &lt;module&gt;
    148   ,
    149   'DESCRIPTOR' : _RESOURCEHANDLEPROTO,
--&gt; 150   '__module__' : 'tensorflow.core.framework.resource_handle_pb2'
    151   # @@protoc_insertion_point(class_scope:tensorflow.ResourceHandleProto)
    152   })

SystemError: google/protobuf/pyext/descriptor.cc:358: bad argument to internal function
</code></pre>
<p>here's my colab <a href=""https://colab.research.google.com/drive/1AlsVUwtlzFnBeQN3wQCUGubkfDbLccIH?usp=sharing"" rel=""nofollow noreferrer"">link</a> for reference</p>
<p>Let me know what can be done to resolve this error<br />
Thank you</p>
"
73459649,BART Tokenizer tokenises same word differently?,"<p>I have noticed that if I tokenize a full text with many sentences, I sometimes get a different number of tokens than if I tokenise each sentence individually and add up the tokens. I have done some debugging and have this small reproducible example to show the issue</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')

print(tokenizer.tokenize(&quot;Thames is a river&quot;))
print(tokenizer.tokenize(&quot;We are in London. Thames is a river&quot;))
</code></pre>
<p>I get the following output</p>
<pre><code>['Th', 'ames', 'Ä is', 'Ä a', 'Ä river']
['We', 'Ä are', 'Ä in', 'Ä London', '.', 'Ä Thames', 'Ä is', 'Ä a', 'Ä river']
</code></pre>
<p>I would like to understand why the word Thames has been split into two tokens when itâ€™s at the start of sequence, whereas itâ€™s a single word if itâ€™s not at the start of sequence. I have noticed this behaviour is very frequent and, assuming itâ€™s not a bug, I would like to understand why the BART tokeniser behaves like this.</p>
"
73530622,The size of Logits of Roberta model is weird,"<p>My input size is [8,22]. A batch with 8 tokenized sentences with a length of 22.
I dont want to use the default classifier.</p>
<pre><code>model = RobertaForSequenceClassification.from_pretrained(&quot;xlm-roberta-large&quot;)
 
model.classifier=nn.Identity()
</code></pre>
<p>After model(batch)
The size of result is torch.Size([8, 22, 1024]). I have no idea why. Should it be [8,1024]?</p>
"
73593136,"TypeError: dropout(): argument 'input' (position 1) must be Tensor, not tuple","<p>I am studying NLP and trying to make a model for classifying sentences.  I am creating my class with a model but I get an error saying that the input should be of type Tensor, not tuple. I use 4.21.2 transformers version.</p>
<pre class=""lang-py prettyprint-override""><code>class BertClassificationModel(nn.Module):
    def __init__(self, bert_model_name, num_labels, dropout=0.1):
        super(BertClassificationModel, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained(bert_model_name, return_dict=False)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(768, num_labels)
        self.num_labels = num_labels
    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        pooled_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits
</code></pre>
<p><code>TypeError: dropout(): argument 'input' (position 1) must be Tensor, not tuple</code></p>
"
73675928,some parameters appear in more than one parameter group,"<p>When I try the below I get the error <code>ValueError: some parameters appear in more than one parameter group</code>. However, inspecting the model it is not clear to me what is the overlapping module.</p>
<p>The only possiblity as to why it might think so <strong>may be because</strong> <code>lm_head</code> and <code>transformer.wte</code> have parameters named <code>weight</code>. I'm wondering if this name is what is causing this error.</p>
<p>I am doing this so that I can have the lower layers &quot;moving slowly&quot; compared to the upper layers. Happy to hear if there is an alternative way to do these discriminative learning rates where I don't have overlapping parameters (if any).</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModelForCausalLM

language_model = AutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)
FREEZE_LAYERS = 2

caption_params = [
        {&quot;params&quot;: language_model.lm_head.parameters() , &quot;lr&quot;: 1e-4},
        {&quot;params&quot;: language_model.transformer.ln_f.parameters() , &quot;lr&quot;: 1e-4},
        {&quot;params&quot;: language_model.transformer.h[FREEZE_LAYERS:].parameters() , &quot;lr&quot;: 5e-5},
        {&quot;params&quot;: language_model.transformer.wte.parameters() , &quot;lr&quot;: 1e-5},
]
optimizer = torch.optim.Adam(caption_params)
</code></pre>
"
74073113,Output tensors of a Functional model must be the output of a TensorFlow `Layer`,"<p>So I'm trying to expand the Roberta Pretrained Model and I was doing a basic model for testing but I'm getting this error from TensorFlow: <code>ValueError: Output tensors of a Functional model must be the output of a TensorFlow Layer.</code> which is from the Model api of Keras but I don't exactly know what's causing it.</p>
<p>Code:</p>
<pre><code>LEN_SEQ = 64
BATCH_SIZE = 16
TEST_TRAIN_SPLIT = 0.9
TRANSFORMER = 'roberta-base'

df = pd.read_csv('train-processed.csv')
df = df.head(100)
samples_count = len(df)

# Create labels
target = df['first_Sentiment'].values.astype(int)
labels = np.zeros((samples_count, target.max() + 1))
labels[np.arange(samples_count), target] = 1

tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER)
tokens = tokenizer(
    df['first_Phrase'].tolist(),
    max_length=LEN_SEQ,
    truncation=True,
    padding='max_length',
    add_special_tokens=True,
    return_tensors='tf'
)

base_model = TFAutoModel.from_pretrained(TRANSFORMER)

embedding = base_model.roberta(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])
embedding.trainable = False

# Define inputs
input_ids = Input(shape=(LEN_SEQ,), name='input_ids', dtype='int32')
input_mask = Input(shape=(LEN_SEQ,), name='input_mask', dtype='int32')

# Define hidden layers
layer = Dense(LEN_SEQ * 2, activation='relu')(embedding[1])
layer = Dense(LEN_SEQ, activation='relu')(layer)

# Define output
output = Dense(target.max() + 1, activation='softmax', name='output')(layer)

model = Model(inputs=[input_ids, input_mask], outputs=[output])
</code></pre>
<p>Full error traceback:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-80-9a6ccb1b4ca8&gt; in &lt;module&gt;
     10 output = Dense(target.max() + 1, activation='softmax', name='output')(layer)
     11 
---&gt; 12 model = Model(inputs=[input_ids, input_mask], outputs=[output])
     13 
     14 model.compile(

/usr/local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = False  # pylint: disable=protected-access
    529     try:
--&gt; 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.8/site-packages/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)
    107     generic_utils.validate_kwargs(kwargs, {})
    108     super(Functional, self).__init__(name=name, trainable=trainable)
--&gt; 109     self._init_graph_network(inputs, outputs)
    110 
    111   @tf.__internal__.tracking.no_automatic_dependency_tracking

/usr/local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = False  # pylint: disable=protected-access
    529     try:
--&gt; 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.8/site-packages/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)
    144         base_layer_utils.create_keras_history(self._nested_outputs)
    145 
--&gt; 146     self._validate_graph_inputs_and_outputs()
    147 
    148     # A Network does not create weights of its own, thus it is already

/usr/local/lib/python3.8/site-packages/keras/engine/functional.py in _validate_graph_inputs_and_outputs(self)
    719       if not hasattr(x, '_keras_history'):
    720         cls_name = self.__class__.__name__
--&gt; 721         raise ValueError('Output tensors of a ' + cls_name + ' model must be '
    722                          'the output of a TensorFlow `Layer` '
    723                          '(thus holding past layer metadata). Found: ' + str(x))

ValueError: Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: tf.Tensor(
[[0.18333092 0.18954797 0.22477032 0.21039596 0.1919548 ]
 [0.18219706 0.1903447  0.2256843  0.20587942 0.19589448]
 [0.18239683 0.1907878  0.22491893 0.20824413 0.19365236]
 [0.18193942 0.1898969  0.2259874  0.20646562 0.1957107 ]
 [0.18132213 0.1893883  0.22565623 0.21005587 0.1935775 ]
 [0.18237704 0.18789911 0.22692119 0.20759228 0.1952104 ]
 [0.18217668 0.18732095 0.22601548 0.21063867 0.19384828]
 [0.1817196  0.18970788 0.22175607 0.21405536 0.19276106]
 [0.18154216 0.18738106 0.22770867 0.2091297  0.19423842]
 [0.1839993  0.19110405 0.2193769  0.2124882  0.19303149]
 [0.18009834 0.19029345 0.22552258 0.21138497 0.19270067]
 [0.18262982 0.18932794 0.22548872 0.20995376 0.19259974]
 [0.18132062 0.1894746  0.22257458 0.21089785 0.19573237]
 [0.18127124 0.18927224 0.2275273  0.2061446  0.19578464]
 [0.18001163 0.1883382  0.22915907 0.20934238 0.19314879]
 [0.18409619 0.19204247 0.22269006 0.20967877 0.19149245]
 [0.18143429 0.18780865 0.22895294 0.21044146 0.1913626 ]
 [0.18210162 0.18980804 0.22135185 0.21205473 0.1946838 ]
 [0.18077913 0.18933856 0.22730026 0.2079047  0.19467732]
 [0.18248595 0.19133545 0.2252994  0.20402898 0.1968502 ]
 [0.18053354 0.18830904 0.22379933 0.21369977 0.19365832]
 [0.18100418 0.1889128  0.22656825 0.21134934 0.19216539]
 [0.18219638 0.18901002 0.22543809 0.20894748 0.194408  ]
 [0.17991781 0.18693839 0.23250549 0.21227528 0.18836297]
 [0.18322821 0.1881207  0.22497904 0.20976694 0.19390512]
 [0.17972894 0.18888594 0.2251662  0.21268585 0.19353302]
 [0.1822505  0.18769115 0.22729188 0.21127912 0.19148737]
 [0.18432644 0.18830952 0.22477935 0.20987424 0.19271052]
 [0.1801894  0.18920776 0.22684936 0.20734173 0.19641179]
 [0.181594   0.1880084  0.22798598 0.20937674 0.19303486]
 [0.18252885 0.19045824 0.22497422 0.207161   0.19487773]
 [0.18196142 0.18878765 0.22479571 0.2105628  0.19389246]
 [0.18600896 0.18686578 0.2283819  0.21188499 0.18685843]
 [0.18056509 0.18865508 0.22694935 0.21080662 0.19302382]
 [0.18446274 0.1887065  0.22405164 0.21271324 0.19006592]
 [0.1812612  0.18995184 0.22384171 0.20790772 0.19703752]
 [0.1861402  0.189157   0.2236694  0.21078445 0.19024895]
 [0.18149142 0.18862149 0.2255336  0.20888737 0.19546609]
 [0.18088317 0.1882689  0.22780944 0.20749897 0.19553955]
 [0.1824722  0.18926203 0.22691077 0.2071967  0.1941583 ]
 [0.18111941 0.18773855 0.22366299 0.21535842 0.19212064]
 [0.18248987 0.18920848 0.22602491 0.20733926 0.19493747]
 [0.18306294 0.19167435 0.22505572 0.21000686 0.19020009]
 [0.18466519 0.1885763  0.22352514 0.21257839 0.19065501]
 [0.18297954 0.18976018 0.2262897  0.20864752 0.19232307]
 [0.18216778 0.18953851 0.22490299 0.21057723 0.1928135 ]
 [0.18181367 0.19077264 0.2232015  0.21115994 0.1930523 ]
 [0.18345618 0.18753015 0.22660162 0.20830849 0.1941036 ]
 [0.18212378 0.18797131 0.2247642  0.21066691 0.19447377]
 [0.18199605 0.19106121 0.22245005 0.21217921 0.19231346]
 [0.18243583 0.18764758 0.22628336 0.21369886 0.18993443]
 [0.18162242 0.18957089 0.22591078 0.20930369 0.19359224]
 [0.18090473 0.18757755 0.22858356 0.20813066 0.19480348]
 [0.17951688 0.18841572 0.22520997 0.21235934 0.19449812]
 [0.1850496  0.18895829 0.22575855 0.20854111 0.1916925 ]
 [0.18254244 0.18938984 0.22754729 0.20879866 0.19172177]
 [0.1816532  0.18972425 0.22676478 0.20679341 0.19506434]
 [0.18303266 0.19159187 0.22373216 0.20538329 0.19625996]
 [0.18126963 0.18750906 0.2258774  0.21198079 0.1933631 ]
 [0.18387978 0.18828613 0.22228165 0.21189795 0.19365448]
 [0.1834729  0.18976368 0.22469373 0.20830937 0.19376035]
 [0.18359789 0.18833868 0.22379532 0.21078889 0.19347927]
 [0.18039297 0.18886234 0.22411437 0.2105467  0.19608359]
 [0.17980678 0.18979622 0.2266618  0.20471531 0.19901991]
 [0.18554561 0.19003332 0.22477089 0.21021138 0.18943883]
 [0.18349187 0.18941568 0.22224301 0.21004184 0.19480757]
 [0.18351436 0.19169463 0.22155108 0.21009424 0.19314568]
 [0.18123321 0.18985166 0.22660086 0.21186577 0.19044854]
 [0.18183744 0.192495   0.22091088 0.21275932 0.1919973 ]
 [0.18028514 0.18943599 0.22416686 0.21241388 0.19369814]
 [0.18061554 0.18873625 0.22677769 0.21073307 0.19313747]
 [0.18186866 0.18851075 0.22588421 0.21183755 0.19189876]
 [0.18126652 0.18949142 0.22501452 0.20897155 0.19525598]
 [0.1835434  0.19079022 0.22333461 0.21146008 0.19087164]
 [0.18269798 0.19171126 0.22150221 0.21224435 0.19184415]
 [0.17996274 0.19000672 0.22470033 0.2105299  0.19480029]
 [0.18345153 0.19032337 0.2239142  0.21167503 0.19063583]
 [0.18224017 0.19025423 0.22567508 0.2087501  0.19308044]
 [0.18233515 0.18966553 0.22833474 0.20635706 0.1933075 ]
 [0.18210347 0.18650064 0.22770585 0.21101129 0.19267873]
 [0.18199693 0.19086935 0.22255068 0.20988034 0.19470267]
 [0.18119748 0.18983872 0.22518982 0.20845842 0.19531558]
 [0.18367417 0.19071157 0.22310348 0.21277103 0.18973975]
 [0.17965038 0.18936628 0.22479466 0.21279414 0.19339451]
 [0.18141513 0.18989322 0.22380653 0.21031635 0.19456872]
 [0.18295668 0.19067182 0.22385122 0.20624346 0.1962768 ]
 [0.17981796 0.18981294 0.22544417 0.21043345 0.19449154]
 [0.18068986 0.1897383  0.22433658 0.21027999 0.1949553 ]
 [0.18146665 0.18844193 0.22996067 0.20703284 0.19309792]
 [0.18278767 0.18972701 0.22451803 0.20893572 0.19403161]
 [0.18077034 0.1892612  0.2236769  0.21081012 0.19548143]
 [0.18254872 0.19220418 0.22300169 0.20895892 0.19328652]
 [0.18032935 0.19029863 0.22319157 0.21000609 0.19617435]
 [0.18328631 0.18907256 0.22911799 0.20782094 0.19070214]
 [0.17863902 0.18771355 0.23066713 0.21065918 0.19232109]
 [0.18178153 0.19022569 0.22538401 0.20857622 0.1940325 ]
 [0.18072292 0.18907587 0.22616044 0.21096109 0.19307965]
 [0.18215105 0.18966101 0.22436853 0.21200544 0.191814  ]
 [0.18104836 0.18830387 0.22495148 0.21120267 0.19449359]
 [0.18192047 0.18981694 0.22512193 0.2107065  0.19243418]], shape=(100, 5), dtype=float32)
</code></pre>
<p>Data example:</p>
<p><a href=""https://i.stack.imgur.com/Y2jwG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y2jwG.png"" alt=""enter image description here"" /></a></p>
<p>Any help appreciated. I'm new to transformers so please feel free to point any extra considerations.</p>
"
74228640,Which HuggingFace summarization models support more than 1024 tokens? Which model is more suitable for programming related articles?,"<p>If this is not the best place to ask this question, please lead me to the most accurate one.</p>
<p>I am planning to use one of the Huggingface summarization models (<a href=""https://huggingface.co/models?pipeline_tag=summarization"" rel=""nofollow noreferrer"">https://huggingface.co/models?pipeline_tag=summarization</a>) to summarize my lecture video transcriptions.</p>
<p>So far I have tested <code>facebook/bart-large-cnn</code> and <code>sshleifer/distilbart-cnn-12-6</code>, but they only support a maximum of 1,024 tokens as input.</p>
<p>So, here are my questions:</p>
<ol>
<li><p>Are there any summarization models that support longer inputs such as 10,000 word articles?</p>
</li>
<li><p>What are the optimal output lengths for given input lengths? Let's say for a 1,000 word input, what is the optimal (minimum) output length (the min. length of the summarized text)?</p>
</li>
<li><p>Which model would likely work on programming related articles?</p>
</li>
</ol>
"
74290324,TypeError: unsupported operand type(s) for /: 'SequenceClassifierOutput' and 'int',"<p>I am using hugginface library to train a bert model on classification problem.</p>
<pre><code>    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10) 

    def training_step(self, batch, batch_nb):
       sequence, label = batch
       input_ids, attention_mask, labels = self.prepare_batch(sequence=sequence, label=label)
       loss = self.model(input_ids=input_ids,
                      attention_mask=attention_mask,
                      labels=labels) 
       tensorboard_logs = {'train_loss': loss}
</code></pre>
<p>I am getting the following error just before the training starts:</p>
<pre><code>in training_step
closure_loss = closure_loss / self.trainer.accumulate_grad_batches 
TypeError: unsupported operand type(s) for /: 'SequenceClassifierOutput' and 'int'
</code></pre>
<p>I am using pytorch-lightning</p>
"
74304875,OSError: There was a specific connection error when trying to load CompVis/stable-diffusion-v1-4: <class 'requests.exceptions.HTTPError'>,"<h3>System Info</h3>
<p>Google Colab, Free version, GPU</p>
<h3>Information</h3>
<ul>
<li>[ ] The official example scripts</li>
<li>[X] My own modified scripts</li>
</ul>
<h3>Tasks</h3>
<ul>
<li>[ ] An officially supported task in the <code>examples</code> folder (such as GLUE/SQuAD, ...)</li>
<li>[X] My own task or dataset (give details below)</li>
</ul>
<h3>Reproduction</h3>
<ol>
<li><a href=""https://github.com/woctezuma/stable-diffusion-colab"" rel=""nofollow noreferrer"">https://github.com/woctezuma/stable-diffusion-colab</a></li>
<li><a href=""https://colab.research.google.com/github/woctezuma/stable-diffusion-colab/blob/main/stable_diffusion.ipynb#scrollTo=GR4vF2bw-sHR"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/woctezuma/stable-diffusion-colab/blob/main/stable_diffusion.ipynb#scrollTo=GR4vF2bw-sHR</a></li>
<li>copy create to drive</li>
<li>run 1st cell</li>
<li>run 2nd cell</li>
<li>copy my token from  <a href=""https://huggingface.co/settings/tokens"" rel=""nofollow noreferrer"">https://huggingface.co/settings/tokens</a></li>
<li>paste it to the filed</li>
<li>press enter</li>
<li>#1st error - <a href=""https://discuss.huggingface.co/t/invalid-token-passed/22711"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/invalid-token-passed/22711</a></li>
<li><a href=""https://huggingface.co/settings/tokens"" rel=""nofollow noreferrer"">https://huggingface.co/settings/tokens</a> mange invalidate and refres</li>
<li>run 2nd cell again</li>
<li>copy and paste in new token</li>
</ol>
<pre><code>        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|

        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens .
        
Token: 
Login successful
Your token has been saved to /root/.huggingface/token
Authenticated through git-credential store but this isn't the helper defined on your machine.
You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default

git config --global credential.helper store
</code></pre>
<ol start=""13"">
<li>I have run <code>git config --global credential.helper store</code> than I could  rerun everything and move forward 2 cells</li>
<li>Cell CODE</li>
</ol>
<pre><code>import mediapy as media
import torch
from torch import autocast
from diffusers import StableDiffusionPipeline

model_id = &quot;CompVis/stable-diffusion-v1-4&quot;
device = &quot;cuda&quot;
remove_safety = False


pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16, revision=&quot;fp16&quot;, use_auth_token=True)
if remove_safety:
  pipe.safety_checker = lambda images, clip_input: (images, False)
pipe = pipe.to(device)
</code></pre>
<ol start=""15"">
<li>ERROR</li>
</ol>
<pre><code>[/usr/local/lib/python3.7/dist-packages/requests/models.py](https://localhost:8080/#) in raise_for_status(self)
    940         if http_error_msg:
--&gt; 941             raise HTTPError(http_error_msg, response=self)
    942 

HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/fp16/model_index.json

The above exception was the direct cause of the following exception:

HfHubHTTPError                            Traceback (most recent call last)
[/usr/local/lib/python3.7/dist-packages/diffusers/configuration_utils.py](https://localhost:8080/#) in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    233                     subfolder=subfolder,
--&gt; 234                     revision=revision,
    235                 )

[/usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py](https://localhost:8080/#) in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)
   1056                     proxies=proxies,
-&gt; 1057                     timeout=etag_timeout,
   1058                 )

[/usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py](https://localhost:8080/#) in get_hf_file_metadata(url, use_auth_token, proxies, timeout)
   1358     )
-&gt; 1359     hf_raise_for_status(r)
   1360 

[/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_errors.py](https://localhost:8080/#) in hf_raise_for_status(response, endpoint_name)
    253         # as well (request id and/or server error message)
--&gt; 254         raise HfHubHTTPError(str(HTTPError), response=response) from e
    255 

HfHubHTTPError: &lt;class 'requests.exceptions.HTTPError'&gt; (Request ID: esduBFUm9KJXSxYhFffq4)

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
[&lt;ipython-input-6-9b05f13f8bf3&gt;](https://localhost:8080/#) in &lt;module&gt;
      9 
     10 
---&gt; 11 pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16, revision=&quot;fp16&quot;, use_auth_token=True)
     12 if remove_safety:
     13   pipe.safety_checker = lambda images, clip_input: (images, False)

[/usr/local/lib/python3.7/dist-packages/diffusers/pipeline_utils.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    371                 local_files_only=local_files_only,
    372                 use_auth_token=use_auth_token,
--&gt; 373                 revision=revision,
    374             )
    375             # make sure we only download sub-folders and `diffusers` filenames

[/usr/local/lib/python3.7/dist-packages/diffusers/configuration_utils.py](https://localhost:8080/#) in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    254             except HTTPError as err:
    255                 raise EnvironmentError(
--&gt; 256                     &quot;There was a specific connection error when trying to load&quot;
    257                     f&quot; {pretrained_model_name_or_path}:\n{err}&quot;
    258                 )

OSError: There was a specific connection error when trying to load CompVis/stable-diffusion-v1-4:
&lt;class 'requests.exceptions.HTTPError'&gt; (Request ID: esduBFUm9KJXSxYhFffq4)
</code></pre>
<h3>Expected behavior</h3>
<p>Run all the cells and generating photo's as on the GitHub project shows
<a href=""https://github.com/woctezuma/stable-diffusion-colab"" rel=""nofollow noreferrer"">https://github.com/woctezuma/stable-diffusion-colab</a></p>
"
74437271,"Data collation step causing ""ValueError: Unable to create tensor..."" due to unnecessary padding attempts to extra inputs","<p>I am trying to fine-tune a Bart model from the huggingface transformers framework on a dialogue summarisation task. The Bart model by default takes in the conversations as a monolithic piece of text as the input and takes the summaries as the decoder input while training. I want to explicitly train the model on dialogue speaker and utterance information rather than waiting for the model to implicitly learn them. For this reason, I am extracting the position IDs of the speaker name tokens and their utterance tokens when I send them to the model along with the original input tokens and summary tokens and send them separately. However, the model's data collator/padding automation expects this information to also be the same size as the inputs (I need to disable this behaviour/change the way I am encoding the speaker to utterance mapping).</p>
<p>Please find the code and description for the above issue below:
I am using the SAMSum dataset for the dialogue summarisation task. The dataset looks like this</p>
<p>Conversation:</p>
<pre><code>Amanda: I baked  cookies. Do you want some?
Jerry: Sure!
Amanda: I'll bring you tomorrow :-)
</code></pre>
<p>Summary:</p>
<pre><code>Amanda baked cookies and will bring Jerry some tomorrow.
</code></pre>
<p>The conversation gets tokenized as:</p>
<pre><code>tokens = [0, 10127, 5219, 35, 38, 17241, 1437, 15269, 4, 1832, 47, 236, 103, 116, 50121, 50118, 39237, 35, 9136, 328, 50121, 50118, 10127, 5219, 35, 38, 581, 836, 47, 3859, 48433, 2]
</code></pre>
<p>The explicit speaker-utterance information is encoded as:</p>
<pre><code>[0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0]
</code></pre>
<p>Where 1s indicate that tokens[1:3] map to a name &quot;Amanda&quot; and the 2s indicate that tokens[3:16] map to an utterance &quot;: I baked  cookies. Do you want some?&quot;</p>
<p>I am trying to send this speaker utterance association information to the forward function in the hopes of adding a loss on the basis of this information. I intend to override the compute_loss method of the Trainer class from huggingface framework to edit the loss after I can successfully relay this explicit information.</p>
<p>I am currently trying the following:</p>
<pre><code>tokenized_dataset_train = train_datasets.map(preprocess_function, batched=True)

</code></pre>
<p>where the preprocess_function tokenizes and adds the speaker-utterance information in the form of a key-value pair. tokenized_dataset_train is of the form <code>{'input_ids':[...], 'attention_mask':[...], 'spk_utt_pos':[...], ...}</code></p>
<p>The preprocess function makes sure that the lengths for each of 'input_ids', 'attention_masks', and 'spk_utt_pos' is the same.</p>
<p>The data_collator from the <code>DataCollatorForSeq2Seq</code> pads 'input_ids' and 'attention_masks', but also tries to pad 'spk_utt_pos' which gives an error:</p>
<p><code>Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`spk_utt_pos` in this case) have excessive nesting (inputs type `list` where type `int` is expected).</code></p>
<p>Upon printing the sizes of 'input_ids', 'attention_masks', and 'spk_utt_pos' inside the train loop during the data collation step I found that the sizes of were not the same.
Example: (A 32 instance batch)</p>
<pre><code>'input_ids' sizes      357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357
'attention_mask' sizes 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357
'spk_utt_pos' sizes    285 276 276 321 58 93 77 69 198 266 55 107 85 235 47 280 209 357 86 186 27 52 80 77 85 231 266 237 322 125 251 126
</code></pre>
<p><strong>My question is: Is there something wrong with my approach to adding this explicit information to my model? What can be another method to send the speaker-utterance information to my model?</strong></p>
"
74491959,Convert all items in a list to string format,"<p>I am trying to seperate sentences (with spacy sentencizer) within a larger text format to process them in a transformers pipeline.</p>
<p>Unfortunately, this pipeline is not able to process the sentences correctly, since the sentences are not yet in string format after sentencizing the test. Please see the following information.</p>
<pre><code>    string = 'The Chromebook is exactly what it was advertised to be.  It is super simple to use. The picture quality is great, stays connected to WIfi with no interruption.  Quick, lightweight yet sturdy.  I bought the Kindle Fire HD 3G and had so much trouble with battery life, disconnection problems etc. that I hate it and so I bought the Chromebook and absolutely love it. The battery life is good. Finally a product that lives up to its hype!'

    #Added the sentencizer model to the classification package, so all the sentences in the summary texts of the reviews are being disconnected from each other
import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)

doc = nlp(string)

sentences = list(doc.sents)
sentences
</code></pre>
<p>This leads to the following list:</p>
<pre><code>[The Chromebook is exactly what it was advertised to be.,
It is super simple to use.,
The picture quality is great, stays connected to WIfi with no interruption.,
Quick, lightweight yet sturdy.,
I bought the Kindle Fire HD 3G and had so much trouble with battery life, disconnection problems etc.,
that I hate it,
and so I bought the Chromebook and absolutely love it.,
The battery life is good.,
Finally a product that lives up to its hype!]
</code></pre>
<p>When I provide this list to the following pipline, I get this error: ValueError:  <code>args[0]</code>: The Chromebook is exactly what it was advertised to be. have the wrong format. The should be either of type <code>str</code> or type <code>list</code></p>
<pre><code>    #Now in this line the list of reviews are being processed into triplets
from transformers import pipeline

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')

model_output = triplet_extractor(sentences, return_tensors=True, return_text=False)

extracted_text = triplet_extractor.tokenizer.batch_decode([x[&quot;generated_token_ids&quot;] for x in model_output])
print(&quot;\n&quot;.join(extracted_text))
</code></pre>
<p>Therefore, can someone please indicate how I can convert all the sentences in the 'sentences' list to string format?</p>
<p>Looking forward for the response. : )</p>
"
74497166,"Huggingface: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu","<p>I am confusing about my fine-tune model implemented by Huggingface model. I am able to train my model, but while I want to predict it, I always get this error. The most similar problem is <a href=""https://discuss.huggingface.co/t/runtimeerror-expected-all-tensors-to-be-on-the-same-device-but-found-at-least-two-devices-cpu-and-cuda-0-when-checking-arugment-for-argument-index-in-method-wrapper-index-select/9255"" rel=""nofollow noreferrer"">this</a>. My transformers version is 4.24.0, but it didn't seem to help me. I also try <a href=""https://stackoverflow.com/questions/70102323/runtimeerror-expected-all-tensors-to-be-on-the-same-device-but-found-at-least"">this</a>. Below is my code snippet.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer
from transformers import DataCollatorForSeq2Seq
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import pipeline
from tqdm import tqdm
from datasets import Dataset

import pandas as pd
import numpy as np
import pyarrow as pa
import gc
import torch as t
import pickle

PATH = './datas/Batch_answers - train_data (no-blank).csv'
EPOCH = 1
LEARNING_RATE = 2e-5
TRAIN_BATCH_SIZE = 16
EVAL_BATCH_SIZE = 16
DEVICE = 'cuda' if t.cuda.is_available() else 'cpu'

df = pd.read_csv(PATH)
df = df.drop(labels='s', axis=1)
df = df.iloc[:, 1:5]
df = df.to_numpy()
qData = []

for i in tqdm(range(len(df))):
    argument = df[i][0][1:-1]
    response = df[i][1][1:-1]
    qprime = df[i][2][1:-1]
    
    qData.append({'statement':argument+'\n'+response, 'argument_sentence_summary':qprime})
    
qtable = pa.Table.from_pylist(qData)
qDataset = Dataset(qtable)
qDataset = qDataset.train_test_split(train_size=0.8)

qModel = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-small&quot;)
qTokenizer = AutoTokenizer.from_pretrained(&quot;t5-small&quot;)
qData_collator = DataCollatorForSeq2Seq(tokenizer=qTokenizer, model=qModel)

def Qpreprocessing(data):
    model_input = qTokenizer(data['statement'], max_length=250, truncation=True)
    labels = qTokenizer(text_target=data['argument_sentence_summary'], max_length=75, truncation=True)

    model_input['labels'] = labels['input_ids']
    
    return model_input

qToken = qDataset.map(Qpreprocessing, batched=True)

qTraining_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./result&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=EVAL_BATCH_SIZE,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=EPOCH,
    fp16=True,
)

qTrainer = Seq2SeqTrainer(
    model=qModel,
    args=qTraining_args,
    train_dataset=qToken['train'],
    eval_dataset=qToken['test'],
    tokenizer=qTokenizer,
    data_collator=qData_collator
)

old_collator = qTrainer.data_collator
qTrainer.data_collator = lambda data: dict(old_collator(data))
qTrainer.train()

qp = pipeline('summarization', model=qModel, tokenizer=qTokenizer)
qp(qDataset['test'][0]['statement']) #break in this line
</code></pre>
<p>The full traceback:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
Cell In [20], line 3
      1 qp = pipeline('summarization', model=qModel, tokenizer=qTokenizer)
      2 # temp = t.tensor(qDataset['test'][0]['statement']).to(DEVICE)
----&gt; 3 qp(qDataset['train'][0]['statement'])

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:250, in SummarizationPipeline.__call__(self, *args, **kwargs)
    226 def __call__(self, *args, **kwargs):
    227     r&quot;&quot;&quot;
    228     Summarize the text(s) given as inputs.
    229 
   (...)
    248           ids of the summary.
    249     &quot;&quot;&quot;
--&gt; 250     return super().__call__(*args, **kwargs)

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:150, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs)
    121 def __call__(self, *args, **kwargs):
    122     r&quot;&quot;&quot;
    123     Generate the output text(s) using text(s) given as inputs.
    124 
   (...)
    147           ids of the generated text.
    148     &quot;&quot;&quot;
--&gt; 150     result = super().__call__(*args, **kwargs)
    151     if (
    152         isinstance(args[0], list)
    153         and all(isinstance(el, str) for el in args[0])
    154         and all(len(res) == 1 for res in result)
    155     ):
    156         return [res[0] for res in result]

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:1074, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1072     return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)
   1073 else:
-&gt; 1074     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:1081, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1079 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1080     model_inputs = self.preprocess(inputs, **preprocess_params)
-&gt; 1081     model_outputs = self.forward(model_inputs, **forward_params)
   1082     outputs = self.postprocess(model_outputs, **postprocess_params)
   1083     return outputs

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:990, in Pipeline.forward(self, model_inputs, **forward_params)
    988     with inference_context():
    989         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
--&gt; 990         model_outputs = self._forward(model_inputs, **forward_params)
    991         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(&quot;cpu&quot;))
    992 else:

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:172, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)
    170 generate_kwargs[&quot;max_length&quot;] = generate_kwargs.get(&quot;max_length&quot;, self.model.config.max_length)
    171 self.check_inputs(input_length, generate_kwargs[&quot;min_length&quot;], generate_kwargs[&quot;max_length&quot;])
--&gt; 172 output_ids = self.model.generate(**model_inputs, **generate_kwargs)
    173 out_b = output_ids.shape[0]
    174 if self.framework == &quot;pt&quot;:

File ~\anaconda3\envs\ame\lib\site-packages\torch\autograd\grad_mode.py:27, in _DecoratorContextManager.__call__.&lt;locals&gt;.decorate_context(*args, **kwargs)
     24 @functools.wraps(func)
     25 def decorate_context(*args, **kwargs):
     26     with self.clone():
---&gt; 27         return func(*args, **kwargs)

File ~\anaconda3\envs\ame\lib\site-packages\transformers\generation_utils.py:1339, in GenerationMixin.generate(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)
   1331         logger.warning(
   1332             &quot;A decoder-only architecture is being used, but right-padding was detected! For correct &quot;
   1333             &quot;generation results, please set `padding_side='left'` when initializing the tokenizer.&quot;
   1334         )
   1336 if self.config.is_encoder_decoder and &quot;encoder_outputs&quot; not in model_kwargs:
   1337     # if model is encoder decoder encoder_outputs are created
   1338     # and added to `model_kwargs`
-&gt; 1339     model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
   1340         inputs_tensor, model_kwargs, model_input_name
   1341     )
   1343 # 4. Prepare `input_ids` which will be used for auto-regressive generation
   1344 if self.config.is_encoder_decoder:

File ~\anaconda3\envs\ame\lib\site-packages\transformers\generation_utils.py:583, in GenerationMixin._prepare_encoder_decoder_kwargs_for_generation(self, inputs_tensor, model_kwargs, model_input_name)
    581 encoder_kwargs[&quot;return_dict&quot;] = True
    582 encoder_kwargs[model_input_name] = inputs_tensor
--&gt; 583 model_kwargs[&quot;encoder_outputs&quot;]: ModelOutput = encoder(**encoder_kwargs)
    585 return model_kwargs

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\module.py:1130, in Module._call_impl(self, *input, **kwargs)
   1126 # If we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130     return forward_call(*input, **kwargs)
   1131 # Do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

File ~\anaconda3\envs\ame\lib\site-packages\transformers\models\t5\modeling_t5.py:941, in T5Stack.forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    939 if inputs_embeds is None:
    940     assert self.embed_tokens is not None, &quot;You have to initialize the model with valid token embeddings&quot;
--&gt; 941     inputs_embeds = self.embed_tokens(input_ids)
    943 batch_size, seq_length = input_shape
    945 # required mask seq length can be calculated via length of past

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\module.py:1130, in Module._call_impl(self, *input, **kwargs)
   1126 # If we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130     return forward_call(*input, **kwargs)
   1131 # Do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\sparse.py:158, in Embedding.forward(self, input)
    157 def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 158     return F.embedding(
    159         input, self.weight, self.padding_idx, self.max_norm,
    160         self.norm_type, self.scale_grad_by_freq, self.sparse)

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\functional.py:2199, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2193     # Note [embedding_renorm set_grad_enabled]
   2194     # XXX: equivalent to
   2195     # with torch.no_grad():
   2196     #   torch.embedding_renorm_
   2197     # remove once script supports set_grad_enabled
   2198     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2199 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)
</code></pre>
<p>Should that mean I need another way to predict my test dataset instead of using pipeline? Big thanks for help.</p>
"
74631411,Why tflite model output shape is different than the original model converted from T5ForConditionalGeneration?,"<p><strong>T5ForConditionalGeneration Model to translate English to German</strong></p>
<pre><code>from transformers import T5TokenizerFast, T5ForConditionalGeneration

tokenizer = T5TokenizerFast.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

input_ids = tokenizer(&quot;translate English to German: the flowers are wonderful.&quot;, return_tensors=&quot;pt&quot;).input_ids
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<p>Output : Die Blumen sind wunderbar.</p>
<p><strong>Input Shape</strong></p>
<pre><code>input_ids.shape
</code></pre>
<p>Output : torch.Size([1, 11])</p>
<p><strong>Output Shape</strong></p>
<pre><code>outputs.shape
</code></pre>
<p>Output : torch.Size([1, 7])</p>
<p><strong>Save Pretrained model</strong></p>
<pre><code>!mkdir /content/test
model.save_pretrained('/content/test')
</code></pre>
<p><strong>Load TFT5Model model from pretrained</strong></p>
<pre><code>from transformers import TFT5Model
t5model = TFT5Model.from_pretrained('/content/test',from_pt=True)
!mkdir /content/test/t5
t5model.save('/content/test/t5')
</code></pre>
<p><strong>Convert TFT5Model to TFlite</strong></p>
<pre><code>import tensorflow as tf

saved_model_dir = '/content/test/t5'
!mkdir  /content/test/tflite
tflite_model_path = '/content/test/tflite/model.tflite'

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.experimental_new_converter = True
converter.experimental_new_quantizer = True
converter.experimental_new_dynamic_range_quantizer = True
converter.allow_custom_ops=True

converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
#print(tflite_model)
print(type(tflite_model))


# Save the model
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)
</code></pre>
<p><strong>Load The TFLite model</strong></p>
<pre><code>import numpy as np
import tensorflow as tf

tflite_model_path = '/content/test/tflite/model.tflite'
# Load the TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)

interpreter.resize_tensor_input(0, [1,5], strict=True)
interpreter.resize_tensor_input(1, [1,5], strict=True)
interpreter.resize_tensor_input(2, [1,5], strict=True)
interpreter.resize_tensor_input(3, [1,5], strict=True)
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

#print the output
input_data = np.array(np.random.random_sample((input_shape)), dtype=np.int64)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
</code></pre>
<p><strong>Get The Output Shape</strong></p>
<pre><code>print(output_data.shape)
</code></pre>
<p>Output : (1, 8, 5, 64)</p>
<p>Expected something like : (1, 7)</p>
<p>Can someone let me know where am I going wrong ?</p>
<p>The output shape of the tflite model is completely different from the T5ForConditionalGeneration model</p>
<p>Output : (1, 8, 5, 64)</p>
<p>Expected something like : (1, 7)</p>
"
74664286,Converting a dataset to CoNLL format. Label remaining tokens with O,"<p>I have a manually annotated dataset that contains records in the following format:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;id&quot;: 1,
    &quot;text&quot;: &quot;At the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the Third Amendment and until December 30, 1996, the Company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0.&quot;,
    &quot;label&quot;: [
        [
            209,
            230,
            &quot;COV_3&quot;
        ],
        [
            379,
            390,
            &quot;VAL_3&quot;
        ]
    ],
}
</code></pre>
<p>In the example above, <code>&quot;label&quot;</code> represents the custom entities I have in my dataset. In the example shown above, the phrase <code>fixed charge coverage</code> is located at position <code>[309, 336]</code> and is given the label <code>COV_3</code>. Likewise, the phrase <code>1.25 to 1.0</code> is located at <code>[379, 390]</code> and is given the label <code>VAL_3</code>.</p>
<p>Now, I would like to fine-tune some transformer model like BERT on this dataset, however, I realised that the dataset must be in CoNLL format. Or at least, all the tokens of each datapoint must be labelled. Is there any way I can easily label the remaining tokens with label <code>&quot;O&quot;</code> or I can transform this dataset in the CoNLL format?</p>
"
74671279,Train T5/BART to convert a string into multiple strings,"<p>Is it possible to train a seq2seq model like T5 or BART to convert a string into a list of strings? On my first attempt, the tokenizer complained that my 2D list of labels isnâ€™t the correct data type:</p>
<pre><code>File &quot;/home/matt/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py&quot;, line 429, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]
</code></pre>
<p>I suppose I could concatenate the multiple strings in each of my training examples, but then Iâ€™d have to use a potentially error-prone splitter to split them up again. Maybe using a special character as a delimiter is the answer here?</p>
<p>It's not super relevant, but here's how I'm invoking the tokenizer. Also, I'm using a subclass of <code>torch.utils.data.Dataset</code>:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(args.model_name)
encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')
decodings = tokenizer(labels, truncation=True, padding=True, return_tensors='pt')
dataset_tokenized = Dataset(encodings, decodings)
</code></pre>
<p>What is relevant is that my <code>texts</code> variable is a list of strings, and my <code>labels</code> variable is a 2D list of strings, which obviously isn't allowed.</p>
"
74408892,Ray Tune scheduler hyperparam_mutations vs. param_space,"<p>I am having a hard time understanding the need for what seems like two search space definitions in the same program flow. The tune.Tuner() object takes in a param_space argument, where we can set up the hyperparameter space to look into, however, it can also take in a scheduler.</p>
<p>As an example, I have a HuggingFace transformer setup with a Population Based Training scheduler, with its own hyperparam_mutations, which looks like another hyperparameter space to look into.</p>
<ol>
<li>What is the interaction between these two spaces?</li>
<li>If I just want to perturb learning_rate to see its effect on my accuracy, would I put this into the tuner's param_space or into the scheduler's hyperparam_mutations?</li>
</ol>
<pre><code>import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import PopulationBasedTraining

num_tune_trials = 3
batch_size = 2
num_labels = 2
model_ckpt = 'imaginary_ckpt'
odel_name = f&quot;{model_ckpt}-finetuned&quot;
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)

def training_args():
    return TrainingArguments(output_dir=model_name,
                                        num_train_epochs=4,
                                        learning_rate=2e-5,
                                        per_device_train_batch_size=batch_size,
                                        per_device_eval_batch_size=batch_size,
                                        weight_decay=0.01,
                                        evaluation_strategy=&quot;epoch&quot;,
                                        push_to_hub=False,
                                        log_level=&quot;error&quot;)

def trainer_hyperparam():
    return Trainer(model=model, args=training_args,
                compute_metrics=compute_metrics,
                train_dataset=data_encoded[&quot;train&quot;],
                eval_dataset=data_encoded[&quot;validation&quot;],
                model_init=model_init,
                tokenizer=tokenizer)

trainer = trainer_hyperparam()

tune_config = {
    &quot;per_device_train_batch_size&quot;: batch_size,
    &quot;per_device_eval_batch_size&quot;: batch_size,
}

scheduler = PopulationBasedTraining(
    time_attr=&quot;training_iteration&quot;,
    metric=&quot;eval_accuracy&quot;,
    mode=&quot;max&quot;,
    perturbation_interval=1,
    hyperparam_mutations={
        &quot;weight_decay&quot;: tune.uniform(0.005, 0.02),
        &quot;learning_rate&quot;: tune.uniform(1e-3, 1e-6),
        &quot;per_device_train_batch_size&quot;: [4,5,6,7,8,9],
    },
)

reporter = CLIReporter(
    parameter_columns={
        &quot;weight_decay&quot;: &quot;w_decay&quot;,
        &quot;learning_rate&quot;: &quot;lr&quot;,
        &quot;per_device_train_batch_size&quot;: &quot;train_bs/gpu&quot;,
    },
    metric_columns=[&quot;eval_accuracy&quot;, &quot;eval_loss&quot;, &quot;epoch&quot;, &quot;training_iteration&quot;],
)

trainer.hyperparameter_search(
    hp_space=lambda _: tune_config,
    backend=&quot;ray&quot;,
    n_trials=num_tune_trials,
    resources_per_trial={&quot;cpu&quot;: 4, &quot;gpu&quot;: 1},
    scheduler=scheduler,
    keep_checkpoints_num=1,
    checkpoint_score_attr=&quot;training_iteration&quot;,
    stop=None,
    progress_reporter=reporter,
    local_dir=&quot;~/ray_results/&quot;,
    name=&quot;tune_transformer_pbt&quot;,
)
</code></pre>
"
74360282,BOS token for encoder decoder models,"<p>Iâ€™m using T5-base for my model, and it seems to be generating something reasonable when I do <code>model.generate</code>. But my question is how?</p>
<p>The decoder part of this model needs a starting token to start decoding doesnâ€™t it? How does it figure out what the very first token is supposed to look like?</p>
<p>Or am I doing the training wrong where I should have included a token?</p>
<p>If needed <a href=""https://www.kaggle.com/code/sachin/t5-for-grammar-correction"" rel=""nofollow noreferrer"">here is a code sample</a> where I used <code>model.generate</code>.</p>
<h2>Edit 1</h2>
<p>Apart from the answer below, I found that there was a <code>model.config.decoder_start_token_id</code>. This is not necessarily <code>&lt;bos&gt;</code>. In the case of <code>T5/Flan-T5</code> it ended up being <code>&lt;pad&gt;</code>.</p>
"
74358056,Tensor to Dataframe for each sentence,"<p>For a 6 class sentence classification task, I have a list of sentences where I retrieve the absolute values before the softmax is applied. Example list of sentences:</p>
<pre><code>s = ['I like the weather today', 'The movie was very scary', 'Love is in the air']
</code></pre>
<p>I get the values the following way:</p>
<pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_name = &quot;Emanuel/bertweet-emotion-base&quot;
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

for i in s:
  sentence = tokenizer(i, return_tensors=&quot;pt&quot;)
  output = model(sentence[&quot;input_ids&quot;])
  print(output.logits.detach().numpy())

  # returns [[-0.8390876   2.9480567  -0.5134539   0.70386493 -0.5019671  -2.619496  ]]
  #[[-0.8847909  -0.9642067  -2.2108874  -0.43932158  4.3386173  -0.37383893]]
  #[[-0.48750368  3.2949197   2.1660519  -0.6453249  -1.7101991  -2.817954  ]]
</code></pre>
<p>How do I create a data frame with columns <code>sentence, class_1, class_2, class_3, class_4, class_5, class_6</code> where I add values iteratively or maybe in a more optimal way where I append each new sentence and its absolute values? What would be the best way?</p>
<p>Expected output:</p>
<pre><code>     sentence                   class_1        class_2    class_3      ....
0    I like the weather today   -0.8390876     2.9480567  -0.5134539   ....
1    The movie was very scary   -0.8847909     -0.9642067 -2.2108874   ....
2    Love is in the air         -0.48750368    3.2949197   2.1660519   ....  
...
</code></pre>
<p>If I only had one sentence, I could transform it to a data frame like this, but I would still need to append the sentence somehow</p>
<pre><code>sentence = tokenizer(&quot;Love is in the air&quot;, return_tensors=&quot;pt&quot;)
output = model(sentence[&quot;input_ids&quot;])

px = pd.DataFrame(output.logits.detach().numpy())
</code></pre>
<p>Maybe creating two separate data frames and then appending them would be one plausible way of doing this?</p>
"
74138756,use_cuda is set True even though it was specified as False T5,"<p>I am trying to train a T5 model using <code>simpletransformers</code>. Here is my code:</p>
<pre><code>from simpletransformers.t5 import T5Model
model_args = {
    &quot;max_seq_length&quot;: MAX_LEN,
    &quot;train_batch_size&quot;: 8,
    &quot;eval_batch_size&quot;: 8,
    &quot;num_train_epochs&quot;: 1,
    &quot;evaluate_during_training&quot;: True,
    &quot;evaluate_during_training_steps&quot;: 15000,
    &quot;evaluate_during_training_verbose&quot;: True,
    
    &quot;learning_rate&quot;: 1e-4,
    
    &quot;evaluate_generated_text&quot;: True,
    
    &quot;use_multiprocessing&quot;: False,
    &quot;fp16&quot;: False,
    &quot;use_cuda&quot;:False,
    
    &quot;save_steps&quot;: -1,
    &quot;save_eval_checkpoints&quot;: False,
    &quot;save_model_every_epoch&quot;: False,
    &quot;reprocess_input_data&quot;: True,
    &quot;overwrite_output_dir&quot;: True,
    &quot;wandb_project&quot;: None
}

model = T5Model('t5', 't5-base', args=model_args)
</code></pre>
<p>But I am getting this error:</p>
<pre><code>ValueError: 'use_cuda' set to True when cuda is unavailable.Make sure CUDA is available or set `use_cuda=False`.
</code></pre>
<p>I have specified both <code>use_cuda=False</code> and <code>fp16 =False</code>, not sure why I am getting this error. I am running my code on Jupyter and I tried restarting the kernel and re-running the code but with no hope.</p>
"
74021237,"If I train a custom tokenizer on my dataset, I would still be able to leverage a pre-trained model weight","<p>This is a declaration, but I'm not sure it is correct. I can elaborate.</p>
<p>I have a considerably large dataset (23Gb). I'd like to pre-train the Roberta-base or XLM-Roberta-base, so my language model would fit better to be used in further downstream tasks.</p>
<p>I know I can just run it against my dataset for a few epochs and get good results. But, what if I also train the tokenizer to generate a new vocab, and merge files? The weights from the pre-trained model I started from will still be used, or the new set of tokens will demand complete training from scratch?</p>
<p>I'm asking this because maybe some layers can still contribute with knowledge, so the final model will have the better of both worlds: A tokenizer that fits my dataset, and the weights from previous training.</p>
<p>That makes sense?</p>
"
73851375,Use sentence transformers models with Apache Beam,"<p>I have an apache beam pipeline that <strong>works flawlessly</strong> with a <code>DirectRunner</code>, <strong>but not</strong> with a <code>DataflowRunner</code> :</p>
<p>When using a DataflowRunner I get a <code>&quot;Error 413 (Request entity too large)&quot;</code>
From what I understand, it is because the pipeline file is too large. (I get it with the following option : <code>--dataflow_job_file=gs://...</code>
And this is caused by the model I use :</p>
<pre><code>embeding_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')
</code></pre>
<p>Have anyone already experimented something similar ?</p>
"
73704193,HuggingFace Summarization: effect of specifying both `do_sample` and `num_beams`,"<p>I am using a HuggingFace summarization pipeline to generate summaries using a fine-tuned model. The <code>summarizer</code> object is initialised as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

summarizer = pipeline(
    &quot;summarization&quot;, 
    model=model, 
    tokenizer=tokenizer, 
    num_beams=5, 
    do_sample=True, 
    no_repeat_ngram_size=3,
    max_length=1024,
    device=0,
    batch_size=8
)
</code></pre>
<p>According to the documentation, setting <code>num_beams=5</code> means that the top 5 choices are retained when a new token in the sequence is generated based on a language model, and the model moves forward discarding all other possibilities, and repeating this after every new token is generated. However, this option seems to be apparently incompatible with <code>do_sample=True</code> which seems to activate a behaviour where new tokens are picked based on some random strategy (which doesn't have to be uniformly random of course, but I don't know the details of this process). Could anyone explain clearly how  <code>num_beams=5</code> and <code>do_sample=True</code> would work together (no error is raised so I assume this is a valid <code>summarizer</code> configuration)?</p>
"
73645084,Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way,"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.
Is there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs/GPUs.</p>
<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.</p>
<p>Any insights into this?</p>
"
73610869,The expanded size of the tensor (1011) must match the existing size (512) at non-singleton dimension 1,"<p>I have a trained a LayoutLMv2 model from huggingface and when I try to inference it on a single image, it gives the runtime error. The code for this is below:</p>
<pre><code>query = '/Users/vaihabsaxena/Desktop/Newfolder/labeled/Others/Two.pdf26.png'
image = Image.open(query).convert(&quot;RGB&quot;)
encoded_inputs = processor(image, return_tensors=&quot;pt&quot;).to(device)
outputs = model(**encoded_inputs)
preds = torch.softmax(outputs.logits, dim=1).tolist()[0]
pred_labels = {label:pred for label, pred in zip(label2idx.keys(), preds)}
pred_labels
</code></pre>
<p>The error comes when when I do <code>model(**encoded_inputs)</code>. The <code>processor</code> is called directory from Huggingface and is initialized as follows along with other APIs:</p>
<pre><code>feature_extractor = LayoutLMv2FeatureExtractor()
tokenizer = LayoutLMv2Tokenizer.from_pretrained(&quot;microsoft/layoutlmv2-base-uncased&quot;)
processor = LayoutLMv2Processor(feature_extractor, tokenizer)
</code></pre>
<p>The model is defined and trained as follows:</p>
<pre><code>model = LayoutLMv2ForSequenceClassification.from_pretrained(
    &quot;microsoft/layoutlmv2-base-uncased&quot;,  num_labels=len(label2idx)
)
model.to(device);


optimizer = AdamW(model.parameters(), lr=5e-5)
num_epochs = 3


for epoch in range(num_epochs):
    print(&quot;Epoch:&quot;, epoch)
    training_loss = 0.0
    training_correct = 0
    #put the model in training mode
    model.train()
    for batch in tqdm(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss

        training_loss += loss.item()
        predictions = outputs.logits.argmax(-1)
        training_correct += (predictions == batch['labels']).float().sum()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    print(&quot;Training Loss:&quot;, training_loss / batch[&quot;input_ids&quot;].shape[0])
    training_accuracy = 100 * training_correct / len(train_data)
    print(&quot;Training accuracy:&quot;, training_accuracy.item())  
        
    validation_loss = 0.0
    validation_correct = 0
    for batch in tqdm(valid_dataloader):
        outputs = model(**batch)
        loss = outputs.loss

        validation_loss += loss.item()
        predictions = outputs.logits.argmax(-1)
        validation_correct += (predictions == batch['labels']).float().sum()

    print(&quot;Validation Loss:&quot;, validation_loss / batch[&quot;input_ids&quot;].shape[0])
    validation_accuracy = 100 * validation_correct / len(valid_data)
    print(&quot;Validation accuracy:&quot;, validation_accuracy.item())
</code></pre>
<p>The complete error trace:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
/Users/vaihabsaxena/Desktop/Newfolder/pytorch.ipynb Cell 37 in &lt;cell line: 4&gt;()
      2 image = Image.open(query).convert(&quot;RGB&quot;)
      3 encoded_inputs = processor(image, return_tensors=&quot;pt&quot;).to(device)
----&gt; 4 outputs = model(**encoded_inputs)
      5 preds = torch.softmax(outputs.logits, dim=1).tolist()[0]
      6 pred_labels = {label:pred for label, pred in zip(label2idx.keys(), preds)}

File ~/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)
   1126 # If we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130     return forward_call(*input, **kwargs)
   1131 # Do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

File ~/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:1071, in LayoutLMv2ForSequenceClassification.forward(self, input_ids, bbox, image, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1061 visual_position_ids = torch.arange(0, visual_shape[1], dtype=torch.long, device=device).repeat(
   1062     input_shape[0], 1
   1063 )
   1065 initial_image_embeddings = self.layoutlmv2._calc_img_embeddings(
   1066     image=image,
   1067     bbox=visual_bbox,
...
    896     input_shape[0], 1
    897 )
    898 final_position_ids = torch.cat([position_ids, visual_position_ids], dim=1)

RuntimeError: The expanded size of the tensor (1011) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 1011].  Tensor sizes: [1, 512]
</code></pre>
<p>I have tried to set up the tokenizer to cut off the max length but it finds <code>encoded_inputs</code> as Nonetype however the image is still there. What is going wrong here?</p>
"
73567055,Extend BERT or any transformer model using manual features,"<p>I have been doing a thesis in my citation classifications. I just implemented Bert model for the classification of citations. I have 4 output classes and I give an input sentence and my model returns an output that tells the category of citation. Now my supervisor gave me another task.</p>
<p>You have to search that whether it is possible to extend BERT or any transformer model using manual features. e.g. You are currently giving a sentence as the only input followed by its class. What if you can give a sentence, and some other features as input; as we do in other classifiers?</p>
<p>I need some guidance about this problem. How can I add an extra feature in my Bert model and the feature would be categorical not numerical.</p>
"
73566299,xlm-roberta tokenizer sticks all words together,"<p>I am trying to use a xlm-roberta model I have fine-tuned for token classification, but no matter what I do, I always get as an output all tokens stuck together, like:</p>
<pre><code>[{'entity_group': 'LABEL_0',
'score': 0.4824247,
'word': 'Thedogandthecatwenttothehouse',
'start': 0,
'end': 325}]
</code></pre>
<p>What could I do to get the words properly separated as an output as it happens with other models, like Bert?</p>
<p>I have tried to conduct the training with <code>add_prefix_space=True</code> but it does not seem to have any effect:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('MMG/xlm-roberta-large-ner-spanish', add_prefix_space=True)
model = AutoModelForTokenClassification.from_pretrained(&quot;xlm-roberta-large-finetuned-conll03-english&quot;, use_cache=None, num_labels=NUM_LABELS, ignore_mismatched_sizes=True)
pipe = pipeline(task=&quot;token-classification&quot;, model=model.to(&quot;cpu&quot;), binary_output=True, tokenizer=tokenizer, aggregation_strategy=&quot;average&quot;)
</code></pre>
<p>Thanks a lot in advance for your help.</p>
"
73518635,Can't use BloomAI locally,"<p>So I just finished installing Bloom's model from Huggingface &amp; I tried to run it in my notebook.</p>
<p>Here's the code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
model_path = &quot;D:/bloom&quot;
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModel.from_pretrained(model_path)
</code></pre>
<p>and I get this error:</p>
<pre><code>DefaultCPUAllocator: not enough memory: you tried to allocate xxx bytes
</code></pre>
<p>So then I went on to google searching and found this article:</p>
<p><a href=""https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"" rel=""nofollow noreferrer"">https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a></p>
<p>However, I get this error:</p>
<pre><code>TypeError: build_alibi_tensor() missing 1 required positional argument: 'device'
</code></pre>
<p>How to run Bloom locally?</p>
"
73506939,Attempt to convert a value (<PIL.BmpImagePlugin.BmpImageFile image mode=L size=190x100 at 0x7F35C52AD210>) with an unsupported type to a Tensor,"<p>I tried to execute the ViT model from <a href=""https://github.com/philschmid/keras-vision-transformer-huggingface/blob/master/image-classification.ipynb"" rel=""nofollow noreferrer"">Image Classification with Hugging Face Transformers and <code>Keras</code></a>, I got an error, particularly in this instruction:</p>
<pre><code>processed_dataset = ds.map(augmentation, batched=True)
</code></pre>
<p>the error :</p>
<blockquote>
<p>ValueError: Exception encountered when calling layer &quot;resizing_8&quot;
(type Resizing).</p>
<p>Attempt to convert a value (&lt;PIL.BmpImagePlugin.BmpImageFile image
mode=L size=190x100 at 0x7F35C52AD210&gt;) with an unsupported type
(&lt;class 'PIL.BmpImagePlugin.BmpImageFile'&gt;) to a Tensor.</p>
<p>Call arguments received:   â€¢ inputs=&lt;PIL.BmpImagePlugin.BmpImageFile
image mode=L size=190x100 at 0x7F35C52AD210&gt;</p>
</blockquote>
<p>I tried the answer in this link <a href=""https://stackoverflow.com/questions/71147064/arrowtypeerror-could-not-convert-pil-pngimageplugin-pngimagefile-image-mode-rg"">ArrowTypeError: Could not convert &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7F2223B6ED10&gt;</a>, where I added <code>'img': Image(decode=True, id=None)</code> to my features in create_image_folder_dataset() and I still have the same problem except for a small change in this part</p>
<blockquote>
<p>ValueError: Exception encountered when calling layer &quot;resizing_13&quot;
(type Resizing).</p>
</blockquote>
<p><strong>What I should do to solve this problem?</strong></p>
<p>create_image_folder_dataset function:</p>
<pre><code>def create_image_folder_dataset(root_path):
  &quot;&quot;&quot;creates `Dataset` from image folder structure&quot;&quot;&quot;
  
  # get class names by folders names
  _CLASS_NAMES= os.listdir(root_path)
  # defines `datasets` features`
  features=datasets.Features({
                      &quot;img&quot;: datasets.Image(decode=True, id=None),
                      #&quot;img&quot;: datasets.Image(),
                      &quot;label&quot;: datasets.features.ClassLabel(names=_CLASS_NAMES),
                      
                  })
  #print(_CLASS_NAMES)
  # temp list holding datapoints for creation
  img_data_files=[]
  label_data_files=[]
  # load images into list for creation
  for img_class in os.listdir(root_path):
    for img in os.listdir(os.path.join(root_path,img_class)):
      path_=os.path.join(root_path,img_class,img)
      img_data_files.append(path_)
      label_data_files.append(img_class)
  # create dataset
  ds = datasets.Dataset.from_dict({&quot;img&quot;:img_data_files,&quot;label&quot;:label_data_files},features=features)
  return ds
ds = create_image_folder_dataset(&quot;/content/drive/MyDrive/FINAL_DATASET&quot;)
ds[0] &quot;&quot;&quot; return: 
{'img': &lt;PIL.BmpImagePlugin.BmpImageFile image mode=L size=190x100 at 0x7F35C54ECC10&gt;,
 'label': 0}&quot;&quot;&quot;
</code></pre>
<p>my Augmentation function :</p>
<pre><code>from transformers import ViTFeatureExtractor
from tensorflow import keras 
from tensorflow.keras import layers


model_id = &quot;google/vit-base-patch16-224-in21k&quot;
#google/vit-base-patch32-384
feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)

# learn more about data augmentation here: https://www.tensorflow.org/tutorials/images/data_augmentation
data_augmentation = keras.Sequential(
    [
        layers.Resizing(feature_extractor.size, feature_extractor.size),
        layers.Rescaling(1./255),
        layers.RandomFlip(&quot;horizontal&quot;),
        layers.RandomRotation(factor=0.02),
        layers.RandomZoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name=&quot;data_augmentation&quot;,
)
# use keras image data augementation processing
def augmentation(examples):
    print(examples[&quot;img&quot;])
    examples[&quot;pixel_values&quot;] = [data_augmentation(image) for image in examples[&quot;img&quot;]]
    return examples


# basic processing (only resizing)
def process(examples):
    examples.update(feature_extractor(examples['img'], ))
    return examples


# we are also renaming our label col to labels to use `.to_tf_dataset` later
#ds = ds.rename_column(&quot;label&quot;, &quot;labels&quot;)
</code></pre>
"
74785188,Pytorch complaining about input and label batch size mismatch,"<p>I am using Huggingface to implement a BERT model using <code>BertForSequenceClassification.from_pretrained()</code>.</p>
<p>The model is trying to predict 1 of 24 classes. I am using a batch size of 32 and a sequence length of 66.</p>
<p>When I try to call the model in training, I get the following error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Expected input batch_size (32) to match target batch_size (768).
</code></pre>
<p>However, my target shape is 32x24. It seems like somewhere when the model is called, this is being flattened to 768x1. Here is a test I ran to check:</p>
<pre class=""lang-py prettyprint-override""><code>for i in train_dataloader:
    i = tuple(t.to(device) for t in i)
    print(i[0].shape, i[1].shape, i[2].shape) # here i[2].shape is (32, 24)
    output = model(i[0], attention_mask=i[1], labels=i[2]) # here PyTorch complains that i[2]'s shape is now (768, 1)
    print(output.logits.shape)
    break
</code></pre>
<p>This outputs:</p>
<pre class=""lang-py prettyprint-override""><code>torch.Size([32, 66]) torch.Size([32, 66]) torch.Size([32, 24])
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-68-c69db6168cc3&gt; in &lt;module&gt;
      2     i = tuple(t.to(device) for t in i)
      3     print(i[0].shape, i[1].shape, i[2].shape)
----&gt; 4     output = model(i[0], attention_mask=i[1], labels=i[2])
      5     print(output.logits.shape)
      6     break

4 frames
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   3024     if size_average is not None or reduce is not None:
   3025         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3026     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
   3027 
   3028 

ValueError: Expected input batch_size (32) to match target batch_size (768).
</code></pre>
"
74856703,Fine-tuning distilbert takes hours,"<p>I am fine tuning the distilbert pretrained model for sentiment analysis (multilabel with 6 labels) using Huggingface emotion dataset. I am new to this, but 1 epoch, 250 steps takes around 2 hours to train on Google Colab notebook, is this normal? The train dataset has 16.000 twitter text data which of course affects the performance but isn't this too long? What is the reason behind this?</p>
<p>Also after 3 epochs, the accuracy started to drop. What could be the reason for this?</p>
"
74885225,Cast features to ClassLabel,"<p>I have a dataset with type dictionary which I converted to <code>Dataset</code>:</p>
<p>ds = datasets.Dataset.from_dict(bio_dict)</p>
<p>The shape now is:</p>
<pre><code>Dataset({
    features: ['id', 'text', 'ner_tags', 'input_ids', 'attention_mask', 'label'],
    num_rows: 8805
})
</code></pre>
<p>When I use the <code>train_test_split</code> function of <code>Datasets</code> I receive the following error:</p>
<pre><code>train_testvalid = ds.train_test_split(test_size=0.5, shuffle=True, stratify_by_column=&quot;label&quot;)
</code></pre>
<blockquote>
<p>ValueError: Stratifying by column is only supported for ClassLabel
column, and column label is Sequence.</p>
</blockquote>
<p>How can I change the type to ClassLabel so that stratify works?</p>
"
74948551,"IndexError: index out of range in self when using summarization, hugging face","<p>I'm getting an error when attempting to run this code:</p>
<pre><code>import nltk
nltk.download('punkt')
from youtube_transcript_api import YouTubeTranscriptApi

video_id = 'wK4XmXJ299k'

transcript = YouTubeTranscriptApi.get_transcript(video_id)

corpus = ' '.join([line['text'] for line in transcript])

from transformers import pipeline
mysummarization = pipeline(&quot;summarization&quot;)
mysummary = mysummarization(corpus)
mysummary[0]['summary_text']
</code></pre>
<p>The code gets a transcript from a YouTube video and attempts to summarize with the Hugging Face Transformers model.  The error is <code>IndexError: index out of range in self.</code></p>
<p>I am also seeing a <code>No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6). Using a pipeline without specifying a model name and revision in production is not recommended. Token indices sequence length is longer than the specified maximum sequence length for this model (11628 &gt; 1024). Running this sequence through the model will result in indexing errors</code> message as well.</p>
<p>How do I fix this?</p>
"
74996994,Do BERT word embeddings change depending on context?,"<p>Before answering &quot;yes, of course&quot;, let me clarify what I mean:</p>
<p>After BERT has been trained, and I want to use the pretrained embeddings for some other NLP task, can I once-off extract all the word-level embeddings from BERT for all the words in my dictionary, and then have a set of static key-value word-embedding pairs, from where I retrieve the embedding for let's say &quot;bank&quot;, or will the embeddings for &quot;bank&quot; change depending on whether the sentence is &quot;Trees grow on the river bank&quot;, or &quot;I deposited money at the bank&quot; ?</p>
<p>And if the latter is the case, how do I practically use the BERT embeddings for another NLP task, do I need to run every input sentence through BERT before passing it into my own model?</p>
<p>Essentially - do embeddings stay the same for each word / token after the model has been trained, or are they dynamically adjusted by the model weights, based on the context?</p>
"
75011964,saving weights of a tensorflow model in Databricks,"<p>In a Databricks notebook which is running on Cluster1 when I do</p>
<pre><code>path='dbfs:/Shared/P1-Prediction/Weights_folder/Weights'
model.save_weights(path)
</code></pre>
<p>and then immediately try</p>
<pre><code>ls 'dbfs:/Shared/P1-Prediction/Weights_folder'
</code></pre>
<p>I see the actual weights file in the output display</p>
<p>But When I run the exact same command
<code>ls 'dbfs:/Shared/P1-Prediction/Weights_folder'</code>
on a different Databricks notebook which is running on cluster 2, I am getting the error</p>
<pre><code>ls: cannot access 'dbfs:/Shared/P1-Prediction/Weights_folder': No such file or directory
</code></pre>
<p>I am not able to intrepret this. Does that mean my &quot;save_weights&quot; is saving the weights in clusters memory and not in an actual physical location? If so is there a solution for it.
Any help is highly appreciated.</p>
"
75050748,Why is positional encoding needed while input ids already represent the order of words in Bert?,"<p>For example, in Huggingface's example:</p>
<pre><code>encoded_input = tokenizer(&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;)
print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>The input_ids vector already encode the order of each token in the original sentence. Why does it need positional encoding again with an extra vector to represent it?</p>
"
75059015,Problem with trained model and load model,"<p>I'm trying to create model from <code>wav2vec2</code> with <code>facebook/wav2vec2-base-960h</code> pretrained model and this is my <code>training_args</code></p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir=save_dir,
    group_by_length=True,
    per_device_train_batch_size=10,
    per_device_eval_batch_size=10,
    gradient_accumulation_steps=2,
    evaluation_strategy=&quot;steps&quot;,
    num_train_epochs=0.5,
    fp16=True,
    save_steps=10,
    eval_steps=10,
    logging_steps=10,
    learning_rate=1e-4,
    warmup_steps=500,
    save_total_limit=2,
)
</code></pre>
<p>and this is my <code>trainer</code></p>
<pre><code>from transformers import Trainer

trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=_common_voice_train,
    eval_dataset=_common_voice_test,
    tokenizer=processor.feature_extractor,
)
</code></pre>
<p>now when the training part is over and model trained the <code>trainer.evaluate()</code> part show me the good result like this</p>
<blockquote>
<p>reference: &quot;Ø´Ù…Ø§ Ø§Ù…Ø±ÙˆØ² ØµØ¨ÙˆØ±ÛŒ Ø¨ÙØ±Ù…Ø§ÛŒÛŒÙ† Ø«Ø¨Øª Ø´Ø¯Ù‡ ØªØ§ Ø§Ù…Ø±ÙˆØ² Ø¨Ø§ Ø´Ù…Ø§ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø´Ù‡&quot;
<br/>predicted: &quot;Ø´Ù…Ø§ Ø§Ù…Ø±ÙˆØ² Ø³Ø¨ÙˆØ±ÛŒ Ø¨ÙØ±Ù…Ø§ÛŒ Ø³Ø¨Ø² Ø´Ø¯Ù‡ ØªØ§ Ø§Ù…Ø±ÙˆØ² Ø¨Ø§ Ø´Ù…Ø§ Ù‡Ù…Ù‡Ù…Ù†Ú¯ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø§Ø´Ù‡&quot;</p>
</blockquote>
<p>but when I'm trying to load and use the model I got this</p>
<blockquote>
<p>Ø±Ú†Ø³ØµØ¬Ù¾ Ù‡Ø¯Ø«Ø¬ ÛŒÙˆ ØªÙˆ ÛŒØªÙ†Ù¾ Ù‡Ø± ÙˆØºØ³Ù‡Ø±ÙˆØºØ¬ Ø³Ú† Ø«Ø²ØªØ³Ù‡ Ø´ØªØ°Ø³ ØµÙ…Ø±Ø¬Ú†Ùˆ</p>
</blockquote>
<p>I load my model like this</p>
<pre><code>sample_rate = 16_000

model = Wav2Vec2ForCTC.from_pretrained(&quot;/content/drive/MyDrive/model&quot;)
processor = Wav2Vec2Processor.from_pretrained(&quot;/content/drive/MyDrive/model&quot;)
audio_input, sample_rate = librosa.load(&quot;/content60_L4.wav&quot;, sr=sample_rate)
input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=&quot;pt&quot;).input_values
logits = model(input_values).logits
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.decode(predicted_ids[0])
</code></pre>
<p>I can't find my mistake</p>
"
75140385,how to use csv data to train a hugging face model?,"<p>I want to fine-tune a pre-trained DistilBERT (transformer model based on the BERT architecture) model available Hugging Face. I did some data cleaning up/pre-processing step to generate a csv data and uploaded to an s3 bucket.</p>
<p>based on the example provided here (<a href=""https://github.com/aws-samples/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face"" rel=""nofollow noreferrer"">https://github.com/aws-samples/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face</a>) , the code below is a train.py file .</p>
<p>I have a couple of csv file that i want to use for training and testing. in the code below, it looks like , it is loading data as below, how can i change this to be able to read and use csv, given csv is an s3 location.</p>
<pre><code>
train_dataset = load_from_disk(args.training_dir)
</code></pre>
<pre><code>&quot;&quot;&quot;
Training script for Hugging Face SageMaker Estimator
&quot;&quot;&quot;
import logging
import sys
import argparse
import os
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import Trainer, TrainingArguments
from datasets import load_from_disk
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

if __name__ == &quot;__main__&quot;:

    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to   the script.
    parser.add_argument(&quot;--epochs&quot;, type=int, default=3)
    parser.add_argument(&quot;--train_batch_size&quot;, type=int, default=32)
    parser.add_argument(&quot;--eval_batch_size&quot;, type=int, default=64)
    parser.add_argument(&quot;--warmup_steps&quot;, type=int, default=500)
    parser.add_argument(&quot;--model_name&quot;, type=str)
    parser.add_argument(&quot;--tokenizer_name&quot;, type=str)
    parser.add_argument(&quot;--learning_rate&quot;, type=str, default=5e-5)

    # Data, model, and output directories
    parser.add_argument(&quot;--output-data-dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DATA_DIR&quot;])
    parser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ[&quot;SM_MODEL_DIR&quot;])
    parser.add_argument(&quot;--n_gpus&quot;, type=str, default=os.environ[&quot;SM_NUM_GPUS&quot;])
    parser.add_argument(&quot;--training_dir&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_TRAIN&quot;])
    parser.add_argument(&quot;--test_dir&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_TEST&quot;])

    args, _ = parser.parse_known_args()

    # load datasets
    train_dataset = load_from_disk(args.training_dir)
    test_dataset = load_from_disk(args.test_dir)

    # download model and tokenizer from model hub
    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)

    # define training args
    training_args = TrainingArguments(
        output_dir=args.model_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        warmup_steps=args.warmup_steps,
        evaluation_strategy=&quot;epoch&quot;,
        logging_dir=f&quot;{args.output_data_dir}/logs&quot;,
        learning_rate=float(args.learning_rate),
    )

    # create Trainer instance
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
    )

    # train model
    trainer.train()
    ...
    ...
</code></pre>
"
75154742,BERT model convertation from DeepPavlov to HuggingFace format,"<p>I have a folder with the ruBERT model, which was fine-tuned with the application of the Deeppavlov library.
The folder contains the following model files:</p>
<p><a href=""https://i.stack.imgur.com/mKdZE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mKdZE.png"" alt=""enter image description here"" /></a></p>
<p>How do I convert it to Huggingface format so that I can load it this way?</p>
<pre><code>from transformers import TFAutoModelForSequenceClassification

model_name = &quot;folder_with_ruBERT&quot;
auto_model_rubert = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case = False)
</code></pre>
"
75158430,Error 'img' when applying increment with keras and transformers for image classification,"<p>I would like to apply VIT for image classification. But I have one problem and I don't know as resolve it. My error is this &quot;KeyError: 'img'&quot;. The error is shown when I apply the last comand, and I don't know where is my error. The image within dataset are in .png, but I don't think that this was mistake.
Below there is the script:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
import os
import cv2
import matplotlib.pyplot as plt
from transformers import ViTFeatureExtractor, ViTForImageClassification
from transformers import TrainingArguments, Trainer
from tensorflow import keras 
from tensorflow.keras import layers
from datasets import load_metric
from PIL import Image as img
from IPython.display import Image, display
from datasets import load_dataset 
import torch
dataset = load_dataset(&quot;imagefolder&quot;, data_dir=&quot;Datasets&quot;)

dataset
example = dataset[&quot;train&quot;][10]
example
dataset[&quot;train&quot;].features
example['image']
example['image'].resize((200, 200))
example['label']
dataset[&quot;train&quot;].features[&quot;label&quot;]


img_class_labels = dataset[&quot;train&quot;].features[&quot;label&quot;].names

from transformers import ViTFeatureExtractor
from tensorflow import keras
from tensorflow.keras import layers


model_id = &quot;google/vit-base-patch16-224-in21k&quot;
feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)

# learn more about data augmentation here: https://www.tensorflow.org/tutorials/images/data_augmentation
data_augmentation = keras.Sequential(
    [
        layers.Resizing(feature_extractor.size, feature_extractor.size),
        layers.Rescaling(1./255),
        layers.RandomFlip(&quot;horizontal&quot;),
        layers.RandomRotation(factor=0.02),
        layers.RandomZoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name=&quot;data_augmentation&quot;,
)
# use keras image data augementation processing
def augmentation(examples):
    # print(examples[&quot;img&quot;])
    examples[&quot;pixel_values&quot;] = [data_augmentation(image) for image in examples[&quot;img&quot;]]
    return examples


# basic processing (only resizing)
def process(examples):
    examples.update(feature_extractor(examples['img'], ))
    return examples

# we are also renaming our label col to labels to use `.to_tf_dataset` later
dataset_ds = dataset[&quot;train&quot;].rename_column(&quot;label&quot;, &quot;labels&quot;)

processed_dataset = dataset_ds.map(augmentation, batched=True)
processed_dataset
</code></pre>
"
75209070,txtai.database.sql.base.SQLError: no such function: json_extract,"<p>Using TxtAI python module with sql query <kbd>SELECT id, text, score, solution_id, column_name FROM txtai WHERE similar('{query}') AND score &gt;= 0.5</kbd>, I am seeing this error <kbd>txtai.database.sql.base.SQLError: no such function: json_extract</kbd></p>
<p>I am trying to use dynamic search using TxtAI module, which uses a SQL based context manager but it is not working as expected.</p>
<p>This are working perfectly on my widows machine, but the same is not happening on the CentOS server. I have tried to maintain all the modules and packages versions same using requirement.txt files.</p>
"
75227030,Getting an embedded output from huggingface transformers,"<p>To compare different paragraphs, I am trying to use a transformer model, fitting each paragraph onto the model and then in the end I intend to compare the outputs and see which paragraph has the most similarity.</p>
<p>For this purpose, I am using Roberta-base model. I first used roberta tokenizer on a paragraph. Then I used the roberta model on that tokenized output. But the process is failing due to lack of memory. Even 25GB ram is not enough to complete the process for the paragraphs with 1324 lines.</p>
<p>Any idea how can I make it better or any suggestion what mistakes i might be doing?</p>
<pre><code>from transformers import RobertaTokenizer, RobertaModel
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tokenizer = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)

model = RobertaModel.from_pretrained(&quot;roberta-base&quot;).to(device)

inputs = tokenizer(dict_anrika['Anrika'], return_tensors=&quot;pt&quot;, truncation=True, 
padding=True).to(device)
outputs = model(**inputs)
</code></pre>
"
75229395,TypeError: '<' not supported between instances of 'torch.device' and 'int',"<pre><code>2023-01-25 08:21:21,659 - ERROR - Traceback (most recent call last):
  File &quot;/home/xyzUser/project/queue_handler/document_queue_listner.py&quot;, line 148, in __process_and_acknowledge
    pipeline_result = self.__process_document_type(message, pipeline_input)
  File &quot;/home/xyzUser/project/queue_handler/document_queue_listner.py&quot;, line 194, in __process_document_type
    pipeline_result = bill_parser_pipeline.process(pipeline_input)
  File &quot;/home/xyzUser/project/main/billparser/__init__.py&quot;, line 18, in process
    bill_extractor_model = MachineGeneratedBillExtractorModel()
  File &quot;/home/xyzUser/project/main/billparser/models/qa_model.py&quot;, line 25, in __new__
    cls.__model = TransformersReader(model_name_or_path=cls.__model_path, use_gpu=False)
  File &quot;/home/xyzUser/project/.env/lib/python3.8/site-packages/haystack/nodes/base.py&quot;, line 48, in wrapper_exportable_to_yaml
    init_func(self, *args, **kwargs)
  File &quot;/home/xyzUser/project/.env/lib/python3.8/site-packages/haystack/nodes/reader/transformers.py&quot;, line 93, in __init__
    self.model = pipeline(
  File &quot;/home/xyzUser/project/.env/lib/python3.8/site-packages/transformers/pipelines/__init__.py&quot;, line 542, in pipeline
    return task_class(model=model, framework=framework, task=task, **kwargs)
  File &quot;/home/xyzUser/project/.env/lib/python3.8/site-packages/transformers/pipelines/question_answering.py&quot;, line 125, in __init__
    super().__init__(
  File &quot;/home/xyzUser/project/.env/lib/python3.8/site-packages/transformers/pipelines/base.py&quot;, line 691, in __init__
    self.device = device if framework == &quot;tf&quot; else torch.device(&quot;cpu&quot; if device &lt; 0 else f&quot;cuda:{device}&quot;)
TypeError: '&lt;' not supported between instances of 'torch.device' and 'int'
</code></pre>
<p>This is the error message i got after installing a requirement.txt file from my project. I think it is related to torch but also dont know how to fix it. I am new to hugging face transformers and dont know if it is a version issue.</p>
"
75237628,tokenizer.save_pretrained TypeError: Object of type property is not JSON serializable,"<p>I am trying to save the GPT2 tokenizer as follows:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2LMHeadModel
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
tokenizer.pad_token = GPT2Tokenizer.eos_token
dataset_file = &quot;x.csv&quot;
df = pd.read_csv(dataset_file, sep=&quot;,&quot;)
input_ids = tokenizer.batch_encode_plus(list(df[&quot;x&quot;]), max_length=1024,padding='max_length',truncation=True)[&quot;input_ids&quot;]

# saving the tokenizer
tokenizer.save_pretrained(&quot;tokenfile&quot;)
</code></pre>
<p>I am getting the following error:
TypeError: Object of type property is not JSON serializable</p>
<p>More details:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Cell In[x], line 3
      1 # Save the fine-tuned model
----&gt; 3 tokenizer.save_pretrained(&quot;tokenfile&quot;)

File /3tb/share/anaconda3/envs/ak_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2130, in PreTrainedTokenizerBase.save_pretrained(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)
   2128 write_dict = convert_added_tokens(self.special_tokens_map_extended, add_type_field=False)
   2129 with open(special_tokens_map_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
-&gt; 2130     out_str = json.dumps(write_dict, indent=2, sort_keys=True, ensure_ascii=False) + &quot;\n&quot;
   2131     f.write(out_str)
   2132 logger.info(f&quot;Special tokens file saved in {special_tokens_map_file}&quot;)

File /3tb/share/anaconda3/envs/ak_env/lib/python3.10/json/__init__.py:238, in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    232 if cls is None:
    233     cls = JSONEncoder
    234 return cls(
    235     skipkeys=skipkeys, ensure_ascii=ensure_ascii,
    236     check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237     separators=separators, default=default, sort_keys=sort_keys,
--&gt; 238     **kw).encode(obj)

File /3tb/share/anaconda3/envs/ak_env/lib/python3.10/json/encoder.py:201, in JSONEncoder.encode(self, o)
    199 chunks = self.iterencode(o, _one_shot=True)
...
    178     &quot;&quot;&quot;
--&gt; 179     raise TypeError(f'Object of type {o.__class__.__name__} '
    180                     f'is not JSON serializable')

TypeError: Object of type property is not JSON serializable
</code></pre>
<p>How can I solve this issue?</p>
"
75385142,tokenizer.push_to_hub(repo_name) is not working,"<p>I'm trying to puch my tokonizer to my huggingface repo...
it consist of the model vocab.Json (I'm making a speech recognition model)
My code:</p>
<pre><code>vocab_dict[&quot;|&quot;] = vocab_dict[&quot; &quot;]
del vocab_dict[&quot; &quot;]
vocab_dict[&quot;[UNK]&quot;] = len(vocab_dict)
vocab_dict[&quot;[PAD]&quot;] = len(vocab_dict)
len(vocab_dict)
</code></pre>
<pre><code>import json
with open('vocab.json', 'w') as vocab_file:
    json.dump(vocab_dict, vocab_file)
</code></pre>
<pre><code>from transformers import Wav2Vec2CTCTokenizer

tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(&quot;./&quot;, unk_token=&quot;[UNK]&quot;, pad_token=&quot;[PAD]&quot;, word_delimiter_token=&quot;|&quot;)
</code></pre>
<pre><code>from huggingface_hub import login

login('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')
repo_name = &quot;Foxasdf/ArabicTextToSpeech&quot;
add_to_git_credential=True
tokenizer.push_to_hub(repo_name)
</code></pre>
<p>the tokenizer.push_to_hub(repo_name) is giving me this error:
<strong>TypeError: create_repo() got an unexpected keyword argument 'organization'</strong></p>
<p>I have logged in my huggingface account using
from huggingface_hub import notebook_login
notebook_login()
but the error is still the same..
here's a link of the my collab notebook you can see the full code there and the error: <a href=""https://colab.research.google.com/drive/11tkQ85SfaT6U_1PXDNwk0Q6qogw2r2sw?hl=ar&amp;hl=en&amp;authuser=0#scrollTo=WkbZ_Wcidq8Z"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/11tkQ85SfaT6U_1PXDNwk0Q6qogw2r2sw?hl=ar&amp;hl=en&amp;authuser=0#scrollTo=WkbZ_Wcidq8Z</a></p>
"
75437911,I would like to finetune the blip model on ROCO data set for image captioning of chest x-rays,"<p>I want to fine tune the blip model on ROCO database for image captioning chest x-ray images. But I am getting an error regarding integer indexing.</p>
<p>Can anyone please help me understand the cause of the error and how to rectify it.</p>
<p>This is the code:</p>
<pre><code>def read_data(filepath,csv_path,n_samples):
    df = pd.read_csv(csv_path)
    images = []
    capts = []
    for idx in range(len(df)):
        if 'hest x-ray' in df['caption'][idx] or 'hest X-ray' in df['caption'][idx]:
            if len(images)&gt;n_samples:
                break            
            else:
                images.append(Image.open(os.path.join(filepath,df['name'][idx])).convert('L'))
                capts.append(df['caption'][idx])
    return images, capts
    

def get_data():
    imgtrpath = 'all_data/train/radiology/images'
    trcsvpath = 'all_data/train/radiology/traindata.csv'
    imgtspath = 'all_data/test/radiology/images'
    tscsvpath = 'all_data/test/radiology/testdata.csv'
    imgvalpath = 'all_data/validation/radiology/images'
    valcsvpath = 'all_data/validation/radiology/valdata.csv'

    print('Extracting Training Data')
    trainimgs, traincapts = read_data(imgtrpath, trcsvpath, 1800)
    
    print('Extracting Testing Data')
    testimgs, testcapts = read_data(imgtrpath, trcsvpath, 100)
    
    print('Extracting Validation Data')
    valimgs, valcapts = read_data(imgtrpath, trcsvpath, 100)


                
    return trainimgs, traincapts, testimgs, testcapts, valimgs, valcapts

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

trainimgs, traincapts, testimgs, testcapts, valimgs, valcapts = get_data() 
model = BlipForConditionalGeneration.from_pretrained(&quot;Salesforce/blip-image-captioning-large&quot;)
processor = BlipProcessor.from_pretrained(&quot;Salesforce/blip-image-captioning-large&quot;)

metric = evaluate.load(&quot;accuracy&quot;)
traindata = processor(text=traincapts, images=trainimgs, return_tensors=&quot;pt&quot;, padding=True, truncation=True)
evaldata =  processor(text=testcapts, images=testimgs, return_tensors=&quot;pt&quot;, padding=True, truncation=True)
training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=traindata,
    eval_dataset=evaldata,
    compute_metrics=compute_metrics
)
trainer.train()
</code></pre>
<p>The code is meant to fine-tune the BLIP model on the ROCO dataset chest x-ray images for the purpose of image captioning.
But when I run it, I am getting this error:</p>
<pre><code>  File &quot;C:\Users\omair\anaconda3\envs\torch\lib\site-packages\transformers\feature_extraction_utils.py&quot;, line 86, in __getitem__
    raise KeyError(&quot;Indexing with integers is not available when using Python based feature extractors&quot;)

KeyError: 'Indexing with integers is not available when using Python based feature extractors'
</code></pre>
"
75438284,Why is the parallel version of my code slower than the serial one?,"<p>I am trying to run a model multiple times. As a result it is time consuming. As a solution I try to make it parallel. However, it ends up to be slower. <strong>Parallel is 40 seconds</strong> while <strong>serial is 34 seconds</strong>.</p>
<pre><code># !pip install --target=$nb_path transformers
oracle = pipeline(model=&quot;deepset/roberta-base-squad2&quot;)
question = 'When did the first extension of the Athens Tram take place?'
print(data)
print(&quot;Data size is: &quot;, len(data))


parallel = True

if parallel == False:
  counter = 0
  l = len(data)
  cr = []
  for words in data:
    counter+=1
    print(counter, &quot; out of &quot;, l)
    cr.append(oracle(question=question, context=words))
elif parallel == True:
  from multiprocessing import Process, Queue
  import multiprocessing

  no_CPU = multiprocessing.cpu_count()
  print(&quot;Number of cpu : &quot;, no_CPU)
  l = len(data)


  def answer_question(data, no_CPU, sub_no):
    cr_process = []
    counter_process = 0
    for words in data:
      counter_process+=1
      l_data = len(data)
      # print(&quot;n is&quot;, no_CPU)
      # print(&quot;l is&quot;, l_data)
      print(counter_process, &quot; out of &quot;, l_data, &quot;in subprocess number&quot;, sub_no)
      cr_process.append(oracle(question=question, context=words))
      # Q.put(cr_process)
    cr.append(cr_process)


  n = no_CPU      # number of subprocesses
  m = l//n        # number of data the n-1 first subprocesses will handle
  res = l % n     # number of extra data samples the last subprocesses has
  
  # print(m)
  # print(res)
  procs = []
  # instantiating process with arguments
  for x in range(n-1):
    # print(x*m)
    # print((x+1)*m)
    proc = Process(target=answer_question, args=(data[x*m:(x+1)*m],n, x+1,))
    procs.append(proc)
    proc.start()
  proc = Process(target=answer_question, args=(data[(n-1)*m:n*m+res],n,n,))

  procs.append(proc)
  proc.start()

  # complete the processes
  for proc in procs:
    proc.join()
</code></pre>
<p>A sample of the <code>data</code> variable can be found <a href=""https://pastebin.pl/view/272b15ed"" rel=""nofollow noreferrer"">here</a> (to not flood the question). Argument <code>parallel</code> controls the serial and the parallel version. So my question is, why does it happen and how do I make the parallel version faster? I use google colab so it has 2 CPU cores available , that's what <code>multiprocessing.cpu_count()</code> is saying at least.</p>
"
75445494,Bert Tokenizer punctuation for named entity recognition task,"<p>I'm working on a named entity recognition task, where I need to identify person names, books etc.</p>
<p>I am using Huggingface Transformer package and BERT with PyTorch. Generally it works very good, however, my issue is that for some first names a dot &quot;.&quot; is a part of the first name and shouldn't be separate it from it. For example, for the person name &quot;Paul Adam&quot;, the first name in the training data is shortened to one letter combined with dot &quot;P. Adam&quot;. The tokenizer tokenize it as [&quot;P&quot;, &quot;.&quot;, &quot;Adam&quot;] which later negatively impact the ner trained model performance as &quot;P.&quot; is presented in the training data and not only &quot;P&quot;. The model is capable to recognize the full names but fails in the shortened one. I used Spacy tokenizer before and I didn't face this issue. Here more details:</p>
<pre><code>from transformers import BertTokenizer, BertConfig, AutoTokenizer, AutoConfig, BertModel
path_pretrained_model='/model/bert/'
tokenizer = BertTokenizer.from_pretrained(path_pretrained_model)

print(tokenizer.tokenize(&quot;P. Adam is a scientist.&quot;))

Output:
['p', '.', 'adam', 'is', 'a', 'scientist', '.']

The helpful output would be 
['p.', 'adam', 'is', 'a', 'scientist', '.']
</code></pre>
"
75471067,torch crossentropy loss calculation difference between 2D input and 3D input,"<p>i am running a test on torch.nn.CrossEntropyLoss. I am using the example shown on the official page.</p>
<pre><code>loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5, requires_grad=False)
target = torch.randn(3, 5).softmax(dim=1)
output = loss(input, target)
</code></pre>
<p>the output is 2.05.
in the example, both the input and the target are 2D tensors. Since in most NLP case, the input should be 3D tensor and correspondingly the output should be 3D tensor as well. Therefore, i wrote the a couple lines of testing code, and found a weird issue.</p>
<pre><code>input = torch.stack([input])
target = torch.stack([target])
output = loss(ins, ts)
</code></pre>
<p>the output is 0.9492
This result really confuse me, except the dimensions, the numbers inside the tensors are totally the same. Does anyone know the reason why the difference is?</p>
<p>the reason why i am testing on the method is i am working on project with Transformers.BartForConditionalGeneration. the loss result is given in the output, which is always in (1,) shape. the output is confusing.  If my batch size is greater than 1, i am supposed to get batch size number of loss instead of just one. I took a look at the code, it just simply use nn.CrossEntropyLoss(), so i am considering that the issue may be in the nn.CrossEntropyLoss() method. However, it is stucked in the method.</p>
"